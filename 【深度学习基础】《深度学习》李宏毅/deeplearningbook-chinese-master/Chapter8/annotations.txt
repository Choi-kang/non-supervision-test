{'user': 'acct:happynoom@hypothes.is', 'text': '复杂度随着输入维度增加呈几何级数增长', 'origin_text': '性分类器而言，精确地最小化0−10−10-1损失通常是不可解的（  !!!复杂度几何级数增长于输入维数!!!  ）{cite?}。在这种情况下，我们通常会优化替代损失函数。', 'time': '2017-01-16T10:34'}
{'user': 'acct:happynoom@hypothes.is', 'text': '，从而，', 'origin_text': '别的距离以改进分类器的鲁棒性，获得一个更强壮的，值得信赖的分类器  !!!。因而，!!!  相较于简单地最小化训练集上的平均0−10−10-1损失，从训练数', 'time': '2017-01-16T10:41'}
{'user': 'acct:happynoom@hypothes.is', 'text': '可以考虑将batch翻译为“批量”，以适应汉语读者。', 'origin_text': '练集的优化算法被称为batch或确定性梯度算法，因为它们会同时大  !!!batch!!!  地处理所有的样本。这个术语可能有点令人困惑，因为这个词”bat', 'time': '2017-01-16T10:51'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '代理损失函数', 'origin_text': '同的方法，我们真正优化的目标会更加不同于我们希望优化的目标。  !!!替代损失函数!!!  和提前终止有时，我们真正关心的损失函数（比如分类误差）并不能', 'time': '2017-01-17T09:28'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '要抛掷n次硬币都正面朝上的难度是指数级的', 'origin_text': '易抛硬币得到正面朝上一次而获取局部极小值。在nnn-维空间中，  !!!指数级不太可能nnn次抛掷硬币都得到正面朝上!!!  。参看{Dauphin-et-al-NIPS2014-smal', 'time': '2017-02-05T02:56'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '简化', 'origin_text': '要很大的改动。局部极小值凸优化问题的一个突出特点是其可以  !!!归约!!!  为寻找一个局部极小值的问题。任何一个局部极小值都是全局最小值。', 'time': '2017-01-18T02:32'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '较优的可行解', 'origin_text': '个凸问题时，若发现了任何形式的临界点，我们都会知道已经找到了一个  !!!很好的解!!!  。对于非凸函数时，如神经网络，有可能会存在多个局部极小值', 'time': '2017-01-18T02:36'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '鞍点扩散对于训练算法来说有哪些影响呢？', 'origin_text': '理论证明了另一类和神经网络相关的高维随机函数也满足这种情况。  !!!训练算法时的鞍点扩散会有哪些影响呢!!!  ？对于只使用梯度信息的一阶优化算法而言，情况是不明的。鞍点附', 'time': '2017-01-18T07:14'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '目前情况还不清楚。', 'origin_text': '鞍点扩散会有哪些影响呢？对于只使用梯度信息的一阶优化算法而言，  !!!情况是不明的。!!!  鞍点附近的梯度通常会非常小。另一方面，实验中梯度下降似乎可以', 'time': '2017-01-18T07:42'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '也主张，应该可以通过分析来表明连续时间的梯度下降会逃离而不是吸引到鞍点，不过， 对梯度下降在更多的使用场景来说，情况或许会有不同。', 'origin_text': '出该区间。{GoodfellowOptimization15}  !!!还认为，或许可以解析地表明连续时间的梯度下降会逃离，而不是吸引，附近的鞍点，但是对于梯度下降更现实的使用，情况或许是不同的!!!  。\\begin{figure}[!htb]\\ifOpenSo', 'time': '2017-01-26T04:00'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '可能会使大量已完成的优化工作成为无用功。', 'origin_text': '当参数接近这样的悬崖区域时，梯度下降更新可以使参数弹射得非常远，  !!!可能会无效化已经完成的大量优化工作!!!  。图经\xa0{Pascanu+al-ICML2013-small}许', 'time': '2017-01-19T05:42'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '当计算图变的极深时，神经网络优化算法会面临的另外一个难题就是长期依赖问题——由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难。\n深层的计算图不仅前馈网络中存在，在循环神经网络（在第十章中描述）结构中也是一样的，因为要在很长时间序列的各时刻重复操作相同的网络模块，并且模型参数共享，这使问题更加凸显。\n\n说明：\n\n这段感觉原文表述就有些问题，读起来感觉意思不连贯，在此小节并没有详说“长期依赖”是个什么东西，我调整了一下， 增加已经对长期依赖的简单说明来呼应标题，在破折号后面的一句行文解释来说明长期依赖对优化的困难，后接作者描述就显得意思上连贯了。', 'origin_text': '因素的相乘。因此，长期时间序列会产生大量相乘。长期依赖  !!!当计算图变得非常之深时，神经网络优化算法必须克服的另一个难题出现了。多层前馈网络会有这么深的计算图。\\chap?会介绍的循环网络会在很长的时间序列的每个时间点上重复应用相同的操作，因此也会有很深的计算图。反复使用相同的参数产生了尤为突出的困难。!!!  例如，假设某个计算图中包含一条重复与矩阵WW\\MW相乘的', 'time': '2017-01-20T02:25'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '隐变量', 'origin_text': '训练集可以唯一确定一组模型参数，那么该模型被称为可辨认的。带有  !!!潜变量!!!  的模型通常是不可辨认的，因为通过批次交换潜变量我们能得到等价的模', 'time': '2017-01-19T14:37'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'explosion 建议翻译成 “膨胀”\n', 'origin_text': 'λi\\lambda_i不在111附近时，若在量级上大于111则会  !!!爆炸!!!  ；若小于111时则会消失。\\textbf{梯度消失（或弥散）问', 'time': '2017-01-20T02:30'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '膨胀的', 'origin_text': '。消失的梯度使得难以知道参数朝哪个方向移动能够改进代价函数，而  !!!爆炸!!!  梯度会使得学习不稳定。之前描述的诱发梯度截断的悬崖结构便是爆炸', 'time': '2017-01-20T02:30'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '各时刻', 'origin_text': '诱发梯度截断的悬崖结构便是爆炸梯度现象的一个例子。此处描述的  !!!每个时间点!!!  重复与WW\\MW相乘非常类似于寻求矩阵WW\\MW的最大特征值及对', 'time': '2017-01-20T02:49'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '各时刻', 'origin_text': 'x\\Vx中所有与WW\\MW的主特征向量垂直的成分。循环网络在  !!!每个时间步上!!!  使用相同的矩阵WW\\MW，而前馈网络并没有。因而即使是非常深层', 'time': '2017-01-20T02:52'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '一定', 'origin_text': 'WW\\MW，而前馈网络并没有。因而即使是非常深层的前馈网络也能  !!!很大!!!  程度上避免梯度消失和爆炸问题{cite?}。在循环网络已经被', 'time': '2017-01-20T02:52'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '时刻', 'origin_text': '崖结构在循环神经网络的代价函数中很常见，因为这类模型会涉及到多个  !!!时间步长!!!  因素的相乘。因此，长期时间序列会产生大量相乘。长期依赖', 'time': '2017-01-20T03:09'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '系数？', 'origin_text': '循环神经网络的代价函数中很常见，因为这类模型会涉及到多个时间步长  !!!因素!!!  的相乘。因此，长期时间序列会产生大量相乘。长期依赖当计', 'time': '2017-01-20T03:09'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '在更详细地描述循环网络之后', 'origin_text': '前馈网络也能很大程度上避免梯度消失和爆炸问题{cite?}。  !!!在循环网络已经被更详细地描述了之后!!!  ，我们将会在\\sec?进一步讨论循环网络训练中的挑战。非精确', 'time': '2017-01-20T03:09'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '噪音', 'origin_text': '知道精确的梯度或是Hessian矩阵。在实践中，通常这些量会有  !!!噪扰!!!  ，甚至是有偏的估计。几乎每一个深度学习算法都需要基于采样的估计', 'time': '2017-01-20T03:22'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '代理', 'origin_text': '到了梯度估计的欠完整性。我们可以选择比真实损失函数更容易估计的  !!!替代!!!  损失函数来避免这个问题。局部和全局结构间的弱对应迄今为止', 'time': '2017-01-20T03:28'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '找到', 'origin_text': '寻一个围绕山形结构的宽弧。大多数优化研究的难点集中于训练是否  !!!到达!!!  了全局最小值，局部最小值，或是鞍点，但在实践中神经网络不会到达任', 'time': '2017-01-20T03:55'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '不一定存在', 'origin_text': 'ig?表明神经网络通常不会到达梯度很小的区域。甚至，这些临界点  !!!不是必然存在的!!!  。例如，损失函数 −logp(y∣x;θ)−log\u2061p(y∣x', 'time': '2017-01-20T03:56'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '而是当随着训练模型逐渐稳定后，收敛于某个值。', 'origin_text': '\\log p(y\\mid\\Vx;\\Vtheta)没有全局极小点，  !!!而是当模型更自信时会逐渐趋向某个值!!!  。  对于具有离散标签yyy和softmax分布p(y∣x)p', 'time': '2017-01-20T04:02'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '则负对数似然可以无限趋近但不会等于零。', 'origin_text': 'id\\Vx)的分类器而言，若模型能够正确分类训练集上的每个样本，  !!!则负对数似然可以任意地趋近于零，但是不可能实际达到零值!!!  。同样地，实值模型p(y∣x)=N(y;f(θ),β−1)p(', 'time': '2017-01-20T10:04'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '无穷', 'origin_text': '(\\Vtheta),\\beta^{-1})的负对数似然会趋向于负  !!!无限大!!!  ——如果f(θ)f(θ)f(\\Vtheta)能够正确预测所有的训', 'time': '2017-01-20T04:08'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '先决条件', 'origin_text': '一步讨论循环网络训练中的挑战。非精确梯度大多数优化算法的  !!!出发点!!!  都是我们知道精确的梯度或是Hessian矩阵。在实践中，通常这', 'time': '2017-01-20T06:45'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '若J(θ)J(θ)J(\\Vtheta)是当前点θθ\\Vtheta的病态条件', 'origin_text': '迄今为止，我们讨论的许多问题都是关于损失函数在单点的性质——  !!!若J(θ)J(θ)J(\\Vtheta)在当前点θθ\\Vtheta是病态条件数!!!  ，或者θθ\\Vtheta在悬崖中，或者θθ\\Vtheta是一个隐', 'time': '2017-01-20T06:59'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '无限制', 'origin_text': '(\\Vtheta)能够正确预测所有的训练集目标yyy，学习算法会  !!!无边界!!!  地增加ββ\\beta。如\\fig?所示，即使没有局部极小值和鞍', 'time': '2017-01-20T07:03'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '局部下降不确定能不能找到有效解的短路径', 'origin_text': '一些性质，如近似的有偏梯度或正确方向估计的方差。在这些情况下，  !!!局部下降或许能或许不能定义到达有效解的短路径!!!  ，我们并不能真的遵循局部下降的路径。目标函数可能有诸如病态条件', 'time': '2017-01-20T07:07'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '困难', 'origin_text': 'ig?所示。目前，我们还不了解这些问题中的哪一个与神经网络优化  !!!难题!!!  最相关，这是研究领域的热点方向。不管哪个问题最重要，如果', 'time': '2017-01-20T07:12'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '输出光滑的连续值', 'origin_text': '适用于神经网络的单元输出离散值的情况。然而，大多数神经网络单元  !!!输出平稳的增值!!!  ，使得局部搜索求解优化可行。一些理论结果表明，存在某类问题是不', 'time': '2017-01-20T08:31'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '但是 在实际情况中 我们通过设置更多参数，很容易发现针对大型网络来说还不错的解决方案。\n\n说明：\n目前的翻译 有三个定语，太冗长。', 'origin_text': '。其他结果表明，寻求给定规模的网络的一个解决方案是不可解的，  !!!但在实践中，我们可以通过一个有很多对应于可接受解决方案的参数设定的更大的神经网络，很容易地找到一个解决方案。!!!  此外，在神经网络训练中，我们通常不关注函数的精确极小值，而只关', 'time': '2017-01-20T10:05'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '想要对优化算法是否能完成此目标进行理论分析是非常困难的', 'origin_text': '确极小值，而只关注将其值下降到足够小以获得一个很好的泛化误差。  !!!关于优化算法能否达到这个目标的理论分析是极其困难的。!!!  因此， 研究优化算法性能上更现实的界限仍然是机器学习研究中的一', 'time': '2017-01-20T09:03'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '在现实中研究优化算法的性能上界仍然是学术界的重要目标', 'origin_text': '关于优化算法能否达到这个目标的理论分析是极其困难的。因此，   !!!研究优化算法性能上更现实的界限仍然是机器学习研究中的一个重要目标。!!!  基本算法之前我们已经介绍了梯度下降（\\sec?），即', 'time': '2017-01-20T09:05'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '应用最广的', 'origin_text': '梯度下降随机梯度下降（SGD）及其变种很可能是一般机器学习中  !!!用得最多的!!!  优化算法，特别是在深度学习中。如\\sec?讨论，通过计算独立同', 'time': '2017-01-20T09:08'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '按照数据生成分布抽取m个小批量（独立同分布的）样本，通过计算它们梯度均值，', 'origin_text': '习中用得最多的优化算法，特别是在深度学习中。如\\sec?讨论，  !!!通过计算独立同分布地从数据生成分布中抽取的mmm个minibatch样本的梯度均值!!!  ，我们可以得到梯度的无偏估计。\\alg?展示了如何使用这个下', 'time': '2017-01-20T09:35'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '学习率', 'origin_text': '\\end{algorithm}SGD算法中的一个关键参数是  !!!学习速率!!!  。之前，我们介绍的SGD使用固定的学习速率。在实践中，有必要', 'time': '2017-01-29T01:18'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '应该谨慎的参考这个问题的大部分指导', 'origin_text': '目标函数值随时间变化的学习曲线。与其说是科学，这更是一门艺术，  !!!关于这个问题的大多数指导都应该被怀疑地看待!!!  。使用线性时间表时，参数选择为ϵ0ϵ0\\epsilon_0，ϵ', 'time': '2017-01-29T02:28'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '这更像', 'origin_text': '的选择方法是画出目标函数值随时间变化的学习曲线。与其说是科学，  !!!这更!!!  是一门艺术，关于这个问题的大多数指导都应该被怀疑地看待。使用线', 'time': '2017-01-29T02:29'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '如果学习率太小', 'origin_text': '的，特别是训练于随机代价函数上，例如由信号丢失引起的代价函数。  !!!如果学习速率太慢!!!  ，那么学习进程会缓慢。如果初始学习速率太低，那么学习可能会卡在', 'time': '2017-01-29T02:31'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '那么学习过程会很缓慢', 'origin_text': '机代价函数上，例如由信号丢失引起的代价函数。如果学习速率太慢，  !!!那么学习进程会缓慢!!!  。如果初始学习速率太低，那么学习可能会卡在一个相当高的损失值。', 'time': '2017-01-29T02:32'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '最优初始学习率的效果会好于大约迭代100次左右后最佳的学习率', 'origin_text': '会卡在一个相当高的损失值。通常，就总训练时间和最终损失值而言，  !!!最优初始学习速率会高于大约迭代100步后输出最好效果的学习速率!!!  。因此，通常最好是检测最早的几轮迭代，使用一个高于此时效果最佳', 'time': '2017-01-29T12:54'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '选择一个比在效果上表现最佳的学习率更大的学习率。', 'origin_text': '后输出最好效果的学习速率。因此，通常最好是检测最早的几轮迭代，  !!!使用一个高于此时效果最佳学习速率的学习速率!!!  ，但又不能太高以致严重的不稳定性。SGD和相关的miniba', 'time': '2017-01-29T13:18'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '但又不能太大导致严重的震荡', 'origin_text': '检测最早的几轮迭代，使用一个高于此时效果最佳学习速率的学习速率，  !!!但又不能太高以致严重的不稳定性!!!  。SGD和相关的minibatch或在线基于梯度的优化的最重', 'time': '2017-01-29T13:19'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'SGD及相关的minibatch亦或更广义的基于梯度优化的在线学习算法，一个重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。', 'origin_text': '效果最佳学习速率的学习速率，但又不能太高以致严重的不稳定性。  !!!SGD和相关的minibatch或在线基于梯度的优化的最重要的性质是每一步更新的计算时间不会随着训练样本数目而增加。!!!  即使训练样本数目非常大时，这也能收敛。对于足够大的数据集，S', 'time': '2017-01-29T13:49'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '用舍近求远的方法来求解问题。', 'origin_text': '梯度下降方向移动，却远离了任何解决方案，如\\fig?所示，或者是  !!!沿着一个不必要的长路径到达解决方法!!!  ，如\\fig?所示。目前，我们还不了解这些问题中的哪一个与神经', 'time': '2017-02-05T03:07'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '却和所有解决方案南辕北辙，', 'origin_text': '。在其他情况下，局部移动可能太过贪心化，朝着梯度下降方向移动，  !!!却远离了任何解决方案!!!  ，如\\fig?所示，或者是沿着一个不必要的长路径到达解决方法，如', 'time': '2017-02-05T03:08'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '最终的观点还是建议在传统优化算法上研究怎样选择更佳的初始化点，来实现目标更切实可行。', 'origin_text': '并且我们能够在该良好区域上初始化学习，那么这些问题都可以避免。  !!!最后一个观点建议研究选择良好的初始点以使用传统优化算法!!!  。优化的理论限制一些理论结果表明，我们为神经网络设计的任', 'time': '2017-02-05T03:18'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '多了个”权“', 'origin_text': '每层的梯度消失和爆炸问题，如\\sec?所述。不幸的是，这些  !!!初始权权重!!!  的最佳准则往往不会带来最佳效果。这可能有三种不同的原因。首先', 'time': '2017-02-15T05:29'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '可惜', 'origin_text': '同权重矩阵用于每层的梯度消失和爆炸问题，如\\sec?所述。  !!!不幸的是!!!  ，这些初始权权重的最佳准则往往不会带来最佳效果。这可能有三种不', 'time': '2017-02-15T05:29'}
{'user': 'acct:hugh0120@hypothes.is', 'text': 'contour翻译为等高线更为合适', 'origin_text': '机梯度的方差。\xa0我们通过此图说明动量如何克服这两个问题的第一个。  !!!轮廓线!!!  描绘了一个二次损失函数（具有病态条件数的Hessian矩阵）。横', 'time': '2017-03-02T13:11'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '自适应学习率算法', 'origin_text': '参数为正确的数值范围，或是设置不同单元计算互相不同的函数。  !!!具有自适应学习速率的算法!!!  神经网络研究员早就意识到学习速率肯定是难以设置的超参数之一，', 'time': '2017-03-05T04:02'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '在本书第三部分讨论的一个常用策略是使用相同的输入数据集，用非监督模型训练出来的参数来初始化监督模型', 'origin_text': '参数的简单固定或随机方法，还有可能使用机器学习初始化模型参数。  !!!本书第III部分讨论的常用策略是初始化监督模型的参数为无监督模型训练于相同的输入上学习出的参数。!!!  我们也可以在相关问题上使用监督训练。即使是运行监督训练在一个', 'time': '2017-03-05T04:18'}
{'user': 'acct:codeVerySlow@hypothes.is', 'text': '正则化项 原文有terms', 'origin_text': 'Vtheta)，该代价函数通常包括整个训练集上的性能评估和额外的  !!!正则化!!!  。首先，我们会介绍机器学习任务训练算法中的优化和纯优化在哪', 'time': '2017-03-05T11:11'}
{'user': 'acct:codeVerySlow@hypothes.is', 'text': '这一点不同于纯优化，纯优化的目标是最小化J本身', 'origin_text': '低代价函数 J(θ)J(θ)J(\\Vtheta)来提高PPP。  !!!这一点不同于纯优化最小化JJJ本身!!!  。训练深度模型的优化算法通常也会包括一些用于机器学习目标函数特', 'time': '2017-03-05T11:12'}
{'user': 'acct:codeVerySlow@hypothes.is', 'text': '误差量', 'origin_text': '机器学习算法的目标是降低方程\\eqn?所示的期望泛化误差。这个  !!!数据量!!!  被称为风险。在这里我们要强调该期望取自真实的潜在分布pdata', 'time': '2017-03-05T11:22'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '具有自适应学习速率（由RMSProp和AdaDelta代表）的算法族都表现得相当鲁棒，不分伯仲，没有哪个算法能脱颖而出，拨得头筹。', 'origin_text': '}展示了许多优化算法在大量学习任务上的价值比较。虽然结果表明，  !!!具有自适应学习速率（由RMSProp和AdaDelta代表）的算法族表现得相当鲁棒，但没有单一的算法表现为最好的。!!!  目前，最流行的活跃使用的优化算法包括SGD，具动量的SGD，', 'time': '2017-03-05T11:38'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '最流行和使用最活跃', 'origin_text': '的算法族表现得相当鲁棒，但没有单一的算法表现为最好的。目前，  !!!最流行的活跃使用!!!  的优化算法包括SGD，具动量的SGD，RMSProp，具动量的R', 'time': '2017-03-05T13:02'}

=============================   Replies   =============================

{'user': 'acct:liber145@hypothes.is', 'text': '这个翻译很赞！', 'time': '2017-03-05T04:08'}
{'user': 'acct:liber145@hypothes.is', 'text': '赞啊！这个翻译很赞！', 'time': '2017-03-05T03:35'}
{'user': 'acct:liber145@hypothes.is', 'text': '读起来 膨胀 舒服，意思上我倾向于 爆炸，指出梯度变化过于剧烈，膨胀的程度可能不够。\n另外，google里，词组 梯度爆炸 比 梯度膨胀 出现得高。', 'time': '2017-03-05T02:50'}
{'user': 'acct:liber145@hypothes.is', 'text': '赞！这个翻译清楚多了！', 'time': '2017-03-05T01:46'}
{'user': 'acct:liber145@hypothes.is', 'text': 'hidden variable 翻译成 隐变量 了，\n这里 latent variable 翻译成 潜变量', 'time': '2017-03-04T23:03'}