% !Mode:: "TeX:UTF-8"
% Translator: Yujun Li 
\chapter{直面\glsentrytext{partition_function}}
\label{chap:confronting_the_partition_function}
% 597
在\secref{sec:undirected_models}中，我们看到许多概率模型（通常是\gls{undirected_graphical_model}）由一个未归一化的\gls{PD} $\tilde{p}(\RVx, \theta)$定义。
我们必须通过除以\gls{partition_function} $Z(\Vtheta)$来归一化$\tilde{p}$，以获得一个有效的\gls{PD}：
\begin{equation}
	p(\RVx; \Vtheta) = \frac{1}{Z(\Vtheta)} \tilde{p}(\RVx; \Vtheta).
\end{equation}
\gls{partition_function}是未归一化概率所有状态的积分（对于连续变量）或求和（对于离散变量）：
\begin{equation}
	\int \tilde{p}(\Vx) d \Vx
\end{equation}
或者
\begin{equation}
	\sum_{\Vx} \tilde{p} (\Vx).
\end{equation}


对于很多有趣的模型而言，以上积分或求和难以计算。


正如我们将在\chapref{chap:deep_generative_models}看到的，有些\gls{DL}模型被设计成具有一个易于处理的归一化常数，或被设计成能够在不涉及计算$p(\RVx)$的情况下使用。
然而，其他一些模型会直接面对难以计算的\gls{partition_function}的挑战。
在本章中，我们会介绍用于训练和评估那些具有难以处理的\gls{partition_function}的模型的技术。
% -- 597 end


\section{对数似然梯度}
\label{sec:the_log_likelihood_gradient}
通过\gls{maximum_likelihood}学习\gls{undirected_model}特别困难的原因在于\gls{partition_function}依赖于参数。
对数似然相对于参数的梯度具有一项对应于\gls{partition_function}的梯度：
\begin{equation}
	\nabla_{\Vtheta} \log p(\RVx; \Vtheta) = \nabla_{\Vtheta} \log \tilde{p}(\RVx; \Vtheta) -
\nabla_{\Vtheta} \log Z(\Vtheta).
\end{equation}
% 598 head

这是机器学习中非常著名的\firstgls{positive_phase}和\firstgls{negative_phase}的分解。


对于大多数感兴趣的\gls{undirected_model}而言，\gls{negative_phase}是困难的。
没有\gls{latent_variable}或\gls{latent_variable}之间很少相互作用的模型通常会有一个易于计算的\gls{positive_phase}。
\glssymbol{RBM}\,的\gls{hidden_unit}在给定可见单元的情况下彼此条件独立，是一个典型的具有简单\gls{positive_phase}和困难\gls{negative_phase}的模型。
\gls{positive_phase}计算困难，\gls{latent_variable}之间具有复杂相互作用的情况将主要在\chapref{chap:approximate_inference}中讨论。本章主要探讨\gls{negative_phase}计算中的难点。


让我们进一步分析$\log Z$的梯度：
\begin{equation}
	\nabla_{\Vtheta} \log Z
\end{equation}
\begin{equation}
	= \frac{ \nabla_{\Vtheta} Z }{Z}
\end{equation}
\begin{equation}
	= \frac{ \nabla_{\Vtheta} \sum_{\RVx} \tilde{p}(\RVx) }{Z}
\end{equation}
\begin{equation}
	= \frac{ \sum_{\RVx} \nabla_{\Vtheta} \tilde{p}(\RVx) }{Z}.
\end{equation}


对于保证所有的$\RVx$都有$p(\RVx) > 0$的模型，我们可以用$\exp(\log \tilde{p}(\RVx))$代替$\tilde{p}(\RVx)$：
\begin{equation}
	\frac{ \sum_{\RVx} \nabla_{\Vtheta} \exp (\log \tilde{p} (\RVx)) }{ Z }
\end{equation}
\begin{equation}
	= \frac{  \sum_{\RVx}  \exp (\log \tilde{p} (\RVx)) \nabla_{\Vtheta} \log \tilde{p}(\RVx)  }{Z}
\end{equation}
\begin{equation}
	=\frac{  \sum_{\RVx} \tilde{p} (\RVx)  \nabla_{\Vtheta} \log \tilde{p}(\RVx)  }{ Z}
\end{equation}
\begin{equation}
	=\sum_{\RVx} p(\RVx) \nabla_{\Vtheta} \log \tilde{p} ( \RVx )
 \end{equation}
\begin{equation}
	=\SetE_{\RVx \sim p(\RVx)} \nabla_{\Vtheta} \log \tilde{p} (\RVx).
\end{equation}
% -- 598 end


上述推导对离散的$\Vx$进行求和，对连续的$\Vx$进行积分也可以得到类似结果。
在连续版本的推导中，使用在积分符号内取微分的\gls{leibniz_rule}可以得到等式
\begin{equation}
	\nabla_{\Vtheta} \int \tilde{p} (\RVx) d \Vx  = \int \nabla_{\Vtheta} 
\tilde{p} (\RVx) d \Vx.
\end{equation}
该等式只适用于$\tilde{p}$和$\nabla_{\Vtheta} \tilde{p} (\RVx)$上的一些特定规范条件。
在测度论术语中，这些条件是：
(1) 对每一个$\Vtheta$而言，未归一化分布$\tilde{p}$必须是$\Vx$的\gls{lebesgue_integrable}函数。
(2) 对于所有的$\Vtheta$和几乎所有$\Vx$，梯度$\nabla_{\Vtheta} \tilde{p}(\RVx)$必须存在。
(3) 对于所有的$\Vtheta$和几乎所有的$\Vx$，必须存在一个可积函数$R(\Vx)$使得$\max_i | \frac{\partial}{\partial \theta_i } \tilde{p} (\RVx) | \leq R(\Vx)$。
幸运的是，大多数感兴趣的\gls{ML}模型都具有这些性质。
% 599 mid

等式
\begin{equation}
\label{eq:18.15}
	\nabla_{\Vtheta} \log Z = \SetE_{\RVx \sim p(\RVx)} \nabla_{\Vtheta} \log \tilde{p}(\RVx)
\end{equation}
是使用各种\gls{monte_carlo}方法近似最大化（具有难计算\gls{partition_function}模型的）似然的基础。
% 599 mid

\gls{monte_carlo}方法为学习\gls{undirected_model}提供了直观的框架，我们能够在其中考虑\gls{positive_phase}和\gls{negative_phase}。
在\gls{positive_phase}中，我们增大从数据中采样得到的$\log \tilde{p}(\RVx)$。
在\gls{negative_phase}中，我们通过降低从模型分布中采样的$\log \tilde{p}(\RVx)$来降低\gls{partition_function}。
% 599 mid

在\gls{DL}文献中，经常会看到用\gls{energy_function}（\eqnref{eqn:167}）来参数化$\log \tilde{p}$。
在这种情况下，\gls{positive_phase}可以解释为压低训练样本的能量，\gls{negative_phase}可以解释为提高模型抽出的样本的能量，如\figref{fig:chap18_pos_and_neg_phase}所示。
% 599 mid

\section{随机\glsentrytext{maximum_likelihood}和\glsentrytext{contrastive_divergence}}
\label{sec:stochastic_maximum_likelihood_and_contrastive_divergence}
% 599 end
实现\eqnref{eq:18.15}的一个朴素方法是，每次需要计算梯度时，\gls{burn_in}随机初始化的一组\gls{markov_chain}。
当使用\gls{SGD}进行学习时，这意味着\gls{markov_chain}必须在每次梯度步骤中\gls{burn_in}。
这种方法引导下的训练过程如\algref{alg:naive_cd}所示。
内循环中\gls{burn_in}\gls{markov_chain}的计算代价过高，导致这个过程在实际中是不可行的，
但是这个过程是其他更加实际的近似算法的基础。
%不过该过程启发了其他计算代价较低的近似算法。

% -- 599 --
\begin{algorithm}[ht]
\caption{一种朴素的\,\glssymbol{mcmc}\,算法，使用梯度上升最大化具有难以计算\gls{partition_function}的对数似然。}
\label{alg:naive_cd}
\begin{algorithmic}
\STATE 设步长 $\epsilon$ 为一个小正数。
\STATE 设\gls{gibbs_steps} $k$ 大到足以允许\gls{burn_in}。在小图像集上训练一个~\glssymbol{RBM}~大致设为100。
\WHILE{不收敛}
\STATE 从训练集中采包含 $m$ 个样本$\{ \RVx^{(1)}, \dots, \RVx^{(m)}\}$的\gls{minibatch}。
\STATE $\RVg \leftarrow \frac{1}{m} \sum_{i=1}^m \nabla_{\Vtheta} \log \tilde{p}(\RVx^{(i)}; \Vtheta)$.
\STATE 初始化$m$ 个样本 $\{ \tilde{\RVx}^{(1)}, \dots, \tilde{\RVx}^{(m)} \}$ 为随机值（例如，从均匀或正态分布中采，或大致与模型边缘分布匹配的分布）。
\FOR{$i=1$ to $k$}
    \FOR{$j=1$ to $m$}
        \STATE $\tilde{\RVx}^{(j)} \leftarrow \text{gibbs\_update}(\tilde{\RVx}^{(j)}).$
    \ENDFOR
\ENDFOR
\STATE $\RVg \leftarrow \RVg - \frac{1}{m} \sum_{i=1}^m \nabla_{\Vtheta} \log \tilde{p}( \tilde{\RVx}^{(i)} ; \Vtheta ).$
\STATE $\Vtheta \leftarrow \Vtheta + \epsilon \RVg.$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


我们可以将最大化似然的\,\glssymbol{mcmc}\,方法视为在两种力之间平衡，一种力拉高数据出现时的模型分布，一种拉低模型采样出现时的模型分布。%??   从而最大化似然     还是  将最大似然提前？
\figref{fig:chap18_pos_and_neg_phase}展示了这个过程。
这两种力分别对应最大化$\log \tilde{p}$和最小化$\log Z$。
对于\gls{negative_phase}会有一些近似方法。
这些近似都可以被理解为使\gls{negative_phase}更容易计算，但是也可能将其推向错误的位置。%??  push down  翻成 拉低/压低  ？推向 不对吧？  push 翻成 推 可以啊

\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter18/figures/pos_and_neg_phase_color}}
\fi
\caption{\algref{alg:naive_cd}角度的``\gls{positive_phase}''和``\gls{negative_phase}''。
\emph{(左)}在\gls{positive_phase}中，我们从数据分布中采样，然后推高它们未归一化的概率。这意味着概率越高的数据点未归一化的概率被推高得越多。
\emph{(右)}在\gls{negative_phase}中，我们从模型分布中采样，然后压低它们未归一化的概率。
这与\gls{positive_phase}的倾向相反，给未归一化的概率处处添加了一个大常数。
当数据分布和模型分布相等时，\gls{positive_phase}推高数据点和\gls{negative_phase}压低数据点的机会相等。
此时，不再有任何的梯度（期望上说），训练也必须停止。}
\label{fig:chap18_pos_and_neg_phase}
\end{figure}


% -- 600 end
因为\gls{negative_phase}涉及到从模型分布中抽样，所以我们可以认为它在找模型信任度很高的点。
因为\gls{negative_phase}减少了这些点的概率，它们一般被认为代表了模型不正确的信念。
在文献中，它们经常被称为``幻觉''或``幻想粒子''。
事实上，\gls{negative_phase}已经被作为人类和其他动物做梦的一种可能解释~\citep{CrickMitchison83}。
这个想法是说，大脑维持着世界的概率模型，并且在醒着经历真实事件时会遵循$\log \tilde{p}$的梯度，在睡觉时会遵循$\log \tilde{p}$的负梯度最小化$\log Z$，其经历的样本采样自当前的模型。
这个视角解释了具有\gls{positive_phase}和\gls{negative_phase}的大多数算法，但是它还没有被神经科学实验证明是正确的。
在\gls{ML}模型中，通常有必要同时使用\gls{positive_phase}和\gls{negative_phase}，而不是按不同时间阶段分为清醒和REM睡眠时期。
正如我们将在\secref{sec:learned_approximate_inference}中看到的，一些其他\gls{ML}算法出于其他原因从模型分布中采样，这些算法也能提供睡觉做梦的解释。
% 601 mid

这样理解学习\gls{positive_phase}和\gls{negative_phase}的作用之后，我们设计了一个比\algref{alg:naive_cd}计算代价更低的替代算法。
简单的\,\glssymbol{mcmc}\,算法的计算成本主要来自每一步的随机初始化\gls{burn_in}\gls{markov_chain}。
一个自然的解决方法是初始化\gls{markov_chain}为一个非常接近模型分布的分布，从而大大减少\gls{burn_in}步骤。

% -- 601 --
\begin{algorithm}[ht]
\caption{\gls{contrastive_divergence}算法，使用梯度上升作为优化过程。}
\label{alg:cd}
\begin{algorithmic}
\STATE 设步长 $\epsilon$ 为一个小正数。
\STATE 设\gls{gibbs_steps} $k$ 大到足以让从 $p_\text{data}$初始化并从 $p(\RVx; \Vtheta)$采样的\gls{markov_chain}\gls{mixing} 。在小图像集上训练一个~\glssymbol{RBM}~大致设为1-20。
\WHILE{不收敛}
\STATE 从训练集中采包含 $m$ 个样本 $\{ \RVx^{(1)}, \dots, \RVx^{(m)}\}$ 的\gls{minibatch}。
\STATE $\RVg \leftarrow \frac{1}{m} \sum_{i=1}^m \nabla_{\Vtheta} \log \tilde{p}(\RVx^{(i)}; \Vtheta).$
    \FOR{$i=1$ to $m$}
        \STATE $\tilde{\RVx}^{(i)} \leftarrow \RVx^{(i)}.$
    \ENDFOR
\FOR{$i=1$ to $k$}
    \FOR{$j=1$ to $m$}
        \STATE $\tilde{\RVx}^{(j)} \leftarrow \text{gibbs\_update}(\tilde{\RVx}^{(j)}).$
    \ENDFOR
\ENDFOR
\STATE $\RVg \leftarrow \RVg - \frac{1}{m} \sum_{i=1}^m \nabla_{\Vtheta} \log \tilde{p}( \tilde{\RVx}^{(i)} ; \Vtheta ) .$
\STATE $\Vtheta \leftarrow \Vtheta + \epsilon \RVg.$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

% 601 end
\textbf{\gls{contrastive_divergence}}（\glssymbol{contrastive_divergence}，或者是具有$k$个Gibbs步骤的~\glssymbol{contrastive_divergence}-$k$）算法在每个步骤中初始化\gls{markov_chain}为采样自数据分布中的样本~\citep{Hinton-PoE-2000,Hinton-RBMguide-small}，如\algref{alg:cd}所示。
从数据分布中获取样本是计算代价最小的，因为它们已经在数据集中了。
初始时，数据分布并不接近模型分布，因此\gls{negative_phase}不是非常准确。
幸运的是，\gls{positive_phase}仍然可以准确地增加数据的模型概率。
进行\gls{positive_phase}阶段一段时间之后，模型分布会更接近于数据分布，并且\gls{negative_phase}开始变得准确。
% 602 mid

当然，\glssymbol{contrastive_divergence}\,仍然是真实\gls{negative_phase}的一个近似。
\glssymbol{contrastive_divergence}\,未能定性地实现真实\gls{negative_phase}的主要原因是，它不能抑制远离真实训练样本的高概率区域。
这些区域在模型上具有高概率，但是在数据生成区域上具有低概率，被称为\firstgls{spurious_modes}。
\figref{fig:chap18_spurious_mode}解释了这种现象发生的原因。
基本上，除非$k$非常大，模型分布中远离数据分布的\gls{mode}不会被使用训练数据初始化的\gls{markov_chain}访问到。
% -- 602 end



\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter18/figures/spurious_mode_color}}
\fi
\caption{一个\gls{spurious_modes}。
说明\gls{contrastive_divergence}（\algref{alg:cd}）的\gls{negative_phase}为何无法抑制\gls{spurious_modes}的例子。
一个\gls{spurious_modes}指的是一个在模型分布中出现数据分布中却不存在的模式。
由于\gls{contrastive_divergence}从数据点中初始化它的\gls{markov_chain}然后仅仅运行了几步\gls{markov_chain}，不太可能到达模型中离数据点较远的模式。
这意味着从模型中采样时，我们有时候会得到一些与数据并不相似的样本。
这也意味着由于在这些模式上浪费了一些概率质量，模型很难把较高的概率质量集中于正确的模式上。
出于可视化的目的，这个图使用了某种程度上说更加简单的距离的概念——在$\SetR$的数轴上\gls{spurious_modes}与正确的模式有很大的距离。
这对应着基于局部移动$\SetR$上的单个变量$x$的\gls{markov_chain}。
对于大部分深度概率模型来说，\gls{markov_chain}是基于\,\gls{gibbs_sampling}的，并且对于单个变量产生非局部的移动但是无法同时移动所有的变量。
对于这些问题来说，考虑编辑距离比欧式距离通常更好。
然而，高维空间的编辑距离很难在二维空间作图展示。}
\label{fig:chap18_spurious_mode}
\end{figure}


\cite{Perpinan+Hinton-2005-small}实验上证明\,\glssymbol{contrastive_divergence}\,\gls{estimator}偏向于~\glssymbol{RBM}~和完全可见的\gls{BM}，因为它会收敛到与\gls{MLE}不同的点。
他们认为，由于偏差较小，\glssymbol{contrastive_divergence}\,可以作为一种计算代价低的方式来初始化模型，之后可以通过计算代价高的\,\glssymbol{mcmc}\,方法进行\gls{fine_tune}。
\cite{Bengio+Delalleau-2009}表明，\glssymbol{contrastive_divergence}\,可以被理解为去掉了正确\,\glssymbol{mcmc}\,梯度更新中的最小项，这解释了偏差的由来。


在训练诸如\,\glssymbol{RBM}\,的浅层网络时\,\glssymbol{contrastive_divergence}\,是很有用的。
反过来，这些可以堆叠起来初始化更深的模型，如\,\glssymbol{DBN}\,或\,\glssymbol{DBM}。
但是\,\glssymbol{contrastive_divergence}\,并不直接有助于训练更深的模型。
这是因为在给定可见单元样本的情况下，很难获得\gls{hidden_unit}的样本。
由于\gls{hidden_unit}不包括在数据中，所以使用训练点初始化无法解决这个问题。
即使我们使用数据初始化可见单元，我们仍然需要\gls{burn_in}在给定这些可见单元的\gls{hidden_unit}条件分布上采样的\gls{markov_chain}。

% -- 603 end

\glssymbol{contrastive_divergence}\,算法可以被理解为惩罚某类模型，这类模型的\gls{markov_chain}会快速改变来自数据的输入。 %??   这句话总是读不通
这意味着使用\,\glssymbol{contrastive_divergence}\,训练从某种程度上说类似于训练\gls{AE}。
即使\,\glssymbol{contrastive_divergence}\,估计比一些其他训练方法具有更大偏差，但是它有助于\gls{pretraining}之后会堆叠起来的浅层模型。
这是因为堆栈中最早的模型会受激励复制更多的信息到其\gls{latent_variable}，使其可用于随后的模型。
这应该更多地被认为是\,\glssymbol{contrastive_divergence}\,训练中经常可利用的副产品，而不是主要的设计优势。


\cite{sutskever2010convergence-small}表明，\glssymbol{contrastive_divergence}\,的更新方向不是任何函数的梯度。
这使得\,\glssymbol{contrastive_divergence}\,可能存在永久循环的情况，但在实践中这并不是一个严重的问题。


另一个解决\,\glssymbol{contrastive_divergence}\,中许多问题的不同策略是，在每个梯度步骤中初始化\gls{markov_chain}为先前梯度步骤的状态值。
这个方法首先被应用数学和统计学社群发现，命名为\textbf{\gls{SML}}（\glssymbol{SML}）\citep{Younes98onthe}，后来又在\gls{DL}社群中以名称\textbf{\gls{persistent_contrastive_divergence}}（\glssymbol{persistent_contrastive_divergence}，或者每个更新中具有$k$个Gibbs步骤的\,\glssymbol{persistent_contrastive_divergence}-k）独立地被重新发现~\citep{Tieleman08-small}。
具体可以参考\algref{alg:sml}。
这种方法的基本思想是，只要随机梯度算法得到的步长很小，那么前一步骤的模型将类似于当前步骤的模型。
因此，来自先前模型分布的样本将非常接近来自当前模型分布的客观样本，用这些样本初始化的\gls{markov_chain}将不需要花费很多时间来完成\gls{mixing}。


因为每个\gls{markov_chain}在整个学习过程中不断更新，而不是在每个梯度步骤中重新开始，\gls{markov_chain}可以自由探索很远，以找到模型的所有\gls{mode}。 
因此，\glssymbol{SML}\,比\,\glssymbol{contrastive_divergence}\,更不容易形成具有\gls{spurious_modes}的模型。
此外，因为可以存储所有采样变量的状态，无论是可见的还是\gls{latent}的，\glssymbol{SML}\,为\gls{hidden_unit}和可见单元都提供了初始值。
\glssymbol{contrastive_divergence}\,只能为可见单元提供初始化，因此\gls{deep_model}需要进行\gls{burn_in}步骤。
\glssymbol{SML}\,能够高效地训练\gls{deep_model}。
\cite{Marlin10Inductive-small}将\,\glssymbol{SML}\,与本章中提出的许多其他标准方法进行比较。
他们发现，\glssymbol{SML}\,在\,\glssymbol{RBM}\,上得到了最佳的测试集对数似然，并且如果\,\glssymbol{RBM}\,的\gls{hidden_unit}被用作\,\glssymbol{SVM}\,分类器的特征，那么\,\glssymbol{SML}\,会得到最好的分类精度。
% -- 604 end

\begin{algorithm}[ht]
\caption{\gls{SML}/\gls{persistent_contrastive_divergence}算法，使用梯度上升作为优化过程。}
\label{alg:sml}
\begin{algorithmic}
\STATE 设步长 $\epsilon$ 为一个小正数。
\STATE 设\gls{gibbs_steps} $k$ 大到足以让从 $p(\RVx; \Vtheta + \epsilon \RVg)$ 采样的\gls{markov_chain}\gls{burn_in}（从采自$p(\RVx; \Vtheta)$的样本开始）。
在小图像集上训练一个\glssymbol{RBM}大致设为1，对于更复杂的模型如\gls{DBM}可能要设为5到50。
\STATE  初始化$m$ 个样本 $\{ \tilde{\RVx}^{(1)}, \dots, \tilde{\RVx}^{(m)} \}$ 为随机值（例如，从均匀或正态分布中采，或大致与模型边缘分布匹配的分布）。
\WHILE{不收敛}
\STATE 从训练集中采包含 $m$ 个样本 $\{ \RVx^{(1)}, \dots, \RVx^{(m)}\}$ 的\gls{minibatch}。
\STATE $\RVg \leftarrow \frac{1}{m} \sum_{i=1}^m \nabla_{\Vtheta} \log \tilde{p}(\RVx^{(i)}; \Vtheta).$
\FOR{$i=1$ to $k$}
    \FOR{$j=1$ to $m$}
        \STATE $\tilde{\RVx}^{(j)} \leftarrow \text{gibbs\_update}(\tilde{\RVx}^{(j)}).$
    \ENDFOR
\ENDFOR
\STATE $\RVg \leftarrow \RVg - \frac{1}{m} \sum_{i=1}^m \nabla_{\Vtheta} \log \tilde{p}( \tilde{\RVx}^{(i)} ; \Vtheta ).$
\STATE $\Vtheta \leftarrow \Vtheta + \epsilon \RVg.$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

% 605 mid
在$k$太小或$\epsilon$太大时，随机梯度算法移动模型的速率比\gls{markov_chain}在迭代步中\gls{mixing}更快，此时\,\glssymbol{SML}\,容易变得不准确。
不幸的是，这些值的容许范围高度依赖于具体问题。
现在还没有方法能够正式地测试\gls{markov_chain}是否能够在迭代步骤之间成功\gls{mixing}。
主观地，如果对于Gibbs步骤数目而言\gls{learning_rate}太大的话，那么
梯度步骤中\gls{negative_phase}采样的方差会比不同\gls{markov_chain}中\gls{negative_phase}采样的方差更大。
例如，一个MNIST模型在一个步骤中只采样得到了$7$。
然后学习过程将会极大降低$7$对应的\gls{mode}，在下一个步骤中，模型可能会只采样得到$9$。

% 605 end
从使用\,\glssymbol{SML}\,训练的模型中评估采样必须非常小心。
在模型训练完之后，有必要从一个随机起点初始化的新\gls{markov_chain}抽取样本。
用于训练的连续负相链中的样本受到了模型最近几个版本的影响，会使模型看起来具有比其实际更大的\gls{capacity}。

% -- 605 --


\cite{BerglundR13}进行了实验来检验由\,\glssymbol{contrastive_divergence}\,和\,\glssymbol{SML}\,进行梯度估计带来的偏差和方差。
结果证明\,\glssymbol{contrastive_divergence}\,比基于精确采样的\gls{estimator}具有更低的方差。
而\,\glssymbol{SML}\,有更高的方差。
\glssymbol{contrastive_divergence}\,方差低的原因是，其在\gls{positive_phase}和\gls{negative_phase}中使用了相同的训练点。
如果从不同的训练点来初始化\gls{negative_phase}，那么方差会比基于精确采样的\gls{estimator}的方差更大。


所有基于\,\glssymbol{mcmc}\,从模型中抽取样本的方法在原则上几乎可以与\,\glssymbol{mcmc}\,的任何变体一起使用。
这意味着诸如\,\glssymbol{SML}\,这样的技术可以使用\chapref{chap:monte_carlo_methods}中描述的任何增强\,\glssymbol{mcmc}\,的技术（例如\gls{parallel_tempering}）来加以改进~\citep{desjardins2010tempered,Cho10IJCNN-small}。
% 606 mid

一种在学习期间加速\gls{mixing}的方法是，不改变\gls{monte_carlo}采样技术，而是改变模型的参数化和\gls{cost_function}。
\firstgls{FPCD}，或者\,\glssymbol{FPCD}~\citep{TielemanT2009-small}使用如下表达式去替换传统模型的参数$\Vtheta$

\begin{equation}
	\Vtheta = \Vtheta^{\text{(slow)}} + \Vtheta^{\text{(fast)}}.
\end{equation}
现在的参数是以前的两倍多，将其逐个相加以定义原始模型的参数。
快速复制参数可以使用更大的\gls{learning_rate}来训练，从而使其快速响应学习的\gls{negative_phase}，并促使\gls{markov_chain}探索新的区域。
这能够使\gls{markov_chain}快速\gls{mixing}，尽管这种效应只会发生在学习期间快速权重可以自由改变的时候。
通常，在短时间地将快速权重设为大值并保持足够长时间，使\gls{markov_chain}改变\gls{mode}之后，我们会对快速权重使用显著的\gls{weight_decay}，促使它们收敛到较小的值。


本节介绍的基于\,\glssymbol{mcmc}\,的方法的一个关键优点是它们提供了$\log Z$梯度的估计，因此我们可以从本质上将问题分解为$\log \tilde{p}$和$\log Z$两块。
然后我们可以使用任何其他的方法来处理$\log \tilde{p}(\RVx)$，只需将我们的\gls{negative_phase}梯度加到其他方法的梯度中。
特别地，这意味着\gls{positive_phase}可以使用那些仅提供$\tilde{p}$下限的方法。
然而，本章介绍处理$\log Z$的大多数其他方法都和基于边界的\gls{positive_phase}方法是不兼容的。

% -- 606 end

\section{\glsentrytext{pseudolikelihood}}
\label{sec:pseudolikelihood}
\gls{monte_carlo}近似\gls{partition_function}及其梯度需要直接处理\gls{partition_function}。
有些其他方法通过训练不需要计算\gls{partition_function}的模型来绕开这个问题。
这些方法大多数都基于以下观察：无向概率模型中很容易计算概率的比率。
这是因为\gls{partition_function}同时出现在比率的分子和分母中，互相抵消：
\begin{equation}
	\frac{p(\RVx)}{ p(\RVy) } = \frac{ \frac{1}{Z} \tilde{p}(\RVx) }{ \frac{1}{Z} \tilde{p}(\RVy) } =
\frac{ \tilde{p}(\RVx) }{ \tilde{p}(\RVy) }.
\end{equation}


\gls{pseudolikelihood}正是基于\gls{conditional_probability}可以采用这种基于比率的形式，因此可以在没有\gls{partition_function}的情况下进行计算。
假设我们将$\RVx$分为$\RVa$，$\RVb$和$\RVc$，其中$\RVa$包含我们想要的条件分布的变量，$\RVb$包含我们想要条件化的变量，$\RVc$包含除此之外的变量：
\begin{equation}
	p(\RVa \mid \RVb) = \frac{ p(\RVa, \RVb) }{ p(\RVb) } = \frac{p(\RVa, \RVb)}{ \sum_{\RVa, \RVc} p(\RVa, \RVb, \RVc) } = \frac{ \tilde{p}(\RVa, \RVb) }{ \sum_{\RVa, \RVc} \tilde{p}(\RVa, \RVb, \RVc) }.
\end{equation}
以上计算需要边缘化$\RVa$，假设$\RVa$和$\RVc$包含的变量并不多，那么这将是非常高效的操作。
在极端情况下，$\RVa$可以是单个变量，$\RVc$可以为空，那么该计算仅需要估计与单个\gls{RV}值一样多的$\tilde{p}$。


不幸的是，为了计算对数似然，我们需要边缘化很多变量。
如果总共有$n$个变量，那么我们必须边缘化$n-1$个变量。
根据概率的\gls{chain_rule}，我们有
\begin{equation}
	\log p(\RVx) = \log p(x_1) + \log p(x_2 \mid x_1) + \dots + \log p(x_n \mid \RVx_{1:n−1}).
\end{equation}
在这种情况下，我们已经使$\RVa$尽可能小，但是$\RVc$可以大到$\RVx_{2:n}$。
如果我们简单地将$\RVc$移到$\RVb$中以减少计算代价，那么会发生什么呢？
这便产生了\firstgls{pseudolikelihood}~\citep{Besag75pseudolikelihood}\gls{objective_function}，给定所有其他特征$\Vx_{-i}$，预测特征$x_i$的值：
\begin{equation}
	\sum_{i=1}^n \log p(x_i \mid \Vx_{-i}).
\end{equation}


如果每个\gls{RV}有$k$个不同的值，那么计算$\tilde{p}$需要$k\times n$次估计，而计算\gls{partition_function}需要$k^n$次估计。

% -- 607 --

这看起来似乎是一个没有道理的策略，但可以证明最大化\gls{pseudolikelihood}的估计是渐近一致的~\citep{Mase1995}。
当然，在数据集不趋近于大采样极限的情况下，\gls{pseudolikelihood}可能表现出与\gls{MLE}不同的结果。
% 608 head

我们可以使用\firstgls{generalized_pseudolikelihood_estimator}来权衡计算复杂度和最大似然表现的偏差~\citep{Huang02}。
\gls{generalized_pseudolikelihood_estimator}使用$m$个不同的集合$\SetS^{(i)}$，$i=1, \dots, m$作为变量的指标出现在条件棒的左侧。
在$m = 1$和$\SetS^{(1)}= 1, \dots, n$的极端情况下，\gls{generalized_pseudolikelihood_estimator}会变为对数似然。
在$m = n$和$\SetS^{(i)} = \{i\}$的极端情况下，\gls{generalized_pseudolikelihood}会恢复为\gls{pseudolikelihood}。%??   recover the pseudolikeli
\gls{generalized_pseudolikelihood_estimator}\gls{objective_function}如下所示
\begin{equation}
	\sum_{i=1}^m \log p(\RVx_{\SetS^{(i)}} \mid \RVx_{- \SetS^{(i)} }).
\end{equation}


基于\gls{pseudolikelihood}的方法的性能在很大程度上取决于模型是如何使用的。
对于完全联合分布$p(\RVx)$模型的任务（例如密度估计和采样），\gls{pseudolikelihood}通常效果不好。
对于在训练期间只需要使用条件分布的任务而言，它的效果比最大似然更好，例如填充少量的缺失值。
如果数据具有规则结构，使得$\SetS$索引集可以被设计为表现最重要的相关性质，同时略去相关性可忽略的变量，那么\gls{generalized_pseudolikelihood}策略将会非常有效。
例如，在自然图像中，空间中相隔很远的像素也具有弱相关性，因此\gls{generalized_pseudolikelihood}可以应用于每个$\SetS$集是小的局部空间窗口的情况。
% 608 mid

\gls{pseudolikelihood}估计的一个弱点是它不能与仅在$\tilde{p}(\RVx)$上提供下界的其他近似一起使用，例如\chapref{chap:approximate_inference}中介绍的\gls{variational_inference}。这是因为$\tilde{p}$出现在了分母中。
分母的下界仅提供了整个表达式的上界，然而最大化上界没有什么意义。
这使得我们难以将\gls{pseudolikelihood}方法应用于诸如\gls{DBM}的\gls{deep_model}，因为变分方法是近似边缘化互相作用的多层\gls{hidden_variable}的主要方法之一。
尽管如此，\gls{pseudolikelihood}仍然可以用在\gls{DL}中，它可以用于单层模型，或使用不基于下界的近似推断方法的\gls{deep_model}中。

% -- 608 end

\gls{pseudolikelihood}比\,\glssymbol{SML}\,在每个梯度步骤中的计算代价要大得多，这是由于其对所有条件进行显式计算。
但是，如果每个样本只计算一个随机选择的条件，那么\gls{generalized_pseudolikelihood}和类似标准仍然可以很好地运行，从而使计算代价降低到和\,\glssymbol{SML}\,差不多的程度~\citep{Goodfellow-et-al-NIPS2013}。


虽然\gls{pseudolikelihood}估计没有显式地最小化$\log Z$，但是我们仍然认为它具有类似\gls{negative_phase}的效果。
每个条件分布的分母会使得学习算法降低所有仅具有一个变量不同于训练样本的状态的概率。


读者可以参考~\cite{Marlin11-small}了解\gls{pseudolikelihood}渐近效率的理论分析，。

% 609 mid
\section{\glsentrytext{score_matching}和\glsentrytext{ratio_matching}}
\label{sec:score_matching_and_ratio_matching}
\gls{score_matching}~\citep{Hyvarinen-2005-small}提供了另一种训练模型而不需要估计$Z$或其导数的一致性方法。
对数密度关于参数的导数$\nabla_{\Vx} \log p(\Vx)$，被称为其\firstgls{score}，\emph{\gls{score_matching}}这个名称正是来自这样的术语。
\gls{score_matching}采用的策略是，最小化模型对数密度和数据对数密度关于输入的导数之间的平方差期望：
\begin{align}
	L(\Vx, \Vtheta) & = \frac{1}{2} \norm{  \nabla_{\Vx} \log p_{\text{model}}(\Vx; \Vtheta) - \nabla_{\Vx} \log p_{\text{data}} (\Vx)  }_2^2, \\
%\end{equation}
%\begin{equation}
	J(\Vtheta) & = \frac{1}{2} \SetE_{p_{\text{data}}(\Vx)}  L(\Vx, \Vtheta), \\
%\end{equation}
%\begin{equation}
	\Vtheta^* & = \min_{\Vtheta} J(\Vtheta) .
\end{align}


该\gls{objective_function}避免了微分\gls{partition_function} $Z$带来的难题，因为$Z$不是$x$的函数，所以$\nabla_{\RVx} Z = 0$。
最初，\gls{score_matching}似乎有一个新的困难：计算数据分布的\gls{score}需要知道生成训练数据的真实分布$p_{\text{data}}$。
幸运的是，最小化$L(\Vx, \Vtheta)$的期望等价于最小化下式的期望
\begin{equation}
	\tilde{L} (\Vx, \Vtheta) = \sum_{j=1}^n \left( \frac{\partial^2}{ \partial x_j^2 } 
	\log p_{\text{model}} (\Vx; \Vtheta) + \frac{1}{2} \left( \frac{\partial}{ \partial x_j }
	\log p_{\text{model}} (\Vx; \Vtheta)
  \right)^2
\right),
\end{equation}
其中$n$是$\Vx$的维度。

% -- 609 end

因为\gls{score_matching}需要关于$\RVx$的导数，所以它不适用于具有离散数据的模型，但是模型中的\gls{latent_variable}可以是离散的。


类似于\gls{pseudolikelihood}，\gls{score_matching}只有在我们能够直接估计$\log \tilde{p}(\RVx)$及其导数的时候才有效。
它与对$\log \tilde{p}(\RVx)$仅提供下界的方法不兼容，因为\gls{score_matching}需要$\log \tilde{p}(\RVx)$的导数和二阶导数，而下限不能传达关于导数的任何信息。
这意味着\gls{score_matching}不能应用于\gls{hidden_unit}之间具有复杂相互作用的模型估计，例如\gls{sparse_coding}模型或\gls{DBM}。
虽然\gls{score_matching}可以用于\gls{pretraining}较大模型的第一个\gls{hidden_layer}，但是它没有被用于预训练较大模型的较深层网络。
这可能是因为这些模型的\gls{hidden_layer}通常包含一些离散变量。


虽然\gls{score_matching}没有明确显示具有\gls{negative_phase}信息，但是它可以被视为使用特定类型\gls{markov_chain}的\gls{contrastive_divergence}的变种~\citep{Hyvarinen-2007b}。
在这种情况下，\gls{markov_chain}并没有采用\,\gls{gibbs_sampling}，而是采用一种由梯度引导局部更新的不同方法。
当局部更新的大小接近于零时，\gls{score_matching}等价于具有这种\gls{markov_chain}的\gls{contrastive_divergence}。
% 610 mid

\cite{Lyu09}将\gls{score_matching}推广到离散的情况（但是推导有误，后由~\cite{Marlin10Inductive-small}修正）。
\cite{Marlin10Inductive-small}发现，\textbf{\gls{GSM}}（\textbf{generalized score matching}， GSM）在许多样本观测概率为$0$的高维离散空间中不起作用。


一种更成功地将\gls{score_matching}的基本想法扩展到离散数据的方法是\firstgls{ratio_matching}~\citep{Hyvarinen-2007}。
\gls{ratio_matching}特别适用于二值数据。
\gls{ratio_matching}最小化以下\gls{objective_function}在样本上的均值：
\begin{equation}
	L^{(\text{RM})} (\Vx, \Vtheta) = \sum_{j=1}^n \left( 
	\frac{1}{ 1 + \frac{ p_{\text{model}}(\Vx; \Vtheta) }{ p_{\text{model}}(f(\Vx), j; \Vtheta) } } 
\right)^2.
\end{equation}
其中$f(\Vx, j)$返回$j$处位值取反的$\RVx$。
\gls{ratio_matching}使用了与\gls{pseudolikelihood}估计相同的策略来绕开\gls{partition_function}：\gls{partition_function}会在两个概率的比率中抵消掉。
\cite{Marlin10Inductive-small}发现，
训练模型给测试集图像\gls{denoising}时，\gls{ratio_matching}的效果要优于\,\glssymbol{SML}、\gls{pseudolikelihood}和\,\glssymbol{GSM}。


% -- 610 end

类似于\gls{pseudolikelihood}估计，\gls{ratio_matching}对每个数据点都需要$n$个$\tilde{p}$的估计，因此每次更新的计算代价大约比~\glssymbol{SML}~的计算代价高出$n$倍。


与\gls{pseudolikelihood}估计一样，我们可以认为\gls{ratio_matching}减小了所有只有一个变量不同于训练样本的状态的概率。
由于\gls{ratio_matching}特别适用于二值数据，这意味着在与数据的\gls{hamming_distance}为$1$内的所有状态上，\gls{ratio_matching}都是有效的。


\gls{ratio_matching}还可以作为处理高维稀疏数据（例如词计数向量）的基础。
这类稀疏数据对基于\,\glssymbol{mcmc}\,的方法提出了挑战，因为以密集格式表示数据是非常消耗计算资源的，而只有在模型学会表示数据分布的稀疏性之后，\glssymbol{mcmc}\,采样才会产生稀疏值。
\cite{Dauphin+Bengio-NIPS2013}设计了\gls{ratio_matching}的\gls{unbiased}随机近似来解决这个问题。
该近似只估计随机选择的目标子集，不需要模型生成完整的样本。


读者可以参考~\cite{Marlin11-small}了解\gls{ratio_matching}渐近效率的理论分析，。
% 611 mid

\section{\glsentrytext{denoising_score_matching}}
\label{sec:denoising_score_matching}
某些情况下，我们希望拟合以下分布来\gls{regularize}\gls{score_matching}
\begin{equation}
	p_{\text{smoothed}}(\Vx) = \int p_{\text{data}} (\Vy) q( \Vx \mid \Vy) d \Vy
\end{equation}
而不是拟合真实分布$p_{\text{data}}$。
分布$q(\Vx \mid \Vy)$是一个损坏过程，通常在形成$\Vx$的过程中会向$\Vy$中添加少量\gls{noise}。


\gls{denoising_score_matching}非常有用，因为在实践中，通常我们不能获取真实的$p_{\text{data}}$，而只能得到其样本确定的\gls{empirical_distribution}。
给定足够\gls{capacity}，任何一致估计都会使$p_{\text{model}}$成为一组以训练点为中心的\,\gls{dirac_distribution}。
考虑在\secref{sec:consistency}介绍的渐近一致性上的\gls{loss}，通过$q$来平滑有助于缓解这个问题。
\cite{Kingma+LeCun-2010}介绍了平滑分布$q$为正态分布\gls{noise}的\gls{regularize}\gls{score_matching}。


回顾\secref{sec:estimating_the_score}，有一些\gls{AE}训练算法等价于\gls{score_matching}或\gls{denoising_score_matching}。
因此，这些\gls{AE}训练算法也是解决\gls{partition_function}问题的一种方式。

% -- 611 --

\section{\glsentrytext{NCE}}
\label{sec:noise_contrastive_estimation}
具有难求解的\gls{partition_function}的大多数模型估计都没有估计\gls{partition_function}。
\glssymbol{SML}\,和\,\glssymbol{contrastive_divergence}\,只估计对数\gls{partition_function}的梯度，而不是估计\gls{partition_function}本身。
\gls{score_matching}和\gls{pseudolikelihood}避免了和\gls{partition_function}相关的计算。 


\textbf{\gls{NCE}}\,（\textbf{noise-contrastive estimation}，\glssymbol{NCE}）\citep{Gutmann+Hyvarinen-2010}采取了一种不同的策略。
 在这种方法中，模型估计的\gls{PD}被明确表示为
\begin{equation}
	\log p_{\text{model}} (\RVx) = \log \tilde{p}_{\text{model}} (\RVx; \Vtheta) + c,
\end{equation}
其中$c$是$-\log Z(\Vtheta)$的近似。
\gls{NCE}过程将$c$视为另一参数，使用相同的算法同时估计$\Vtheta$和$c$，而不是仅仅估计$\Vtheta$，。
因此，所得到的$\log p_{\text{model}}(\RVx)$可能并不完全对应有效的\gls{PD}，但随着$c$估计的改进，它将变得越来越接近有效值\footnote{\glssymbol{NCE}~也适用于具有易于处理的，不需要引入额外参数$c$的\gls{partition_function}的问题。它已经是最令人感兴趣的，估计具有复杂\gls{partition_function}模型的方法。}。


这种方法不可能使用最大似然作为估计的标准。
最大似然标准可以设置$c$为任意大的值，而不是设置$c$以创建一个有效的\gls{PD}。


\glssymbol{NCE}~将估计$p(\RVx)$的\gls{unsupervised_learning}问题转化为学习一个概率二元分类器，其中一个类别对应模型生成的数据。
该\gls{supervised_learning}问题中的\gls{MLE}定义了原始问题的渐近一致估计。


具体地说，我们引入第二个分布，\firstgls{noise_distribution} $p_{\text{noise}}(\RVx)$。
\gls{noise_distribution}应该易于估计和从中采样。
我们现在可以构造一个联合$\RVx$和新二值变量$y$的模型。
在新的联合模型中，我们指定
\begin{equation}
	p_{\text{joint}} (y = 1) = \frac{1}{2},
\end{equation}
\begin{equation}
	p_{\text{joint}} (\RVx \mid y = 1) = p_{\text{model}} (\RVx),
\end{equation}
和
\begin{equation}
	p_{\text{joint}} (\RVx \mid y = 0) = p_{\text{noise}}(\RVx).
\end{equation}
换言之，$y$是一个决定我们从模型还是从\gls{noise_distribution}中生成$\RVx$的开关变量。

% -- 612 --

我们可以在训练数据上构造一个类似的联合模型。
在这种情况下，开关变量决定是从\,\textbf{数据}\,还是从\gls{noise}分布中抽取$\RVx$。
正式地，$p_{\text{train}}(y = 1) = \frac{1}{2}$，$p_{\text{train}}( \RVx \mid y = 1) = p_{\text{data}}( \RVx)$，和
$p_{\text{train}}(\RVx \mid y = 0) = p_{\text{noise}}(\RVx)$。


现在我们可以应用标准的最大似然学习拟合$p_{\text{joint}}$到$p_{\text{train}}$的\,\textbf{监督}\,学习问题：
\begin{equation}
	\Vtheta, c = \underset{ \Vtheta, c}{\arg\max} \SetE_{\RVx, \RSy \sim p_{\text{train}}} \log 
	p_{\text{joint}} (y \mid \RVx).
\end{equation}


分布$p_{\text{joint}}$本质上是将\gls{logistic_regression}模型应用于模型和\gls{noise_distribution}之间的对数概率之差：
\begin{equation}
	p_{\text{joint}} (y = 1 \mid \RVx) = \frac{ p_{\text{model}}(\RVx) }{ p_{\text{model}}(\RVx) + p_{\text{noise}}(\RVx) }
\end{equation}
\begin{equation}
 = \frac{1}{1 + \frac{ p_{\text{noise}}(\RVx) }{ p_{\text{model}}(\RVx) }}
\end{equation}
\begin{equation}
= \frac{1}{1 + \exp \left( \log \frac{ p_{\text{noise}}(\RVx) }{ p_{\text{model}}(\RVx) } \right)}
\end{equation}
\begin{equation}
	= \sigma \left( - \log \frac{ p_{\text{noise}} (\RVx) }{ p_{\text{model}} (\RVx) } \right)
\end{equation}
\begin{equation}
	= \sigma ( \log p_{\text{model}} (\RVx) - \log p_{\text{noise}} (\RVx)  ) .
\end{equation}


因此，只要$\log \tilde{p}_{\text{model}}$易于\gls{back_propagation}，
并且如上所述，$p_{\text{noise}}$应易于估计（以便评估$p_{\text{joint}}$）和采样（以生成训练数据），那么\,\glssymbol{NCE}\,就易于使用。


\glssymbol{NCE}\,能够非常成功地应用于\gls{RV}较少的问题，但即使\gls{RV}有很多可以取的值时，它也很有效。
例如，它已经成功地应用于给定单词上下文建模单词的条件分布~\citep{Mnih2013}。
虽然单词可以采样自一个很大的词汇表，但是只能采样一个单词。

% -- 613 --

当\,\glssymbol{NCE}\,应用于具有许多\gls{RV}的问题时，其效率会变得较低。
当\gls{logistic_regression}分类器发现某个变量的取值不大可能时，它会拒绝这个\gls{noise}样本。
这意味着在$p_{\text{model}}$学习了基本的边缘统计之后，学习进程会大大减慢。
想象一个使用非结构化高斯\gls{noise}作为$p_{\text{noise}}$来学习面部图像的模型。
如果$p_{\text{model}}$学会了眼睛，就算没有学习任何其他面部特征，比如嘴， 它也会拒绝几乎所有的非结构化\gls{noise}样本。
% 614 head

\gls{noise_distribution}~$p_{\text{noise}}$必须是易于估计和采样的约束可能是过于严格的限制。
当$p_{\text{noise}}$比较简单时，大多数采样可能与数据有着明显不同，而不会迫使$p_{\text{model}}$进行显著改进。


类似于\gls{score_matching}和\gls{pseudolikelihood}，如果$\tilde{p}$只有下界，那么\,\glssymbol{NCE}\,不会有效。
这样的下界能够用于构建$p_{\text{joint}}( y = 1 \mid \RVx)$的下界，但是它只能用于构建$p_{\text{joint}}(y = 0 \mid \RVx)$（出现在一半的\,\glssymbol{NCE}\,对象中）的上界。
同样地，$p_{\text{noise}}$的下界也没有用，因为它只提供了$p_{\text{joint}}( y = 1 \mid \RVx)$的上界。


在每个梯度步骤之前，模型分布被复制来定义新的\gls{noise_distribution}时，\glssymbol{NCE}\,定义了一个被称为\firstgls{self_contrastive_estimation}的过程，其梯度期望等价于最大似然的梯度期望~\citep{Goodfellow-ICLR2015}。
特殊情况的\,\glssymbol{NCE}（\gls{noise}采样由模型生成）表明最大似然可以被解释为使模型不断学习以将现实与自身发展的信念区分的过程，而\gls{NCE}通过让模型区分现实和固定的基准（\gls{noise}模型），我们降低了计算成本。%??  以  是不是多余的？


在训练样本和生成样本（使用模型能量函数定义分类器）之间进行分类以得到模型的梯度的方法，已经在更早的时候以各种形式提出来~\citep{Welling2003b,Bengio-2009-book}。


\gls{NCE}是基于良好\gls{generative_model}应该能够区分数据和\gls{noise}的想法。
一个密切相关的想法是，良好的\gls{generative_model}能够生成分类器无法将其与数据区分的样本。
这个想法诞生了\gls{generative_adversarial_networks}（\secref{sec:generative_adversarial_networks}）。


\section{估计\glsentrytext{partition_function}}
\label{sec:estimating_the_partition_function}
尽管本章中的大部分内容都在避免计算与\gls{undirected_graphical_model}相关的难以计算的\gls{partition_function} $Z(\Vtheta)$，但在本节中我们将会讨论几种直接估计\gls{partition_function}的方法。

% -- 614 --

估计\gls{partition_function}可能会很重要，当我们希望计算数据的归一化似然时，我们会需要它。
在\emph{评估}模型，监控训练性能，和比较模型时，这通常是很重要的。


例如，假设我们有两个模型：\gls{PD}为$p_A(\RVx; \Vtheta_A)= \frac{1}{Z_A} \tilde{p}_A(\RVx; \Vtheta_A)$的模型$\CalM_A$和\gls{PD}为$p_B(\RVx; \Vtheta_B)= \frac{1}{Z_B} \tilde{p}_B(\RVx; \Vtheta_B)$的模型$\CalM_B$。
比较模型的常用方法是评估和比较两个模型分配给\gls{iid_chap18}测试数据集的似然。
假设测试集含$m$个样本$\{ \Vx^{(1)}, \dots, \Vx^{(m)} \}$。
如果 $\prod_i p_A ( \RSx^{(i)}; \Vtheta_A) > \prod_i p_B( \RSx^{(i)}; \Vtheta_B)$，或等价地，如果
\begin{equation}
\label{eq:18.38}
	\sum_i \log p_A (\RSx^{(i)}; \Vtheta_A) - \sum_i \log p_B(\RSx^{(i)}; \Vtheta_B) > 0,
\end{equation}
那么我们说$\CalM_A$是一个比$\CalM_B$更好的模型（或者，至少可以说，它在测试集上是一个更好的模型），这是指它有一个更好的测试对数似然。
不幸的是，测试这个条件是否成立需要知道\gls{partition_function}。
\eqnref{eq:18.38}看起来需要估计模型分配给每个点的对数概率，因而需要估计\gls{partition_function}。
我们可以通过将\eqnref{eq:18.38}重新转化为另一种形式来简化情况，在该形式中我们只需要知道两个模型的\gls{partition_function}的\textbf{比率}：
\begin{equation}
\label{eq:18.39}
	\sum_i \log p_A(\RVx^{(i)}; \Vtheta_A) - \sum_i \log p_B(\RVx^{(i)}; \Vtheta_B) =
	\sum_i \left(  \log \frac{ \tilde{p}_A(\RVx^{(i)}; \Vtheta_A) }{ \tilde{p}_B(\RVx^{(i)}; \Vtheta_B) } \right)  - m\log \frac{Z(\Vtheta_A)}{ Z(\Vtheta_B) }.
\end{equation}
因此，我们可以在不知道任一模型的\gls{partition_function}，而只知道它们比率的情况下，判断模型$\CalM_A$是否比模型$\CalM_B$更优。
正如我们将很快看到的，在两个模型相似的情况下，我们可以使用\gls{importance_sampling}来估计比率。


然而，如果我们想要计算测试数据在$\CalM_A$或$\CalM_B$上的真实概率，我们需要计算\gls{partition_function}的真实值。
如果我们知道两个\gls{partition_function}的比率，$r = \frac{Z(\Vtheta_B)}{ Z(\Vtheta_A) }$，并且我们知道两者中一个的实际值，比如说$Z(\Vtheta_A)$，那么我们可以计算另一个的值：
\begin{equation}
	Z(\Vtheta_B) = r Z(\Vtheta_A) = \frac{ Z(\Vtheta_B) }{ Z(\Vtheta_A) } Z(\Vtheta_A).
\end{equation}


一种估计\gls{partition_function}的简单方法是使用\gls{monte_carlo}方法，例如简单\gls{importance_sampling}。
以下用连续变量积分来表示该方法，也可以替换积分为求和，很容易将其应用到离散变量的情况。
我们使用\gls{proposal_distribution} $p_0(\RVx) = \frac{1}{Z_0} \tilde{p}_0( \RVx)$，其在\gls{partition_function} $Z_0$和未归一化分布$\tilde{p}_0(\RVx)$上易于采样和估计。

% -- 615 --

\begin{align}
	Z_1 &= \int \tilde{p}_1 (\RVx) d \RVx \\
	&= \int  \frac{ p_0(\RVx) }{ p_0(\RVx) }   \tilde{p}_1 (\RVx) d \RVx \\
	&= Z_0 \int  p_0(\RVx)   \frac{ \tilde{p}_1 (\RVx) }{ \tilde{p}_0 (\RVx) } d \RVx \\
	\hat{Z}_1 &= \frac{Z_0}{K} \sum_{k=1}^K \frac{ \tilde{p}_1(\RVx^{(k)})  }{ \tilde{p}_0(\RVx^{(k)}) }  \quad\quad \text{s.t.}: \RVx^{(k)} \sim p_0 \label{eq:18.44}
\end{align}

在最后一行，我们使用\gls{monte_carlo}估计，使用从$p_0(\RVx)$中抽取的采样计算积分$\hat{Z}_1$，然后用未归一化的$\tilde{p}_1$和\gls{proposal_distribution} $p_0$的比率对每个采样加权。


这种方法使得我们可以估计\gls{partition_function}之间的比率：
\begin{equation}
	\frac{1}{K} \sum_{k=1}^K \frac{ \tilde{p}_1 (\RVx^{(k)}) }{ \tilde{p}_0 (\RVx^{(k)}) }
	\quad\quad \text{s.t.}: \RVx^{(k)} \sim p_0.
\end{equation}
然后该值可以直接比较\eqnref{eq:18.39}中的两个模型。


如果分布$p_0$接近$p_1$，那么\eqnref{eq:18.44}能够有效地估计\gls{partition_function}~\citep{Minka_2005}。
不幸的是，大多数时候$p_1$都很复杂（通常是\gls{multimodal}的），并且定义在高维空间中。
很难找到一个易求解的$p_0$，既能易于评估，又能充分接近$p_1$以保持高质量的近似。
如果$p_0$和$p_1$不接近，那么$p_0$的大多数采样将在$p_1$中具有较低的概率，从而在\eqnref{eq:18.44}的求和中产生（相对的）可忽略的贡献。


如果求和中只有少数几个具有显著权重的样本，那么将会由于高方差而导致估计的效果很差。
这可以通过估计$\hat{Z}_1$的方差来定量地理解：
\begin{equation}
	\hat{\text{Var}} \left( \hat{Z}_1 \right)  = \frac{Z_0 }{K^2} \sum_{k=1}^K
\left(  \frac{ \tilde{p}_1(\RVx^{(k)}) }{  \tilde{p}_0(\RVx^{(k)}) } - \hat{Z}_1  \right)^2.
\end{equation}
当重要性权重$\frac{ \tilde{p}_1(\RVx^{(k)}) }{ \tilde{p}_0(\RVx^{(k)}) } $存在显著偏差时，上式的值是最大的。

% -- 616 --

我们现在关注两个解决高维空间复杂分布上估计\gls{partition_function}的方法：
\gls{AIS}和\gls{bridge_sampling}。
两者都始于上面介绍的简单\gls{importance_sampling}方法，并且都试图通过引入\emph{缩小}$p_0$和$p_1$之间\emph{差距}的中间分布，来解决$p_0$远离$p_1$的问题。


\subsection{\glsentrytext{AIS}}
\label{sec:annealed_importance_sampling}
在$D_{KL}(p_0 \| p_1)$很大的情况下（即$p_0$和$p_1$之间几乎没有重叠），一种称为\textbf{\gls{AIS}}（\textbf{annealed importance sampling}，AIS）的方法试图通过引入中间分布来缩小这种差距~\citep{Jarzynski1997,Neal-2001}。
考虑分布序列$p_{\eta_0},\dots,p_{\eta_n}$，其中$0=\eta_0 < \eta_1 < \dots < \eta_{n-1} < \eta_n = 1$，分布序列中的第一个和最后一个分别是$p_0$和$p_1$。


这种方法使我们能够估计定义在高维空间多峰分布（例如训练~\glssymbol{RBM}~时定义的分布）上的\gls{partition_function}。
我们从一个已知\gls{partition_function}的简单模型（例如，权重为零的\,\glssymbol{RBM}）开始，估计两个模型\gls{partition_function}之间的比率。
该比率的估计基于许多个相似分布的比率估计，例如在零和学习到的权重之间插值一组权重不同的\,\glssymbol{RBM}。


现在我们可以将比率$\frac{Z_1}{Z_0}$写作
\begin{align}
\frac{Z_1}{Z_0} &= \frac{Z_1}{Z_0} \frac{Z_{\eta_1}}{Z_{\eta_1}} \dots \frac{Z_{\eta_{n-1}}}{Z_{\eta_{n-1}}} \\
&= \frac{Z_{\eta_1}}{Z_{0}}  \frac{Z_{\eta_2}}{Z_{\eta_1}}  \dots \frac{Z_{\eta_{n-1}}}{Z_{\eta_{n-2}}} \frac{Z_{1}}{Z_{\eta_{n-1}}} \\
&= \prod_{j=0}^{n-1} \frac{ Z_{\eta_{j+1}} }{Z_{\eta_j}}. \label{eq:18.49}
\end{align}
如果对于所有的$0 \leq j \leq n-1$，分布$p_{\eta_j}$和$p_{\eta_{j+1}}$足够接近，那么我们能够使用简单的\gls{importance_sampling}来估计每个因子$\frac{Z_{\eta_{j+1}}}{ Z_{\eta_j}}$，然后使用这些得到$\frac{Z_1}{Z_0}$的估计。

% -- 617 --

这些中间分布是从哪里来的呢？
正如最先的\gls{proposal_distribution} $p_0$是一种设计选择，分布序列$p_{\eta_1} \dots p_{\eta_{n-1}}$也是如此。
也就是说，它们可以被特别设计为特定的问题领域。
中间分布的一个通用和流行选择是使用目标分布$p_1$的加权几何平均，起始分布（其\gls{partition_function}是已知的）为$p_0$：
\begin{equation}
	p_{\eta_j} \propto p_1^{\eta_j} p_0^{1-\eta_j}.
\end{equation}


为了从这些中间分布中采样，我们定义了一组\gls{markov_chain}转移函数$T_{\eta_j}(\Vx' \mid \Vx)$，定义了给定$\Vx$转移到$\Vx'$的\gls{conditional_probability_distribution}。
转移算子$T_{\eta_j}(\Vx' \mid \Vx)$定义如下，保持$p_{\eta_j}(\Vx)$不变：
\begin{equation}
	p_{\eta_j}(\Vx) = \int p_{\eta_j} (\Vx') T_{\eta_j} (\Vx \mid \Vx') d\Vx'.
\end{equation}
这些转移可以被构造为任何\gls{mcmc}方法（例如，Metropolis-Hastings，Gibbs），包括涉及多次遍历所有\gls{RV}或其他迭代的方法。


然后，\glssymbol{AIS}\,采样方法从$p_0$开始生成样本，并使用转移算子从中间分布顺序地生成采样，直到我们得到目标分布$p_1$的采样：
\begin{itemize}
	\item 对于 $k=1 \dots K$ 
		\newline
		\quad\quad -- 采样 $ \Vx_{\eta_1}^{(k)} \sim p_0(\RVx) $
		\newline
		\quad\quad -- 采样 $ \Vx_{\eta_2}^{(k)} \sim T_{\eta_1}(\RVx_{\eta_2}^{(k)} \mid \Vx_{\eta_1}^{(k)} ) $
		\newline
		\quad\quad -- $\dots$
		\newline
		\quad\quad -- 采样 $ \Vx_{\eta_{n-1}}^{(k)} \sim T_{\eta_{n-2}}(\RVx_{\eta_{n-1}}^{(k)} \mid \Vx_{\eta_{n-2}}^{(k)} ) $
		\newline
		\quad\quad -- 采样 $ \Vx_{\eta_n}^{(k)} \sim T_{\eta_{n-1}}(\RVx_{\eta_n}^{(k)} \mid \Vx_{\eta_{n-1}}^{(k)} ) $
	\item 结束
\end{itemize}


对于采样$k$，通过连接\eqnref{eq:18.49}给出的中间分布之间的重要性权重，我们可以导出目标重要性权重：
\begin{equation}
\label{eq:18.52}
	w^{(k)} = \frac{ \tilde{p}_{\eta_1} ( \Vx_{\eta_1}^{(k)} )  }{  \tilde{p}_{0} ( \Vx_{\eta_1}^{(k)} )  }
\frac{ \tilde{p}_{\eta_2} ( \Vx_{\eta_2}^{(k)} )  }{  \tilde{p}_{\eta_1} ( \Vx_{\eta_2}^{(k)} )  }
\dots
\frac{ \tilde{p}_{1} ( \Vx_{1}^{(k)} )  }{  \tilde{p}_{\eta_{n-1}} ( \Vx_{\eta_n}^{(k)} )  } .
\end{equation}
为了避免诸如\gls{overflow}的数值问题，最佳方法可能是通过加法或减法计算$\log w^{(k)}$，而不是通过概率乘法和除法计算$w^{(k)}$。

% -- 618 --

利用由此定义的采样过程和\eqnref{eq:18.52}中给出的重要性权重，\gls{partition_function}的比率估计如下所示：
\begin{equation}
	\frac{Z_1}{Z_0} \approx \frac{1}{K} \sum_{k=1}^K w^{(k)}
\end{equation}


为了验证该过程定义的\gls{importance_sampling}方案是否有效，我们可以展示\citep{Neal-2001}~\glssymbol{AIS}~过程对应着扩展状态空间上的简单\gls{importance_sampling}，其中数据点采样自乘积空间$[\Vx_{\eta_1},\dots,\Vx_{\eta_{n-1}},\Vx_1]$。
为此，我们将扩展空间上的分布定义为
\begin{align}
&\tilde{p} (\Vx_{\eta_1}, \dots, \Vx_{\eta_{n-1}}, \Vx_1) \\
= &\tilde{p}_1 (\Vx_1) \tilde{T}_{\eta_{n-1}} (\Vx_{\eta_{n-1}} \mid \Vx_1)
 	\tilde{T}_{\eta_{n-2}}  (\Vx_{\eta_{n-2}} \mid \Vx_{\eta_{n-1}}) \dots
 	\tilde{T}_{\eta_1} (\Vx_{\eta_1} \mid \Vx_{\eta_2}) \label{eq:18.55} ,
\end{align}
其中$\tilde{T}_a$是由$T_a$定义的转移算子的逆（应用\gls{bayes_rule}）：
\begin{equation}
	\tilde{T}_a (\Vx' \mid \Vx) = \frac{p_a(\Vx')}{ p_a(\Vx) } T_a(\Vx \mid \Vx') = 
\frac{  \tilde{p}_a(\Vx')}{ \tilde{p}_a(\Vx) } T_a(\Vx \mid \Vx') .
\end{equation}
将以上代入到\eqnref{eq:18.55}给出的扩展状态空间上的联合分布中，我们得到：
\begin{align}
	&\tilde{p} (\Vx_{\eta_1}, \dots, \Vx_{\eta_{n-1}}, \Vx_1) \\
	= &\tilde{p}_1 (\Vx_1) \frac{ \tilde{p}_{\eta_{n-1}} (\Vx_{\eta_{n-1}})  }{ \tilde{p}_{\eta_{n-1}}(\Vx_1)} T_{\eta_{n-1}} (\Vx_1 \mid \Vx_{\eta_{n-1}})
\prod_{i=1}^{n-2} \frac{ \tilde{p}_{\eta_i}(\Vx_{\eta_i}) }{ \tilde{p}_{\eta_i}(\Vx_{\eta_{i+1}})} T_{\eta_i} (\Vx_{\eta_{i+1}} \mid \Vx_{\eta_i}) \\
	= &\frac{ \tilde{p}_1(\Vx_1) }{ \tilde{p}_{\eta_{n-1}}(\Vx_1) } T_{\eta_{n-1}} (\Vx_1 \mid \Vx_{\eta_{n-1}})
\tilde{p}_{\eta_1} (\Vx_{\eta_1}) \prod_{i=1}^{n-2} \frac{ \tilde{p}_{\eta_{i+1}}(\Vx_{\eta_{i+1}}) }{ \tilde{p}_{\eta_i}(\Vx_{\eta_{i+1}}) } T_{\eta_i} (\Vx_{\eta_{i+1}} \mid \Vx_{\eta_i}) . \label{eq:18.59}
\end{align}
通过上面给定的采样方案，现在我们可以从扩展样本上的联合\gls{proposal_distribution} $q$上生成采样，联合分布如下
\begin{equation}
	q(\Vx_{\eta_1}, \dots, \Vx_{\eta_{n-1}}, \Vx_1)  = p_0(\Vx_{\eta_1}) T_{\eta_1}( \Vx_{\eta_2} \mid \Vx_{\eta_1} ) \dots T_{\eta_{n-1}} (\Vx_1 \mid \Vx_{\eta_{n-1}}) .
\end{equation}
\eqnref{eq:18.59}给出了扩展空间上的联合分布。
将$q(\Vx_{\eta_1}, \dots, \Vx_{\eta_{n-1}}, \Vx_1)$作为扩展状态空间上的\gls{proposal_distribution}（我们会从中抽样），重要性权重如下
\begin{equation}
	w^{(k)} = \frac{ \tilde{p}(\Vx_{\eta_1}, \dots, \Vx_{\eta_{n-1}}, \Vx_1) }{ q( \Vx_{\eta_1}, \dots, \Vx_{\eta_{n-1}}, \Vx_1 ) } =
\frac{ \tilde{p}_1(\Vx_1^{(k)}) }{ \tilde{p}_{\eta_{n-1}}(\Vx_{\eta_{n-1}}^{(k)}) } \dots
\frac{ \tilde{p}_{\eta_{2}}(\Vx_{\eta_{2}}^{(k)}) }{ \tilde{p}_{\eta_1}(\Vx_{\eta_{1}}^{(k)}) } 
\frac{ \tilde{p}_{\eta_{1}}(\Vx_{\eta_{1}}^{(k)}) }{ \tilde{p}_{0}(\Vx_{0}^{(k)}) } .
\end{equation}
这些权重和\,\glssymbol{AIS}\,上的权重相同。
因此，我们可以将\,\glssymbol{AIS}\,解释为应用于扩展状态上的简单\gls{importance_sampling}，其有效性直接来源于\gls{importance_sampling}的有效性。

% -- 619 --

\gls{AIS}首先由~\cite{Jarzynski1997}发现，然后由~\cite{Neal-2001}再次独立发现。
目前它是估计无向概率模型的\gls{partition_function}的最常用方法。
其原因可能与一篇有影响力的论文~\citep{Salakhutdinov+Murray-2008}有关，该论文并没有讨论该方法相对于其他方法的优点，而是介绍了将其应用于估计\gls{RBM}和\gls{DBN}的\gls{partition_function}。


关于\,\glssymbol{AIS}\,估计性质（例如，方差和效率）的讨论，请参看~\cite{Neal-2001}。


\subsection{\glsentrytext{bridge_sampling}}
\label{sec:bridge_sampling}
类似于\,\glssymbol{AIS}，\gls{bridge_sampling}~\citep{Bennet76}是另一种处理\gls{importance_sampling}缺点的方法。
并非将一系列中间分布连接在一起，\gls{bridge_sampling}依赖于单个分布$p_*$（被称为桥），在已知\gls{partition_function}的分布$p_0$和分布$p_1$（我们试图估计其\gls{partition_function} $Z_1$）之间插值。


\gls{bridge_sampling}估计比率$Z_1 / Z_0$：$\tilde{p}_0$和$\tilde{p}_*$之间重要性权重期望与$\tilde{p}_1$和$\tilde{p}_*$之间重要性权重的比率，
\begin{equation}
	\frac{Z_1}{ Z_0} \approx \sum_{k=1}^K \frac{ \tilde{p}_*(\Vx_0^{(k)}) }{ \tilde{p}_0(\Vx_0^{(k)}) } \bigg/ \sum_{k=1}^K \frac{ \tilde{p}_*(\Vx_1^{(k)}) }{ \tilde{p}_1(\Vx_1^{(k)}) } .
\end{equation}
如果仔细选择\gls{bridge_sampling} $p_*$，使其与$p_0$和$p_1$都有很大重合的话，那么\gls{bridge_sampling}能够允许两个分布（或更正式地，$D_{\text{KL}}(p_0 \| p_1)$）之间有较大差距（相对标准\gls{importance_sampling}而言）。

% -- 620 --

可以表明，最优的\gls{bridge_sampling}是$p_*^{(opt)} (\RVx) \propto \frac{ \tilde{p}_0(\Vx) \tilde{p}_1(\Vx) }{ r\tilde{p}_0(\Vx) + \tilde{p}_1(\Vx) }$，其中$r = Z_1 / Z_0$。
这似乎是一个不可行的解决方案，因为它似乎需要我们估计数值$Z_1 / Z_0$。
然而，可以从粗糙的$r$开始估计，然后使用得到的\gls{bridge_sampling}逐步迭代以改进估计~\citep{Neal05estimatingratios}。
也就是说，我们会迭代地重新估计比率，并使用每次迭代更新$r$的值。


\paragraph{\gls{linked_importance_sampling}}
\glssymbol{AIS}\,和\gls{bridge_sampling}各有优点。
如果$D_{\text{KL}}(p_0 \| p_1)$不太大（由于$p_0$和$p_1$足够接近）的话，那么\gls{bridge_sampling}能比\,\glssymbol{AIS}\,更高效地估计\gls{partition_function}比率。
然而，如果对于单个分布$p_*$而言，两个分布相距太远难以桥接差距，那么\,\glssymbol{AIS}\,至少可以使用许多潜在中间分布来跨越$p_0$和$p_1$之间的差距。
\cite{Neal05estimatingratios}展示\gls{linked_importance_sampling}方法如何利用\gls{bridge_sampling}的优点，桥接\,\glssymbol{AIS}\,中使用的中间分布，并且显著改进了整个\gls{partition_function}的估计。


\paragraph{在训练期间估计\gls{partition_function}}
虽然\,\glssymbol{AIS}\,已经被认为是用于估计许多无向模型\gls{partition_function}的标准方法，但是它在计算上代价很高，以致其在训练期间仍然不很实用。
研究者探索了一些在训练过程中估计\gls{partition_function}的替代方法。


使用\gls{bridge_sampling}、短链\,\glssymbol{AIS}\,和\gls{parallel_tempering}的组合，\cite{Desjardins+al-NIPS2011}设计了一种在训练过程中追踪\,\glssymbol{RBM}\,\gls{partition_function}的方法。
该策略的基础是，在\gls{parallel_tempering}方法操作的每个温度下，\glssymbol{RBM}\,\gls{partition_function}的独立估计会一直保持。
作者将相邻链（来自\gls{parallel_tempering}）的\gls{partition_function}比率的\gls{bridge_sampling}估计和跨越时间的\,\glssymbol{AIS}\,估计组合起来，提出一个在每次迭代学习时估计\gls{partition_function}的（且方差较小的）方法。


本章中描述的工具提供了许多不同的方法，以解决难处理的\gls{partition_function}问题，但是在训练和使用\gls{generative_model}时，可能会存在一些其他问题。
其中最重要的是我们接下来会遇到的难以推断的问题。

% -- 621 --
