{'user': 'acct:zhiding@hypothes.is', 'text': '卷积神经网络', 'origin_text': '看作是二维的像素网格）。卷积网络在诸多应用领域都表现优异。“  !!!卷积网络!!!  ”一词表明该网络使用了卷积这种数学运算。卷积是一种特殊的线性运', 'time': '2017-01-12T07:43'}
{'user': 'acct:zhiding@hypothes.is', 'text': '然后我们会介绍池化，这是一种几乎所有卷积网络都会用到的操作。', 'origin_text': '么是卷积运算。接着，我们会解释在神经网络中使用卷积运算的动机。  !!!然后我们会介绍一种几乎所有的卷积神经网络都会用到的操作池化。!!!  通常来说，卷积神经网络中用到的卷积运算和其他领域（例如工程领域', 'time': '2017-01-12T07:48'}
{'user': 'acct:zhiding@hypothes.is', 'text': '广泛应用', 'origin_text': '域以及纯数学领域）中的定义并不完全一致。我们会对神经网络实践中  !!!用得比较多!!!  的几种卷积函数的变体进行说明。我们也会说明如何在多种不同维数的', 'time': '2017-01-12T07:54'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积网络是神经科学的原理中影响着深度学习的典型代表', 'origin_text': '上使用卷积运算。之后我们讨论使得卷积运算更加高效的一些方法。  !!!卷积神经网络是神经科学的原理影响深度学习的典型代表!!!  ，我们之后也会讨论这些神经科学的原理，并对卷积神经网络在深度学习', 'time': '2017-01-12T07:59'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积网络', 'origin_text': '影响深度学习的典型代表，我们之后也会讨论这些神经科学的原理，并对  !!!卷积神经网络!!!  在深度学习发展史中的作用作出评价。本章没有涉及如何为你的卷积神', 'time': '2017-01-12T08:00'}
{'user': 'acct:zhiding@hypothes.is', 'text': '因为本章的目标是说明卷积网络所提供的各种工具，第\\?章将会对如何在具体环境中选择使用相应的工具给出通用的准则。', 'origin_text': '用作出评价。本章没有涉及如何为你的卷积神经网络选择合适的结构，  !!!因为本章的目标是说明卷积神经网络提供的强大工具，第\\?章会对在具体环境中使用相应的工具给出一些指导。!!!  对于卷积网络结构的研究进展得如此迅速，以至于针对特定问题，数月', 'time': '2017-01-12T08:16'}
{'user': 'acct:zhiding@hypothes.is', 'text': '基准测试', 'origin_text': '一些指导。对于卷积网络结构的研究进展得如此迅速，以至于针对特定  !!!问题!!!  ，数月甚至几周就会产生一个新的最优的网络结构，甚至在写这本书时也', 'time': '2017-01-12T08:20'}
{'user': 'acct:zhiding@hypothes.is', 'text': '公开', 'origin_text': '络结构的研究进展得如此迅速，以至于针对特定问题，数月甚至几周就会  !!!产生!!!  一个新的最优的网络结构，甚至在写这本书时也不好描述究竟哪种结构是', 'time': '2017-01-12T08:20'}
{'user': 'acct:zhiding@hypothes.is', 'text': '受到了噪声干扰', 'origin_text': '可以在任意时刻从传感器中读出飞船的位置。现在假设我们的传感器  !!!含有噪声!!!  。为了得到飞船位置的低噪声估计，我们对得到的测量结果进行平均。', 'time': '2017-01-12T08:26'}
{'user': 'acct:zhiding@hypothes.is', 'text': '距', 'origin_text': '加权函数w(a)w(a)w(a) 来实现，其中aaa表示测量结果  !!!据!!!  当前时刻的时间间隔。如果我们对任意时刻都采用这种加权平均的操作', 'time': '2017-01-12T08:32'}
{'user': 'acct:zhiding@hypothes.is', 'text': '就得到了一个新的对于飞船位置的平滑估计函数', 'origin_text': '前时刻的时间间隔。如果我们对任意时刻都采用这种加权平均的操作，  !!!就得到了对于飞船位置的连续估计函数!!!  sss：s(t)=∫x(a)w(t−a)da.s(t)=∫x(', 'time': '2017-01-12T08:35'}
{'user': 'acct:zhiding@hypothes.is', 'text': '在参数为负值时，$w$的取值必须为$0$', 'origin_text': '是一个有效的概率密度函数，否则输出就不再是一个加权平均。另外，  !!!www在参数为负值时必须为0!!!  ，否则它会涉及到未来，这不是我们能够做到的。但这些限制仅仅是对', 'time': '2017-01-12T08:41'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这不是我们能够推测得了的。', 'origin_text': '平均。另外，www在参数为负值时必须为0，否则它会涉及到未来，  !!!这不是我们能够做到的!!!  。但这些限制仅仅是对我们这个例子来说。通常，卷积被定义在满足', 'time': '2017-01-12T08:43'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积网络\n', 'origin_text': '述积分式的任意函数上，并且也可能被用于加权平均以外的目的。在  !!!卷积神经网络!!!  的术语中，第一个参数（在这个例子中，函数xxx）叫做输入，第二个', 'time': '2017-01-12T08:48'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积的第一个参数（在这个例子中，函数$x$）叫做输入', 'origin_text': '并且也可能被用于加权平均以外的目的。在卷积神经网络的术语中，  !!!第一个参数（在这个例子中，函数xxx）叫做输入!!!  ，第二个参数（函数www）叫做核函数。输出有时被称作特征映射。', 'time': '2017-01-12T08:54'}
{'user': 'acct:zhiding@hypothes.is', 'text': '激光传感器在每个瞬间反馈测量结果的想法是不切实际的', 'origin_text': '）叫做核函数。输出有时被称作特征映射。在我们的例子中，  !!!激光传感器能够在任意时刻给出测量结果的想法是不现实的!!!  。一般地，当我们用计算机处理数据时，时间会被离散化，传感器会给', 'time': '2017-01-12T16:44'}
{'user': 'acct:zhiding@hypothes.is', 'text': '传感器会在定期地反馈数据', 'origin_text': '是不现实的。一般地，当我们用计算机处理数据时，时间会被离散化，  !!!传感器会给出特定时间间隔的数据!!!  。所以比较现实的的假设是传感器每秒给出一次测量结果，这样，时间', 'time': '2017-01-12T16:45'}
{'user': 'acct:zhiding@hypothes.is', 'text': '所以在我们的例子中，假设传感器每一秒反馈一次测量结果是比较现实的。', 'origin_text': '机处理数据时，时间会被离散化，传感器会给出特定时间间隔的数据。  !!!所以比较现实的的假设是传感器每秒给出一次测量结果，!!!  这样，时间ttt只能取整数值。如果我们假设xxx和www都定义', 'time': '2017-01-12T16:49'}
{'user': 'acct:zhiding@hypothes.is', 'text': '时刻', 'origin_text': '数据。所以比较现实的的假设是传感器每秒给出一次测量结果，这样，  !!!时间!!!  ttt只能取整数值。如果我们假设xxx和www都定义在整数时刻', 'time': '2017-01-12T16:54'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这些高维数组', 'origin_text': '入通常是高维数据数组，而核也是由算法产生的高维参数数组。我们把  !!!这种高维数组!!!  叫做张量。因为输入与核的每一个元素都分开存储，我们经常假设在存', 'time': '2017-01-12T17:08'}
{'user': 'acct:zhiding@hypothes.is', 'text': '通常', 'origin_text': '这种高维数组叫做张量。因为输入与核的每一个元素都分开存储，我们  !!!经常!!!  假设在存储了数据的有限点集以外，这些函数的值都为零。这意味着在', 'time': '2017-01-12T17:17'}
{'user': 'acct:zhiding@hypothes.is', 'text': '我们有时在多个维度上进行卷积运算', 'origin_text': '可以统一地把无限的求和当作对有限个数组元素的求和来用。最后，  !!!我们有时对多个维度进行卷积运算!!!  。例如，如果把二维的图像III作为输入，我们也相应的需要使用二', 'time': '2017-01-12T17:27'}
{'user': 'acct:zhiding@hypothes.is', 'text': '我们可以通过对有限个数组元素的求和来实现无限求和', 'origin_text': '数据的有限点集以外，这些函数的值都为零。这意味着在实际操作中，  !!!我们可以统一地把无限的求和当作对有限个数组元素的求和来用!!!  。最后，我们有时对多个维度进行卷积运算。例如，如果把二维的', 'time': '2017-01-12T17:27'}
{'user': 'acct:zhiding@hypothes.is', 'text': '如果把一张二维的图像$I$作为输入', 'origin_text': '素的求和来用。最后，我们有时对多个维度进行卷积运算。例如，  !!!如果把二维的图像III作为输入!!!  ，我们也相应的需要使用二维的核KKK：S(i,j)=(I∗K)', 'time': '2017-01-12T17:27'}
{'user': 'acct:zhiding@hypothes.is', 'text': '我们也许也需要使用一个二维的核$K$', 'origin_text': '对多个维度进行卷积运算。例如，如果把二维的图像III作为输入，  !!!我们也相应的需要使用二维的核K!!!  KK：S(i,j)=(I∗K)(i,j)=∑m∑nI(m,n)', 'time': '2017-01-12T17:29'}
{'user': 'acct:zhiding@hypothes.is', 'text': '实现更为直接', 'origin_text': '\\end{equation}通常，下面的公式在机器学习库中  !!!更方便应用!!!  ，因为它在mmm和nnn的有效范围内变化更少。%（这是为什么？）', 'time': '2017-01-12T17:32'}
{'user': 'acct:zhiding@hypothes.is', 'text': '因为$m$和$n$的有效取值范围更加小', 'origin_text': 'equation}通常，下面的公式在机器学习库中更方便应用，  !!!因为它在mmm和nnn的有效范围内变化更少!!!  。%（这是为什么？）卷积运算可交换性的出现是因为我们相对', 'time': '2017-01-12T17:35'}
{'user': 'acct:zhiding@hypothes.is', 'text': '我们将核相对于输入进行了翻转', 'origin_text': '变化更少。%（这是为什么？）卷积运算可交换性的出现是因为  !!!我们相对输入翻转了核!!!  ，这意味着当mmm增大时，输入的索引增大，但核的索引相应的减小。', 'time': '2017-01-12T17:37'}
{'user': 'acct:zhiding@hypothes.is', 'text': '多维数组的数据', 'origin_text': '\\end{equation}在机器学习的应用中，输入通常是  !!!高维数据数组!!!  ，而核也是由算法产生的高维参数数组。我们把这种高维数组叫做张量', 'time': '2017-01-12T17:37'}
{'user': 'acct:zhiding@hypothes.is', 'text': '就可以定义', 'origin_text': '取整数值。如果我们假设xxx和www都定义在整数时刻ttt上，  !!!就得到了!!!  离散形式的卷积：s(t)=(x∗w)(t)=∑a=−∞∞x(a', 'time': '2017-01-12T17:37'}
{'user': 'acct:zhiding@hypothes.is', 'text': '而核通常是由学习算法优化得到的多维数组的参数', 'origin_text': 'quation}在机器学习的应用中，输入通常是高维数据数组，  !!!而核也是由算法产生的高维参数数组!!!  。我们把这种高维数组叫做张量。因为输入与核的每一个元素都分开', 'time': '2017-01-12T17:37'}
{'user': 'acct:zhiding@hypothes.is', 'text': '因为在输入与核中的每一个元素都必须是明确分开存储的', 'origin_text': '核也是由算法产生的高维参数数组。我们把这种高维数组叫做张量。  !!!因为输入与核的每一个元素都分开存储!!!  ，我们经常假设在存储了数据的有限点集以外，这些函数的值都为零。', 'time': '2017-01-12T17:37'}
{'user': 'acct:zhiding@hypothes.is', 'text': '从$m$的取值增大的意义上而言，输入的数据索引在增大，但是核的数据索引在减小', 'origin_text': '么？）卷积运算可交换性的出现是因为我们相对输入翻转了核，  !!!这意味着当mmm增大时，输入的索引增大，但核的索引相应的减小!!!  。翻转核的唯一目的就是为了得到可交换性。尽管可交换性在证明时', 'time': '2017-01-13T17:28'}
{'user': 'acct:zhiding@hypothes.is', 'text': '我们将核翻转的唯一原因只是为了可以得到可交换的性质', 'origin_text': '这意味着当mmm增大时，输入的索引增大，但核的索引相应的减小。  !!!翻转核的唯一目的就是为了得到可交换性!!!  。尽管可交换性在证明时很有用，但在神经网络的应用中却不是一个重', 'time': '2017-01-13T17:29'}
{'user': 'acct:zhiding@hypothes.is', 'text': '对核并没有进行翻转', 'origin_text': '络库会实现一个相关的函数，称为互相关函数，和卷积运算几乎一样但是  !!!并不翻转核!!!  ：S(i,j)=(I∗K)(i,j)=∑m∑nI(i+m,j+', 'time': '2017-01-13T17:50'}
{'user': 'acct:zhiding@hypothes.is', 'text': '应用互相关函数但称之为卷积', 'origin_text': 'K(m, n).\\end{equation}许多机器学习的库  !!!使用互相关函数但是叫它卷积!!!  。在这本书中我们遵循把两种运算都叫做卷积的这个传统，只有在用到', 'time': '2017-01-13T17:51'}
{'user': 'acct:zhiding@hypothes.is', 'text': '在与核翻转有关的上下文中，我们会特别指出是否对核进行了翻转', 'origin_text': '是叫它卷积。在这本书中我们遵循把两种运算都叫做卷积的这个传统，  !!!只有在用到核的翻转时才会在上下文中特别指明区别!!!  。在机器学习中，学习算法会在核合适的位置学得恰当的值， 所以一', 'time': '2017-01-14T15:30'}
{'user': 'acct:zhiding@hypothes.is', 'text': '对核进行了翻转', 'origin_text': '学习中是很少见的，卷积经常和其他的函数一起使用，无论卷积运算是否  !!!翻转了它的核!!!  ，这些函数的组合通常是不可交换的。图\\?演示了一个在2维张量', 'time': '2017-01-14T15:40'}
{'user': 'acct:zhiding@hypothes.is', 'text': '矩阵每一行中元素都与上一行对应位置平移一个单位的元素相同', 'origin_text': '些元素被限制为必须和另一些元素相等。例如对于单变量的离散卷积，  !!!矩阵的每一行都必须和上一行移动一个元素后相等!!!  。这种矩阵叫做Toeplitz矩阵。对于二维情况，卷积对应着', 'time': '2017-01-14T15:54'}
{'user': 'acct:zhiding@hypothes.is', 'text': '一个几乎所有元素都为零的矩阵', 'origin_text': '除了这些元素相等的限制以外，卷积通常对应着一个非常稀疏的矩阵（  !!!几乎所有的元素都为零!!!  ）。这是因为核通常要远小于输入的图像。任何一个使用矩阵乘法但是', 'time': '2017-01-14T16:04'}
{'user': 'acct:zhiding@hypothes.is', 'text': '分块循环矩阵', 'origin_text': '矩阵叫做Toeplitz矩阵。对于二维情况，卷积对应着一个双重  !!!块循环矩阵!!!  。除了这些元素相等的限制以外，卷积通常对应着一个非常稀疏的矩阵', 'time': '2017-01-14T16:04'}
{'user': 'acct:zhiding@hypothes.is', 'text': '核的大小', 'origin_text': '通常对应着一个非常稀疏的矩阵（几乎所有的元素都为零）。这是因为  !!!核!!!  通常要远小于输入的图像。任何一个使用矩阵乘法但是并不依赖矩阵结构', 'time': '2017-01-14T16:05'}
{'user': 'acct:zhiding@hypothes.is', 'text': '输入图像的大小', 'origin_text': '非常稀疏的矩阵（几乎所有的元素都为零）。这是因为核通常要远小于  !!!输入的图像!!!  。任何一个使用矩阵乘法但是并不依赖矩阵结构的特殊性质的神经网络算', 'time': '2017-01-14T16:06'}
{'user': 'acct:corenel@hypothes.is', 'text': '已经被用于读取美国10％以上的支票', 'origin_text': '票的卷积网络{cite?}。到90年代末，NEC部署的这个系统  !!!用于读取美国所有支票的10％以上!!!  。后来，微软部署了若干个基于卷积网络的OCR和手写识别系统{c', 'time': '2017-01-15T10:42'}
{'user': 'acct:corenel@hypothes.is', 'text': '也被用作在许多比赛中的取胜手段。', 'origin_text': '以前的更为深入的卷积网络历史可以参见{cite?}。卷积网络  !!!也被用来赢得许多比赛!!!  。当前对深度学习的商业兴趣的热度始于{Krizhevsky-2', 'time': '2017-01-15T10:44'}
{'user': 'acct:corenel@hypothes.is', 'text': '但是卷积网络也已经被用于前些年影响较小的其他机器学习和计算机视觉竞赛中获胜。', 'origin_text': 'ky-2012-small}赢得了ImageNet对象识别挑战，  !!!但是卷积网络已经被用于赢得其他机器学习和计算机视觉竞赛了，这些比赛在几年前影响较小。!!!  卷积网络是用反向传播训练的第一个有效的深度网络之一。现', 'time': '2017-01-15T10:45'}
{'user': 'acct:corenel@hypothes.is', 'text': '接受', 'origin_text': '在许多方面，它们为余下的深度学习传递火炬，并为一般的神经网络被  !!!接收!!!  铺平了道路。卷积网络提供了一种方法来专业化神经网络，以处理具', 'time': '2017-01-15T10:48'}
{'user': 'acct:corenel@hypothes.is', 'text': '特化', 'origin_text': '并为一般的神经网络被接收铺平了道路。卷积网络提供了一种方法来  !!!专业化!!!  神经网络，以处理具有清楚的网格结构拓扑的数据，以及将这样的模型放', 'time': '2017-01-15T10:49'}
{'user': 'acct:corenel@hypothes.is', 'text': '使其能够', 'origin_text': '络被接收铺平了道路。卷积网络提供了一种方法来专业化神经网络，  !!!以!!!  处理具有清楚的网格结构拓扑的数据，以及将这样的模型放大到非常大的', 'time': '2017-01-15T10:49'}
{'user': 'acct:corenel@hypothes.is', 'text': '特化', 'origin_text': '为了处理一维序列数据，我们接下来转向神经网络框架的另一种强大的  !!!专业化!!!  ：循环神经网络。                 ', 'time': '2017-01-15T10:49'}
{'user': 'acct:corenel@hypothes.is', 'text': '使得它们能够与位于网络结构顶层的分类层相互独立地确定', 'origin_text': '第\\?部分描述了更多的无监督学习方法。使用无监督标准学习特征，  !!!允许它们的确定与位于网络结构顶层的分类层相分离!!!  。然后只需提取一次全部训练集的特征，构造用于最后一层的新训练集', 'time': '2017-01-15T10:56'}
{'user': 'acct:zhiding@hypothes.is', 'text': '传统的神经网络通过一个参数矩阵和另一个参数的矩阵乘法运算来描述每一个输入和每一个输出单元之间的交互。', 'origin_text': '了一种处理大小可变的输入的方法。我们下面依次介绍这些思想。  !!!传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵的每一个独立的参数都描述了每一个输入单元与每一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。!!!  然而，卷积神经网络具有稀疏交互（也叫做稀疏连接或者稀疏权重）的特', 'time': '2017-01-16T14:53'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这是使核的规模远小于输入的规模来达到的。', 'origin_text': '卷积神经网络具有稀疏交互（也叫做稀疏连接或者稀疏权重）的特征。  !!!这通过使得核的规模远小于输入的规模来实现!!!  。举个例子，当进行图像处理时，输入的图像可能包含百万个像素点，', 'time': '2017-01-16T15:06'}
{'user': 'acct:zhiding@hypothes.is', 'text': '当处理一张图像的时候', 'origin_text': '的特征。这通过使得核的规模远小于输入的规模来实现。举个例子，  !!!当进行图像处理时!!!  ，输入的图像可能包含百万个像素点，但是我们可以通过只占用几十到上', 'time': '2017-01-16T15:08'}
{'user': 'acct:zhiding@hypothes.is', 'text': '数千或数百万个', 'origin_text': '入的规模来实现。举个例子，当进行图像处理时，输入的图像可能包含  !!!百万个!!!  像素点，但是我们可以通过只占用几十到上百个像素点的核来探测一些小', 'time': '2017-01-16T15:14'}
{'user': 'acct:zhiding@hypothes.is', 'text': '小于', 'origin_text': 'mes n)的运行时间。在很多应用方面，只需保持kkk的数量级  !!!远小于!!!  mmm，就能在机器学习的任务中取得好的表现。稀疏连接的图形化解', 'time': '2017-01-16T16:42'}
{'user': 'acct:zhiding@hypothes.is', 'text': '处在网络深层的单元可能与绝大部分的输入是间接连接的', 'origin_text': '稀疏连接的图形化解释如图\\?和图\\?所示。在深度卷积网络中，  !!!处在深层的单元可能\\emph{不直接}地与绝大部分输入连接!!!  ，如图\\?所示。这允许网络可以通过只描述稀疏交互的基石来高效地', 'time': '2017-01-16T16:51'}
{'user': 'acct:zhiding@hypothes.is', 'text': '设计决策', 'origin_text': '作用在输入的每一位置上（除了一些可能的边界像素，取决于对于边界的  !!!决策设计!!!  ）。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不', 'time': '2017-01-16T17:10'}
{'user': 'acct:zhiding@hypothes.is', 'text': '可能遇见的一些边界像素', 'origin_text': '在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（除了  !!!一些可能的边界像素!!!  ，取决于对于边界的决策设计）。卷积运算中的参数共享保证了我们只', 'time': '2017-01-16T17:11'}
{'user': 'acct:zhiding@hypothes.is', 'text': '并且$k$比$m$要小很多个数量级', 'origin_text': 'imes n)），但它显著地把模型的存储需求降低至kkk个参数，  !!!并且kkk通常是远小于mmm的数量级!!!  。因为mmm 和nnn通常规模很接近，kkk在实际中相对于m×', 'time': '2017-01-16T17:16'}
{'user': 'acct:zhiding@hypothes.is', 'text': '$k$的大小', 'origin_text': '常是远小于mmm的数量级。因为mmm 和nnn通常规模很接近，  !!!kkk!!!  在实际中相对于m×nm×nm\\times n是很小的。因此，卷', 'time': '2017-01-16T17:17'}
{'user': 'acct:zhiding@hypothes.is', 'text': '通常$m$和$n$有着大致相同的规模', 'origin_text': '需求降低至kkk个参数，并且kkk通常是远小于mmm的数量级。  !!!因为mmm 和nnn通常规模很接近!!!  ，kkk在实际中相对于m×nm×nm\\times n是很小的。', 'time': '2017-01-16T17:20'}
{'user': 'acct:zhiding@hypothes.is', 'text': '稀疏连接和参数共享是如何显著提高线性函数在一张图像上进行边缘检测的效率', 'origin_text': 'igure}作为前两条原则的一个实际例子，图\\?说明了  !!!稀疏连接和参数共享是如何显著地提高用于图像边缘检测的线性函数的效率的!!!  。\\begin{figure}\\ifOpenSource\\c', 'time': '2017-01-16T17:30'}
{'user': 'acct:zhiding@hypothes.is', 'text': '能够使得输入平移的任意函数', 'origin_text': 'f(x)对于变换ggg具有等变性。对于卷积来说，如果令ggg是  !!!输入的任意平移函数!!!  ，那么卷积函数对于ggg具有等变性。举个例子，令III表示图像', 'time': '2017-01-16T17:36'}
{'user': 'acct:zhiding@hypothes.is', 'text': '是表示图像在整数坐标上亮度的函数', 'origin_text': '平移函数，那么卷积函数对于ggg具有等变性。举个例子，令III  !!!表示图像的明亮度函数（取值为整数）!!!  ，ggg表示图像函数的变换函数（把一个图像函数映射到另一个图像函', 'time': '2017-01-16T17:40'}
{'user': 'acct:zhiding@hypothes.is', 'text': '其中图像函数满足', 'origin_text': '函数的函数）使得I′=g(I)I′=g(I)I’ = g(I)，  !!!其中!!!  I′(x,y)=I(x−1,y)I′(x,y)=I(x−1,y)', 'time': '2017-01-16T17:44'}
{'user': 'acct:zhiding@hypothes.is', 'text': '一个单位', 'origin_text': ' = I(x-1, y)。这个函数把III中的每个像素向右移动  !!!一格!!!  。如果我们先对III进行这种变换然后进行卷积操作所得到的结果，', 'time': '2017-01-16T17:44'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这意味着通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴', 'origin_text': '此处误写成了I′I′I’。} 。%译者注当处理时间序列数据时，  !!!卷积产生一条用来表明输入中出现不同特征的某种时间轴!!!  。如果我们把输入中的一个事件向后延时，在输出中也会有完全相同的', 'time': '2017-01-17T03:37'}
{'user': 'acct:zhiding@hypothes.is', 'text': '延后', 'origin_text': '输入中的一个事件向后延时，在输出中也会有完全相同的表示，只是时间  !!!延时!!!  了。图像与之类似，卷积产生了一个2维映射来表明某种属性在输入的', 'time': '2017-01-17T03:53'}
{'user': 'acct:zhiding@hypothes.is', 'text': '某些特征在输入中出现的位置', 'origin_text': '示，只是时间延时了。图像与之类似，卷积产生了一个2维映射来表明  !!!某种属性在输入的什么位置出现了!!!  。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。', 'time': '2017-01-17T04:40'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积网络', 'origin_text': '置时，一些作用在邻居像素的函数是很有用的。例如在处理图像时，在  !!!卷积神经网络!!!  的第一层进行图像的边缘检测是很有用的。相同的边缘或多或少地散落', 'time': '2017-01-17T04:48'}
{'user': 'acct:zhiding@hypothes.is', 'text': '例如，我们在处理已经通过剪裁居中的人脸图像，我们可能想要提取不同位置上的不同特征', 'origin_text': '参数共享。但在某些情况下，我们并不希望对整幅图进行参数共享。  !!!例如当我们在处理人脸图像（图像已经被剪裁成人脸在中心）时，我们可能会希望在不同的部位探测出不同的特征!!!  （处理人脸上部的网络需要去搜寻眉毛，处理人脸下部的网络就需要去搜', 'time': '2017-01-17T04:55'}
{'user': 'acct:zhiding@hypothes.is', 'text': '一个图像尺度和旋转的变化', 'origin_text': '搜寻下巴了）。卷积对其他的一些变换并不是天然等变的，例如  !!!对于图像尺度或者角度的变换!!!  ，需要其他的一些机制来处理这些变换。最后，一些不能被传统的由', 'time': '2017-01-17T05:00'}
{'user': 'acct:zhiding@hypothes.is', 'text': '有一些数据并不能由固定大小的矩阵乘法定义的神经网络来处理，但是卷积可以处理这些数据。', 'origin_text': '尺度或者角度的变换，需要其他的一些机制来处理这些变换。最后，  !!!一些不能被传统的由（固定大小的）矩阵乘法定义的神经网络处理的特殊数据，可能通过卷积神经网络来处理!!!  ，我们将在\\?节中进行讨论。池化卷积神经网络的卷积层通常', 'time': '2017-01-17T05:04'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积网络中一个典型的层包含三级', 'origin_text': '能通过卷积神经网络来处理，我们将在\\?节中进行讨论。池化  !!!卷积神经网络的卷积层通常包含三级!!!  （如图\\?所示）。在第一级中，卷积层并行地进行多个卷积运算来产', 'time': '2017-01-17T05:07'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这一层并行地计算多个卷积生成一组线性激活响应', 'origin_text': '卷积神经网络的卷积层通常包含三级（如图\\?所示）。在第一级中，  !!!卷积层并行地进行多个卷积运算来产生一组线性激活函数!!!  。在第二级中，非线性的激活函数如整流线性单元函数等作用在第一级', 'time': '2017-01-17T05:10'}
{'user': 'acct:zhiding@hypothes.is', 'text': '每一个线性激活响应将会通过一个非线性的激活函数，例如修正线性激活函数', 'origin_text': '积层并行地进行多个卷积运算来产生一组线性激活函数。在第二级中，  !!!非线性的激活函数如整流线性单元函数等作用在第一级中的每一个线性输出上!!!  。这一级有时也被称为探测级。在第三级中，我们使用池化函数来更', 'time': '2017-01-17T05:14'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这一层的输出', 'origin_text': '时也被称为探测级。在第三级中，我们使用池化函数来更进一步地调整  !!!卷积层的输出!!!  。\\begin{figure}[!htb]\\ifOpenSo', 'time': '2017-01-17T05:19'}
{'user': 'acct:zhiding@hypothes.is', 'text': '在所有的情况中', 'origin_text': 'L2L2L^2范数以及依靠据中心像素距离的加权平均函数。  !!!不管采用什么样的池化函数!!!  ，当输入作出少量平移时，池化能帮助我们的表示近似不变。对于平移', 'time': '2017-01-17T05:19'}
{'user': 'acct:zhiding@hypothes.is', 'text': '输出的表征', 'origin_text': '不管采用什么样的池化函数，当输入作出少量平移时，池化能帮助  !!!我们的表示!!!  近似不变。对于平移的不变性是说当我们把输入平移一微小的量，大多', 'time': '2017-01-20T05:54'}
{'user': 'acct:zhiding@hypothes.is', 'text': '平移的不变性是指', 'origin_text': '池化函数，当输入作出少量平移时，池化能帮助我们的表示近似不变。  !!!对于平移的不变性是说!!!  当我们把输入平移一微小的量，大多数通过池化函数的输出值并不会发生', 'time': '2017-01-20T05:57'}
{'user': 'acct:zhiding@hypothes.is', 'text': '输入进行少量的平移', 'origin_text': '时，池化能帮助我们的表示近似不变。对于平移的不变性是说当我们把  !!!输入平移一微小的量!!!  ，大多数通过池化函数的输出值并不会发生改变。图\\?用了一个例子', 'time': '2017-01-20T05:58'}
{'user': 'acct:zhiding@hypothes.is', 'text': '经过池化函数后的大多数输出并不会发生改变', 'origin_text': '表示近似不变。对于平移的不变性是说当我们把输入平移一微小的量，  !!!大多数通过池化函数的输出值并不会发生改变!!!  。图\\?用了一个例子来说明这是如何实现的。\\emph{局部平', 'time': '2017-01-20T06:06'}
{'user': 'acct:zhiding@hypothes.is', 'text': '很有用', 'origin_text': '一个例子来说明这是如何实现的。\\emph{局部平移不变性是一个  !!!很重要!!!  的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置', 'time': '2017-01-20T06:14'}
{'user': 'acct:zhiding@hypothes.is', 'text': '网络', 'origin_text': 'figure}使用池化可以看作是增加了一个无限强的先验：  !!!卷积!!!  层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化', 'time': '2017-01-20T07:06'}
{'user': 'acct:zhiding@hypothes.is', 'text': '关于这一层输入大小的函数', 'origin_text': '算效率，因为下一层少了约kkk 倍的输入。当下一层的参数数目是  !!!其输入大小的函数!!!  时（例如当下一层是全连接的依赖矩阵乘法的网络层时），这种对于输入', 'time': '2017-01-20T07:41'}
{'user': 'acct:zhiding@hypothes.is', 'text': '将特征一起动态地池化', 'origin_text': '在不同情况下应当使用哪种池化函数给出了一些指导{cite?}。  !!!动态地把特征池化在一起!!!  也是可行的，例如，通过针对特定属性的位置运行聚类算法{cite?', 'time': '2017-01-28T06:56'}
{'user': 'acct:zhiding@hypothes.is', 'text': '对于感兴趣特征的位置进行聚类算法', 'origin_text': '些指导{cite?}。动态地把特征池化在一起也是可行的，例如，  !!!通过针对特定属性的位置运行聚类算法!!!  {cite?}。这种方法对于每幅图像产生一个不同的池化区域集合', 'time': '2017-01-20T07:55'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这个先验也要求除了那些处在隐藏单元所在的小的空间连续的感受野的权值外', 'origin_text': '是说一个隐藏单元的权值必须和它邻居的权值相等，但在空间中改变。  !!!这个先验也要求除了那些处在隐藏单元空间连续的小的接收域以内的权值外!!!  ，其余的权值都为零。%这样翻译对吗？其实这里加个图更好总之，我', 'time': '2017-01-20T08:24'}
{'user': 'acct:zhiding@hypothes.is', 'text': '由多个并行的卷积运算组成', 'origin_text': '们提到神经网络中的卷积时，我们通常是指一次特定的运算，而这种运算  !!!包含了并行地使用多个卷积!!!  。这是因为带有单个核的卷积只能提取一种类型的特征，尽管它作用在', 'time': '2017-01-20T12:13'}
{'user': 'acct:zhiding@hypothes.is', 'text': '强度', 'origin_text': '的网格。例如，一幅彩色图像在每一个像素点都会有红绿蓝三种颜色的  !!!亮度!!!  。在多层的卷积神经网络中，第二层的输入是第一层的输出，通常在每', 'time': '2017-01-20T13:09'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积网络', 'origin_text': '一幅彩色图像在每一个像素点都会有红绿蓝三种颜色的亮度。在多层的  !!!卷积神经网络!!!  中，第二层的输入是第一层的输出，通常在每个位置包含多个卷积的输出', 'time': '2017-01-20T13:10'}
{'user': 'acct:zhiding@hypothes.is', 'text': '通道$i$的一个输出单元与通道$j$的一个输入单元之间的连接强度', 'origin_text': 'extsf{K}i,j,k,l\\TEK_{i,j,k,l}，表示  !!!输出的处于通道iii中的一个单元和输入的处于通道jjj中的一个单元的连接强度!!!  ，并且在输出单元和输入单元之间有一个kkk行lll列的偏置。假', 'time': '2017-01-20T13:34'}
{'user': 'acct:zhiding@hypothes.is', 'text': '全卷积', 'origin_text': '的代价是提取特征没有先前那么好了）。我们可以把这一过程看作是对  !!!卷积!!!  函数输出的下采样(downsampling)。如果我们只想对输', 'time': '2017-01-20T13:48'}
{'user': 'acct:zhiding@hypothes.is', 'text': '在输出的每个方向上每间隔$s$个像素进行采样', 'origin_text': '卷积函数输出的下采样(downsampling)。如果我们只想  !!!对输出的每个方向上的sss个像素进行采样!!!  ，那么我们可以定义一个下采样卷积函数ccc使得\\begin{e', 'time': '2017-01-20T13:50'}
{'user': 'acct:zhiding@hypothes.is', 'text': '卷积网络', 'origin_text': '它计算了许多将被丢弃的值。}\\end{figure}在任何  !!!卷积神经网络!!!  的应用中都有一个重要性质，那就是能够隐含地对输入\\𝗍𝖾𝗑�', 'time': '2017-01-20T13:59'}
{'user': 'acct:zhiding@hypothes.is', 'text': '实现', 'origin_text': '被丢弃的值。}\\end{figure}在任何卷积神经网络的  !!!应用!!!  中都有一个重要性质，那就是能够隐含地对输入\\𝗍𝖾𝗑𝗍𝖻', 'time': '2017-01-20T13:59'}
{'user': 'acct:zhiding@hypothes.is', 'text': '表征', 'origin_text': '}\\TSV用零进行填充(pad)使得它加宽。如果没有这个性质，  !!!表示!!!  的宽度在每一层就会缩减，缩减的幅度是比核少一个像素这么多。对输', 'time': '2017-01-20T14:23'}
{'user': 'acct:zhiding@hypothes.is', 'text': '的术语中', 'origin_text': '核只允许访问那些图像中能够完全包含整个核的位置。在MATLAB  !!!中!!!  ，这称为有效卷积。在这种情况下，输出的所有像素都是输入中相同数', 'time': '2017-01-20T16:18'}
{'user': 'acct:zhiding@hypothes.is', 'text': '我们并不真正想用卷积', 'origin_text': '效卷积”和”相同卷积”之间的某个位置。在一些情况下，  !!!我们并不一定真正想用卷积!!!  ，而只是用一些局部连接的网络层{cite?}。在这种情况下，我', 'time': '2017-01-24T12:42'}
{'user': 'acct:zhiding@hypothes.is', 'text': '并且相同的特征不会出现在所有的空间上', 'origin_text': 'figure}当我们知道每一个特征都是一小部分空间的函数  !!!而不是整个空间的特征时!!!  ，局部连接层是很有用的。例如，如果我们想要辨别一张图片是否是人', 'time': '2017-01-24T14:07'}
{'user': 'acct:zhiding@hypothes.is', 'text': '下半部分', 'origin_text': '想要辨别一张图片是否是人脸图像时，我们只需要去寻找嘴是否在图像的  !!!下部中央部分!!!  即可。使用那些连接被更进一步限制的卷积或者局部连接层也是有用', 'time': '2017-01-25T06:34'}
{'user': 'acct:zhiding@hypothes.is', 'text': '平铺', 'origin_text': '通道只和随后的两个输入通道相连。}\\end{figure}  !!!拼贴!!!  卷积{cite?}对卷积层和局部连接层进行了折衷。%这里翻译成啥', 'time': '2017-01-25T07:19'}
{'user': 'acct:zhiding@hypothes.is', 'text': '平铺', 'origin_text': '合的大小，而不是整个输出的特征映射的大小。图\\?对局部连接层、  !!!拼贴!!!  卷积和标准卷积进行了比较。\\begin{figure}[!h', 'time': '2017-01-25T07:19'}
{'user': 'acct:zhiding@hypothes.is', 'text': '平铺', 'origin_text': 'b”的边的核。}\\end{figure}为了代数地定义  !!!拼贴!!!  卷积，令\\𝗍𝖾𝗑𝗍𝖻𝖿{𝖪}\\textbf{K}\\', 'time': '2017-01-25T07:20'}
{'user': 'acct:zhiding@hypothes.is', 'text': '为了用代数的方法定义平铺卷积', 'origin_text': '记为”a”和”b”的边的核。}\\end{figure}  !!!为了代数地定义拼贴卷积!!!  ，令\\𝗍𝖾𝗑𝗍𝖻𝖿{𝖪}\\textbf{K}\\TS', 'time': '2017-01-25T07:25'}
{'user': 'acct:zhiding@hypothes.is', 'text': '各自', 'origin_text': '中的两维对应着输出映射中的不同位置。%译者注我们这里并不是使用  !!!分别!!!  的索引来表示输出映射中的每一个位置，输出的位置在每个方向上在tt', 'time': '2017-01-25T07:48'}
{'user': 'acct:zhiding@hypothes.is', 'text': '推广', 'origin_text': ' 1等等。在每一维上使用不同的ttt可以很直观地对这个方程进行  !!!扩展!!!  。%这里需要重新理解一下局部连接层与拼贴卷积层都和最', 'time': '2017-01-25T08:01'}
{'user': 'acct:zhiding@hypothes.is', 'text': '平铺', 'origin_text': '这个方程进行扩展。%这里需要重新理解一下局部连接层与  !!!拼贴!!!  卷积层都和最大池化有一些有趣的关联：这些层的探测单元都是由不同的', 'time': '2017-01-25T08:02'}
{'user': 'acct:zhiding@hypothes.is', 'text': '滤波器', 'origin_text': '卷积层都和最大池化有一些有趣的关联：这些层的探测单元都是由不同的  !!!过滤器!!!  驱动的。如果这些过滤器能够学会探测相同隐含特征的不同变换形式，', 'time': '2017-01-25T08:03'}
{'user': 'acct:zhiding@hypothes.is', 'text': '其中包含', 'origin_text': '矩阵乘法的形式（如果我们首先把输入张量变形为一个扁平的向量）。  !!!涉及到!!!  的矩阵是卷积核的函数。这个矩阵是稀疏的并且核的每个元素都复制给', 'time': '2017-01-25T08:17'}
{'user': 'acct:zhiding@hypothes.is', 'text': '是关于', 'origin_text': '式（如果我们首先把输入张量变形为一个扁平的向量）。涉及到的矩阵  !!!是!!!  卷积核的函数。这个矩阵是稀疏的并且核的每个元素都复制给矩阵的很', 'time': '2017-01-25T08:17'}
{'user': 'acct:zhiding@hypothes.is', 'text': '矩阵中的多个元素', 'origin_text': '的矩阵是卷积核的函数。这个矩阵是稀疏的并且核的每个元素都复制给  !!!矩阵的很多个元素!!!  。这种观点能够帮助我们导出卷积神经网络需要的很多其他运算。', 'time': '2017-01-25T08:50'}
{'user': 'acct:zhiding@hypothes.is', 'text': '在转置运算与前向传播相协调的过程中必须非常小心', 'origin_text': '来实现，但在一般情况下需要用到第三种运算来实现。%这里不是很懂  !!!必须非常小心地来使这种转置运算和前向传播过程相协调!!!  。转置运算返回的输出的大小取决于三个方面：零填充的策略、前向传', 'time': '2017-01-25T08:59'}
{'user': 'acct:zhiding@hypothes.is', 'text': '相同形式', 'origin_text': '和\\𝗍𝖾𝗑𝗍𝖻𝖿{𝖹}\\textbf{Z}\\TSZ  !!!相同格式!!!  的隐藏单元\\𝗍𝖾𝗑𝗍𝖻𝖿{𝖧}\\textbf{H}', 'time': '2017-01-25T09:27'}
{'user': 'acct:zhiding@hypothes.is', 'text': '这可以通过', 'origin_text': '𝖾𝗑𝗍𝖻𝖿{𝖪}\\textbf{K}\\TSK的梯度，  !!!通过!!!  g(\\𝗍𝖾𝗑𝗍𝖻𝖿{𝖧},\\𝗍𝖾𝗑𝗍𝖻�', 'time': '2017-01-25T09:28'}
{'user': 'acct:zhiding@hypothes.is', 'text': '获取', 'origin_text': '},\\textbf{E},s)g(\\TSH, \\TSE, s)来  !!!得到!!!  。为了训练编码器，我们需要获得对于\\𝗍𝖾𝗑𝗍𝖻𝖿{', 'time': '2017-01-25T09:29'}
{'user': 'acct:zhiding@hypothes.is', 'text': '通过用c和h对g求微分也是可行的', 'origin_text': 'extbf{E},s)c(\\TSK, \\TSE, s)来得到。  !!!也可能通过用ccc和hhh对ggg求微分得到!!!  ，但这些运算对于任何标准神经网络上的反向传播算法来说都是不需要的', 'time': '2017-01-25T09:31'}
{'user': 'acct:zhiding@hypothes.is', 'text': '平铺', 'origin_text': '题。对于局部连接层，很自然地对每个单元都给定它特有的偏置，对于  !!!拼贴!!!  卷积，也很自然地用与核一样的拼贴模式来共享参数。对于卷积层来说', 'time': '2017-01-25T09:33'}
{'user': 'acct:zhiding@hypothes.is', 'text': '处于相同平铺模式下的核所对应的偏置之间共享参数', 'origin_text': '，很自然地对每个单元都给定它特有的偏置，对于拼贴卷积，也很自然地  !!!用与核一样的拼贴模式!!!  来共享参数。对于卷积层来说，通常的做法是在输出的每一个通道上都', 'time': '2017-01-25T09:35'}
{'user': 'acct:zhiding@hypothes.is', 'text': '问题规模', 'origin_text': '信号的逐点相乘，并使用Fourier逆变换转换回时域。对于某些  !!!问题大小!!!  ，这可能比离散型卷积的朴素实现更快。当一个ddd维的核可以表', 'time': '2017-01-28T06:57'}
{'user': 'acct:zhiding@hypothes.is', 'text': '每一维一个向量', 'origin_text': '积的朴素实现更快。当一个ddd维的核可以表示成ddd个向量（  !!!每维一个向量!!!  ）的外积时，该核称为可分离的。当核可分离时，朴素的卷积是低效的', 'time': '2017-01-29T15:17'}
{'user': 'acct:zhiding@hypothes.is', 'text': '当核是可分离的时候', 'origin_text': '表示成ddd个向量（每维一个向量）的外积时，该核称为可分离的。  !!!当核可分离时!!!  ，朴素的卷积是低效的。它等价于组合ddd个一维卷积，每个卷积使', 'time': '2017-01-29T15:19'}
{'user': 'acct:zhiding@hypothes.is', 'text': '并且核也只要更少的参数来表示成向量', 'origin_text': '个。组合方法显著快于使用它们的外积来执行一个ddd维的卷积。  !!!并且核也需要更少的参数表示为向量!!!  。如果核在每一维都是www个元素宽，那么朴素的多维卷积需要O(', 'time': '2017-01-29T15:39'}
{'user': 'acct:zhiding@hypothes.is', 'text': '不是由有监督方式训练得到的特征', 'origin_text': '反向传播通过整个网络。减少卷积网络训练成本的一种方式是使用那些  !!!不是通过有监督方式训练的特征!!!  。有三种基本策略不通过有监督训练而得到卷积核。其中一个是简', 'time': '2017-01-29T16:33'}
{'user': 'acct:zhiding@hypothes.is', 'text': '可以不通过', 'origin_text': '种方式是使用那些不是通过有监督方式训练的特征。有三种基本策略  !!!不通过!!!  有监督训练而得到卷积核。其中一个是简单地随机初始化它们。另一', 'time': '2017-01-29T16:40'}
{'user': 'acct:zhiding@hypothes.is', 'text': '滤波器', 'origin_text': '辑回归或者SVM，那么学习最后一层通常是凸优化问题。随机  !!!过滤器!!!  经常在卷积网络中表现得出乎意料得好{Jarrett-ICCV20', 'time': '2017-01-30T05:22'}
{'user': 'acct:zhiding@hypothes.is', 'text': '代替', 'origin_text': '网络为我们提供了相对于多层感知机更进一步采用预训练策略的机会。  !!!不是!!!  一次训练整个卷积层，我们可以训练一小块模型，就像{Coates2', 'time': '2017-01-30T06:08'}
{'user': 'acct:zhiding@hypothes.is', 'text': '感受野', 'origin_text': '单细胞。简单细胞的活动在某种程度上可以概括为在一个小的空间位置  !!!接受域内!!!  的图像的线性函数。卷积网络的检测器单元被设计为模拟简单细胞的这', 'time': '2017-01-31T13:56'}
{'user': 'acct:zhiding@hypothes.is', 'text': '其他神经元会对Bill Clinton, Jennifer Ansion等人的出现作出响应', 'origin_text': '，这个神经元会触发。当然，这与Halle Berry自己无关；  !!!其他神经元响应Bill Clinton，Jennifer Aniston等的出现!!!  。这些内侧颞叶神经元比现代卷积网络更通用，它们在读取名称', 'time': '2017-01-31T15:06'}
{'user': 'acct:zhiding@hypothes.is', 'text': '在手臂长度距离内一块拇指大小的区域', 'origin_text': '部分是非常低的分辨率，除了一个被称为中央凹的小块。中央凹仅观察  !!!保持在手臂长度的拇指大小的区域!!!  。虽然我们觉得我们可以看到高分辨率的整个场景，但这是由我们的大', 'time': '2017-02-04T11:02'}
{'user': 'acct:zhiding@hypothes.is', 'text': '滤波器', 'origin_text': '用非常不同的激活和池化函数。单个神经元的激活可能不能用单个线性  !!!过滤器!!!  的响应来很好地表征。最近的V1模型涉及对每个神经元的多个二次过', 'time': '2017-02-05T04:40'}
{'user': 'acct:zhiding@hypothes.is', 'text': '物体', 'origin_text': 'rizhevsky-2012-small}赢得了ImageNet  !!!对象!!!  识别挑战，但是卷积网络已经被用于赢得其他机器学习和计算机视觉竞赛', 'time': '2017-02-05T17:52'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '基准', 'origin_text': '一些指导。对于卷积网络结构的研究进展得如此迅速，以至于针对特定  !!!问题!!!  ，数月甚至几周就会产生一个新的最优的网络结构，甚至在写这本书时也', 'time': '2017-02-11T10:43'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '逐步', 'origin_text': '竟哪种结构是最好的。然而，最好的结构也是由本章所描述的基本部件  !!!一点一点!!!  搭建起来的。卷积运算在通常形式中，卷积是对两个实值函', 'time': '2017-02-11T10:45'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '实参', 'origin_text': '点一点搭建起来的。卷积运算在通常形式中，卷积是对两个  !!!实值函数!!!  的一种数学运算。为了给出卷积的定义，我们从两个可能会用到的函数', 'time': '2017-02-11T10:47'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '受到一定程度的噪声干扰', 'origin_text': '可以在任意时刻从传感器中读出飞船的位置。现在假设我们的传感器  !!!含有噪声!!!  。为了得到飞船位置的低噪声估计，我们对得到的测量结果进行平均。', 'time': '2017-02-11T10:51'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '预测到', 'origin_text': '再是一个加权平均。另外，www在参数为负值时必须为0，否则它会  !!!涉及到!!!  未来，这不是我们能够做到的。但这些限制仅仅是对我们这个例子来说', 'time': '2017-02-11T11:01'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '本例', 'origin_text': '数（函数www）叫做核函数。输出有时被称作特征映射。在  !!!我们的例子!!!  中，激光传感器能够在任意时刻给出测量结果的想法是不现实的。一般', 'time': '2017-02-11T11:05'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '经常', 'origin_text': '统一地把无限的求和当作对有限个数组元素的求和来用。最后，我们  !!!有时!!!  对多个维度进行卷积运算。例如，如果把二维的图像III作为输入，', 'time': '2017-02-11T11:14'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '机器学习的上下文中', 'origin_text': '的这个传统，只有在用到核的翻转时才会在上下文中特别指明区别。在  !!!机器学习!!!  中，学习算法会在核合适的位置学得恰当的值， 所以一个基于核翻转的', 'time': '2017-02-18T09:48'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '成千上万', 'origin_text': '入的规模来实现。举个例子，当进行图像处理时，输入的图像可能包含  !!!百万个!!!  像素点，但是我们可以通过只占用几十到上百个像素点的核来探测一些小', 'time': '2017-02-18T11:01'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '时间复杂度', 'origin_text': ' n个参数以及O(k×n)O(k×n)O(k\\times n)的  !!!运行时间!!!  。在很多应用方面，只需保持kkk的数量级远小于mmm，就能在机', 'time': '2017-02-18T11:06'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '实际应用中', 'origin_text': 'k×n)O(k×n)O(k\\times n)的运行时间。在很多  !!!应用方面!!!  ，只需保持kkk的数量级远小于mmm，就能在机器学习的任务中取得', 'time': '2017-02-18T11:07'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '是否考虑边界像素取决于边界设计策略', 'origin_text': '上。在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（  !!!除了一些可能的边界像素，取决于对于边界的决策设计!!!  ）。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不', 'time': '2017-02-18T13:37'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '运算时间', 'origin_text': '每一位置都需要学习一个单独的参数集合。这虽然没有改变前向传播的  !!!时间!!!  （仍然是O(k×n)O(k×n)O(k\\times n)），但它', 'time': '2017-02-18T14:03'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '部分网络', 'origin_text': '测出不同的特征（处理人脸上部的网络需要去搜寻眉毛，处理人脸下部的  !!!网络!!!  就需要去搜寻下巴了）。卷积对其他的一些变换并不是天然等变', 'time': '2017-02-19T05:07'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '部分网络', 'origin_text': '）时，我们可能会希望在不同的部位探测出不同的特征（处理人脸上部的  !!!网络!!!  需要去搜寻眉毛，处理人脸下部的网络就需要去搜寻下巴了）。', 'time': '2017-02-19T05:07'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '可以', 'origin_text': '能被传统的由（固定大小的）矩阵乘法定义的神经网络处理的特殊数据，  !!!可能!!!  通过卷积神经网络来处理，我们将在\\?节中进行讨论。池化卷', 'time': '2017-02-19T05:08'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '进一步', 'origin_text': '出上。这一级有时也被称为探测级。在第三级中，我们使用池化函数  !!!来更进一步!!!  地调整卷积层的输出。\\begin{figure}[!htb]', 'time': '2017-02-19T05:17'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '基于', 'origin_text': '常用的池化函数包括相邻矩形区域内的平均值、L2L2L^2范数以及  !!!依靠!!!  据中心像素距离的加权平均函数。不管采用什么样的池化函数，', 'time': '2017-02-19T05:21'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '精确', 'origin_text': '。例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的  !!!具体!!!  像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了', 'time': '2017-02-19T05:35'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '或多或少的改变参数', 'origin_text': '弱先验具有较高的熵值，例如方差很大的高斯分布，这样的先验允许数据  !!!对于参数的改变具有或多或少的自由性!!!  。强先验具有较低的熵值，例如方差很小的高斯分布，这样的先验在决', 'time': '2017-02-19T12:06'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '很低', 'origin_text': '样的先验允许数据对于参数的改变具有或多或少的自由性。强先验具有  !!!较低!!!  的熵值，例如方差很小的高斯分布，这样的先验在决定参数最终取值时起', 'time': '2017-02-19T12:07'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '。', 'origin_text': '或多或少的自由性。强先验具有较低的熵值，例如方差很小的高斯分布  !!!，!!!  这样的先验在决定参数最终取值时起着更加积极的作用。一个无限强', 'time': '2017-02-19T12:07'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '需要对', 'origin_text': '先验在决定参数最终取值时起着更加积极的作用。一个无限强的先验  !!!对!!!  一些参数的概率置零并且要求禁止对这些参数赋值，无论数据对于这些参', 'time': '2017-02-19T12:09'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '完全', 'origin_text': '起着更加积极的作用。一个无限强的先验对一些参数的概率置零并且  !!!要求!!!  禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。', 'time': '2017-02-19T12:10'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '类比全链接网络', 'origin_text': '数据对于这些参数的值给出了多大的支持。我们可以把卷积神经网络  !!!想成和全连接网络类似!!!  ，但对于这个全连接网络的权值有一个无限强的先验。这个无限强的先', 'time': '2017-02-19T12:11'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '指', 'origin_text': '对于这个全连接网络的权值有一个无限强的先验。这个无限强的先验是  !!!说!!!  一个隐藏单元的权值必须和它邻居的权值相等，但在空间中改变。这个', 'time': '2017-02-19T12:12'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '但可以在空间上移动', 'origin_text': '这个无限强的先验是说一个隐藏单元的权值必须和它邻居的权值相等，  !!!但在空间中改变!!!  。这个先验也要求除了那些处在隐藏单元空间连续的小的接收域以内的', 'time': '2017-02-19T12:16'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '指', 'origin_text': '是对网络中一层的参数引入了一个无限强的先验概率分布。这个先验是  !!!说!!!  该层应该学得的函数只包含局部连接关系并且对平移具有等变性。类似', 'time': '2017-02-19T12:37'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '会', 'origin_text': '洞察卷积神经网络是如何工作的。其中一个关键的洞察是卷积和池化  !!!可能!!!  导致欠拟合。与任何其他先验类似，卷积和池化只有当先验的假设合理', 'time': '2017-02-19T12:45'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '卷积神经网络结构', 'origin_text': '确的空间信息，那么在所有的特征上使用池化将会增大训练误差。一些  !!!卷积神经网络!!!  {cite?}为了既获得具有较高不变性的特征又获得当平移不变性不', 'time': '2017-02-19T12:49'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '有能力', 'origin_text': '。其他不使用卷积的模型即使我们把图像中的所有像素点都置换后依然  !!!有可能!!!  进行学习。对于许多图像数据集，还有一些分别的基准，有些是针对那', 'time': '2017-02-19T12:56'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '植入', 'origin_text': '现拓扑结构的模型，还有一些是针对设计者将空间关系的知识通过硬编码  !!!给了!!!  它们的模型。基本卷积函数的变体当在神经网络的上下文中', 'time': '2017-02-19T13:05'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '略有区别', 'origin_text': '是特指数学文献中使用的那种标准的离散卷积运算。实际应用中的函数  !!!略微有些不同!!!  。这里我们详细讨论一下这些差异，并且对神经网络中用到的函数的一', 'time': '2017-02-19T13:10'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '我们通常是指由多个并行卷积组成的运算', 'origin_text': '些重要性质进行重点说明。首先，当我们提到神经网络中的卷积时，  !!!我们通常是指一次特定的运算，而这种运算包含了并行地使用多个卷积!!!  。这是因为带有单个核的卷积只能提取一种类型的特征，尽管它作用在', 'time': '2017-02-19T13:12'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '每一层', 'origin_text': '类型的特征，尽管它作用在多个空间位置上。我们通常希望神经网络的  !!!一层!!!  能够在多个位置提取多种类型的特征。另外，输入通常也不仅仅是实', 'time': '2017-02-19T13:14'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '观测数据的向量', 'origin_text': '类型的特征。另外，输入通常也不仅仅是实值的网格，而是由一系列  !!!向量值的观测数据!!!  构成的网格。例如，一幅彩色图像在每一个像素点都会有红绿蓝三种颜', 'time': '2017-02-19T13:16'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '多个不同', 'origin_text': '的卷积神经网络中，第二层的输入是第一层的输出，通常在每个位置包含  !!!多个!!!  卷积的输出。当用于图像时，我们通常把卷积的输入输出都看作是3维', 'time': '2017-02-19T13:18'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '处理', 'origin_text': '二层的输入是第一层的输出，通常在每个位置包含多个卷积的输出。当  !!!用于!!!  图像时，我们通常把卷积的输入输出都看作是3维的张量，其中一个索引', 'time': '2017-02-19T13:18'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '实际上它们包含4维向量', 'origin_text': '引标明在每个通道上的空间坐标。软件实现通常使用批处理模式，所以  !!!它们会使用4维的张量!!!  ，第四维索引用于标明批处理中不同的实例，但我们为简明起见这里忽略', 'time': '2017-02-19T13:20'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '所以即使使用了核翻转， 也不一定保证网络的线性运算是可交换的', 'origin_text': '见这里忽略批处理索引。因为卷积神经网络通常使用多通道的卷积，  !!!它们基于的线性运算并不保证一定是可交换的，即使使用了核翻转也是如此!!!  。这些多通道的运算只有当其中的每个运算的输出和输入具有相同的通', 'time': '2017-02-19T13:23'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '只有当其中的每个运算的输出和输入具有相同的通道数时，这些多通道的运算才是可交换的。', 'origin_text': '于的线性运算并不保证一定是可交换的，即使使用了核翻转也是如此。  !!!这些多通道的运算只有当其中的每个运算的输出和输入具有相同的通道数时才是可交换的。!!!  假定我们有一个4维的核张量\\𝗍𝖾𝗑𝗍𝖻𝖿{𝖪}', 'time': '2017-02-19T13:25'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '一个', 'origin_text': '�𝗑𝗍𝖻𝖿{𝖵}\\textbf{V}\\TSV组成，它的  !!!每一个!!!  元素是\\textsf{V}i,j,k\\textsf{V}i,j,', 'time': '2017-02-19T13:26'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '增加', 'origin_text': '的空间维度最终会缩减到1×11×11\\times 1，这种情况下  !!!另外!!!  的层就不可能进行有意义的卷积了。第二种特殊的情况是只进行足够的', 'time': '2017-02-19T13:46'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '只要有硬件支持，网络能包含任意多的卷积层，这是因为卷积运算不改变下一层网络的结构。', 'origin_text': '相同的大小。在MATLAB中，这称为相同卷积。在这种情况下，  !!!网络能够包含任意多的卷积层，只要硬件可以支持，这是因为卷积运算并没有改变相关的结构!!!  。然而，输入像素中靠近边界的部分相比于中间部分对于输出像素的影', 'time': '2017-02-19T13:52'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '例如', 'origin_text': 't+1},\\end{equation}这里百分号是取模运算，  !!!其中!!!  t%t=0,(t+1)%t=1t%t=0,(t+1)%t=1t\\', 'time': '2017-02-20T01:11'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '滤波器', 'origin_text': '有趣的关联：这些层的探测单元都是由不同的过滤器驱动的。如果这些  !!!过滤器!!!  能够学会探测相同隐含特征的不同变换形式，那么最大池化的单元对于学', 'time': '2017-02-20T01:14'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '通常需要除卷积以外的其他运算', 'origin_text': '。卷积层对于平移具有内置的不变性。实现卷积神经网络时，  !!!采用除卷积以外的其他一些运算通常也是必须的!!!  。为了实现学习，必须在给定输出的梯度时能够计算核的梯度。在一', 'time': '2017-02-20T01:17'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '在卷积层用于反向传播误差导数计算', 'origin_text': '算。通过卷积定义的矩阵转置的乘法就是这样一种运算。这种运算  !!!用于通过卷积层反向传播误差的导数!!!  ，所以它在训练多于一个隐藏层的卷积神经网络时是必要的。如果我们', 'time': '2017-02-20T01:23'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '得到', 'origin_text': '其余部分并且被用来计算损失函数JJJ。在反向传播过程中，我们会  !!!收到!!!  一个张量\\𝗍𝖾𝗑𝗍𝖻𝖿{𝖦}\\textbf{G}\\', 'time': '2017-02-20T01:42'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '介绍\n', 'origin_text': '_{q, l, n}.\\end{eqnarray}第\\?章  !!!描述!!!  的自编码器网络，是一些训练成把输入拷贝到输出的前馈网络。一个简', 'time': '2017-02-20T01:44'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '被训练成', 'origin_text': 'end{eqnarray}第\\?章描述的自编码器网络，是一些  !!!训练成!!!  把输入拷贝到输出的前馈网络。一个简单的例子是PCA算法，将输入', 'time': '2017-02-20T01:45'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '我们有充分的理由相信：当计算成本和过拟合不是主要问题时，卷积神经网络是很好的一个网络模型。', 'origin_text': '入。这些类型的输入不能用传统的基于矩阵乘法的神经网络来表示。  !!!这提供了使用卷积网络的令人信服的理由，即使当计算成本和过拟合也不是主要问题时。!!!  例如，考虑一组图像的集合，其中每个图像具有不同的高度和宽度。', 'time': '2017-02-20T02:23'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '直接', 'origin_text': '清楚如何用固定大小的权重矩阵对这样的输入进行建模。卷积就可以很  !!!直观!!!  的应用；核依据输入的大小简单地被使用不同次，并且卷积运算的输出也', 'time': '2017-02-20T02:30'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '成绩等级', 'origin_text': '种类的观察。例如，如果我们正在处理大学申请，并且我们的特征包括  !!!成绩!!!  和标准化测试分数，但不是每个申请人都进行了标准化测试，则使用相同', 'time': '2017-02-20T02:42'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '需要进一步斟酌，但我也没有好的表述方法', 'origin_text': '积算法现代卷积网络的应用通常涉及包含超过百万个单元的网络。  !!!强大的实现!!!  要利用并行计算资源，很关键的，如\\?节中所描述。然而，在很多情', 'time': '2017-02-20T02:45'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '针对某些问题的规模，这种算法比离散卷积的实现更快。', 'origin_text': '，执行两个信号的逐点相乘，并使用Fourier逆变换转换回时域。  !!!对于某些问题大小，这可能比离散型卷积的朴素实现更快!!!  。当一个ddd维的核可以表示成ddd个向量（每维一个向量）的', 'time': '2017-02-20T02:51'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '元素', 'origin_text': '并且核也需要更少的参数表示为向量。如果核在每一维都是www个  !!!元素宽!!!  ，那么朴素的多维卷积需要O(wd)O(wd)O(w^d)的运行时', 'time': '2017-02-20T03:05'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '如何设计', 'origin_text': '和参数存储空间。当然，并不是每个卷积都可以表示成这种形式。  !!!设计!!!  更快的执行卷积或近似卷积，而不损害模型准确性的方法，是一个活跃的', 'time': '2017-02-20T03:07'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '消耗资源', 'origin_text': '网络还要耗资源。随机或无监督的特征通常，卷积网络训练中最  !!!昂贵!!!  的部分是学习特征。 输出层通常相对便宜，因为在通过若干层池化之', 'time': '2017-02-20T03:08'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '每步梯度计算需要完整的运行整个网络的前向传播和反向传播', 'origin_text': '作为该层输入的特征的数量较少。当使用梯度下降执行有监督训练时，  !!!每个梯度步骤需要完整的运行前向传播和反向传播通过整个网络!!!  。减少卷积网络训练成本的一种方式是使用那些不是通过有监督方式训', 'time': '2017-02-20T03:14'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '一个用于特征学习的中间方法', 'origin_text': '的性能，然后选择最好的结构并使用更昂贵的方法来训练整个网络。  !!!一个中间方法是学习特征，但是使用一些特殊的方法，这些方法!!!  不需要在每个梯度步骤中都进行完整的前向和反向传播。与多层感知机', 'time': '2017-02-20T03:22'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '单独地', 'origin_text': '的前向和反向传播。与多层感知机一样，我们使用贪心逐层式预训练，  !!!独立地!!!  训练第一层，然后从第一层提取所有特征一次，然后用那些特征隔离训练', 'time': '2017-02-20T03:24'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '单独地', 'origin_text': '，独立地训练第一层，然后从第一层提取所有特征一次，然后用那些特征  !!!隔离!!!  训练第二层，以此类推。第\\?章描述了如何实现有监督的贪心逐层预', 'time': '2017-02-20T03:24'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '案例', 'origin_text': '络的神经科学基础卷积网络也许是生物学启发人工智能的最为成功的  !!!故事!!!  。虽然卷积网络已经被许多其他领域指导，但是神经网络的一些关键设', 'time': '2017-02-20T03:31'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '虽然卷积神经网络在很多其他领域做了修改', 'origin_text': '科学基础卷积网络也许是生物学启发人工智能的最为成功的故事。  !!!虽然卷积网络已经被许多其他领域指导!!!  ，但是神经网络的一些关键设计原则来自神经科学。卷积网络的历史', 'time': '2017-02-20T03:33'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '此处应另起一段', 'origin_text': '述V1的三个性质：\\begin{enumerate}+ V1  !!!布置在空间图中。!!!    它实际上具有二维结构来反映视网膜中的图像结构。  例如，', 'time': '2017-02-20T03:41'}
{'user': 'acct:zhaoyu611@hypothes.is', 'text': '视图', 'origin_text': '相同的基本原理也适用于视觉系统的其他区域。在我们视觉系统的卡通  !!!试图!!!  中，当我们逐渐深入大脑时，遵循池化的基本探测策略被反复执行。当', 'time': '2017-02-20T03:44'}

=============================   Replies   =============================

{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这个很厉害~我真没注意过这个细节...', 'time': '2017-03-12T14:33'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里不是这样的...原文是\n> This provides a compelling reason to use convolutional networks even when computational cost and overfitting are not significant issues.\n\n因为卷积网络的计算开销会小很多，所以这里才说“即使计算开销和过拟合不是主要问题时，也应该使用卷积网络”。', 'time': '2017-03-12T14:17'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这样说不太好理解...还是简单一点吧...', 'time': '2017-03-12T10:05'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '滤波器会很怪，会让人自然联想到低通滤波器之类的...filter还是翻译成过滤器吧', 'time': '2017-03-12T09:04'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里翻译很棒~', 'time': '2017-03-12T08:04'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这样翻译不太对啊...因为输出会包含很多个通道（例如红绿蓝），然后处在这个通道中的一个单元例如（x,y）。', 'time': '2017-03-12T07:52'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里如果翻译成强度会怪怪的...intensity有亮度的含义，所以我建议翻译成亮度', 'time': '2017-03-12T07:36'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': 'can 翻译成 “可能”更合适吧...', 'time': '2017-03-12T06:30'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里我改成了“关于那一层输入大小的函数”，因为如果说“这一层”可能会引起歧义。', 'time': '2017-03-12T05:18'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里翻译很赞~', 'time': '2017-03-11T08:30'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '为了统一，我们对representation统一翻译为表示。注意这里不是输出的表示，因为输出=表示，这里是输入的表示。', 'time': '2017-03-11T08:20'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '直译确实是“在所有的情况下”，但是这里作为一段话的开头不是很合适，因为读者肯定会产生“你所指的情况是什么”的疑问，所以我这里采用意译。', 'time': '2017-03-11T08:17'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里感觉"尺度"和"旋转"并列在一起有点奇怪，我就改成了“角度”，要不我都改成“对于图像的放缩或者旋转变换”？', 'time': '2017-03-11T07:15'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '额...我觉得还是平移函数 比较好...', 'time': '2017-03-08T07:41'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里翻译很赞~我对于这种长句子的翻译有时读起来会很奇怪...', 'time': '2017-03-08T07:33'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '边界设计策略感觉不是很好...因为会让人误以为是设计不同形状的边界。我倾向于改成 对边界决策的设计。', 'time': '2017-03-08T07:20'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里改成时间复杂度会有点怪...原文是\n> the sparsely connected approach requires only $k \\times n$ parameters and $O(k \\times n)$\nruntime.\n\n如果改成时间复杂度，读起来会变成“稀疏的连接方法只需要$O(k \\times n)$的时间复杂度”，而我们一般情况下会说“稀疏的连接方法的时间复杂度只有$O(k\\times n)$”，所以我还是建议保留 运行时间，你认为怎么样？', 'time': '2017-03-08T06:54'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里不是参数矩阵和参数矩阵的乘法，而是 参数矩阵*输入=输出 这种关系', 'time': '2017-03-08T06:22'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里翻译很赞~', 'time': '2017-03-08T04:26'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这里我觉得省略“上下文”会简洁一些...', 'time': '2017-03-08T04:12'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '这样说太拗口了...我改成了"我们将核翻转的唯一原因在于得到可交换性。"', 'time': '2017-03-08T04:04'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': 'straightforward 我觉得此处翻译为 简单 更加合适一些。', 'time': '2017-03-07T12:55'}
{'user': 'acct:KevinLee1110@hypothes.is', 'text': '感觉“实变函数”更为常见？', 'time': '2017-03-06T15:45'}