{'user': 'acct:cnscottzheng@hypothes.is', 'text': '这里用“普适于” 好像不太准确。', 'origin_text': '很多信息处理任务，非常容易或者非常困难取决于信息是如何表示的。  !!!这是一个普适于日常生活，普适于计算机科学的基本原则，也普适于机器学习。!!!  例如，对于人而言，使用长除法计算210除以6非常直观。但如果', 'time': '2017-02-08T11:39'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '"计算CCX除以VI时"  不知道什么意思', 'origin_text': '但如果使用罗马数字表示，这个问题就没那么直观了。大部分现代人在  !!!计算CCX除以VI时!!!  ，都会将其转化成阿拉伯数字，从而使用位值系统的长除法。更具体地', 'time': '2017-02-08T11:36'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '拗口', 'origin_text': 'oftmax回归分类器。网络的其余部分学习出分类器输入的表示。  !!!监督准则下的训练自然会使得每个隐藏层（比较接近顶层的隐藏层）的表示趋向于具有使训练任务更容易的性质。!!!  例如，输入特征线性不可分的类别可能在最后一个隐藏层变成线性可分的', 'time': '2017-02-08T11:37'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '如何？', 'origin_text': '很多方法来利用无标记的数据。在本章中，我们主要探讨未标记的数据  !!!可以!!!  学习出更好的表示。贪心地逐层无监督预训练无监督学习在', 'time': '2017-02-08T11:42'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '历史上应该去掉吧', 'origin_text': '示。贪心地逐层无监督预训练无监督学习在深度神经网络的  !!!复兴历史上!!!  起到了关键作用，使研究者首次可以训练不含诸如卷积或者循环这类特殊', 'time': '2017-02-08T11:43'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '是？', 'origin_text': '程称为无监督预训练，或者更精确地，贪心逐层无监督预训练。此过程  !!!时!!!  一个任务（无监督学习，尝试抓住输入分布的形状）的表示如何有助于另', 'time': '2017-02-08T11:44'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '发现是不是多余？', 'origin_text': '可以追溯神经认知机{cite?}。深度学习的复兴始于2006年  !!!发现!!!  ，这种贪心学习的过程能够为多层联合训练过程找到一个好的初始值，甚', 'time': '2017-02-08T11:49'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '在发现此算法之前？', 'origin_text': '到一个好的初始值，甚至可以成功训练全连接的结构{cite?}。  !!!在此发现之前!!!  ，只有深度卷积网络或深度循环网络这类特殊结构的深度网络被认为是有', 'time': '2017-02-08T11:50'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '建议保留一个“整个”或“两阶段”词即可', 'origin_text': '常而言，”预训练”不仅单指预训练阶段，也指结合预训练和监督学习的  !!!整个两阶段!!!  学习过程。监督学习阶段可能会使用预训练阶段得到的顶层特征训练一', 'time': '2017-02-08T11:53'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '这是', 'origin_text': '\\sec?所探讨的，也可以进行贪心逐层\\emph{监督}预训练。  !!!这!!!  建立在训练浅层模型比深度模型更容易的前提下，而该前提似乎在一些情', 'time': '2017-02-08T11:54'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '不通顺', 'origin_text': '练的特征添加线性分类器，那么学习到的特征必须使底层类别线性可分。  !!!这些性质通常自然地发生!!!  ，但并非总是这样。这是另一个监督和无监督学习同时训练更可取的原', 'time': '2017-02-08T11:59'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '严格？', 'origin_text': '数目很少（每类几个到几十个）。这些效果也出现在被{cite?}  !!!仔细!!!  控制的实验中。还有一些其他的因素可能会涉及。例如，当要学习', 'time': '2017-02-08T12:00'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '不太通顺', 'origin_text': '曲线就越好。对于深度表示而言，迁移任务只需要较少的标记样本就能  !!!明显地渐近泛化性能!!!  。迁移学习的两种极端形式是一次学习和零次学习，有时也被称', 'time': '2017-02-09T09:52'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '多余的符号吧', 'origin_text': 'xx\\Vx，传统输出或目标yy\\Vy，以及描述任务的附加随机变量  !!!，!!!  TTT。该模型被训练来估计条件分布p(y∣x,T)p(y∣x,', 'time': '2017-02-09T09:54'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '建议改成“从20世纪90年代以来”', 'origin_text': '∣x)p(y∣x)p(\\Vy \\mid \\Vx)的一种良好表示。  !!!从至少20世纪90年代以来!!!  ，这个想法已经指导了大量的深度学习研究工作~{cite?}。关', 'time': '2017-02-16T02:45'}
{'user': 'acct:cnscottzheng@hypothes.is', 'text': '多了空格', 'origin_text': '外，目标函数可能还有一些其他规律。例如，具有最大池化的卷积网络  !!!!!!  可以不考虑对象在图像中的位置（即使对象的空间变换不对应输入空间的', 'time': '2017-02-16T02:58'}

=============================   Replies   =============================

{'user': 'acct:liber145@hypothes.is', 'text': '感觉不是指发现此算法，而是发现该贪心算法有利于初始化。', 'time': '2017-03-05T15:30'}
{'user': 'acct:liber145@hypothes.is', 'text': '监督学习训练模型，一般会使得模型的各个隐藏层（特别是接近顶层的隐藏层）的表示能够更加容易地完成训练任务。\n\n这样你看如何。', 'time': '2017-03-05T15:22'}
{'user': 'acct:liber145@hypothes.is', 'text': '嗯，现改为 大部分现代人在进行罗马数字计算CCX除以VI时', 'time': '2017-03-05T15:03'}