{'user': 'acct:huangpingchun@hypothes.is', 'text': '原文\nA regularized autoencoder can be nonlinear and\novercomplete but still learn something useful about the data distribution even if\nthe model capacity is great enough to learn a trivial identity function\n\n调整后：即使容量足够大的模型去学习一个无意义的恒等函数，非线性和过完备的正则化自编码器仍然能从数据分布中学习到一些有趣的结构，\n\n1：‘模型容量大到足够学习一个简单的复制功能’，让人容易理解为 这个‘简单的复制功能’需要模型足够大才能学习。\n2：为啥 a trivial identity function 不翻译为‘无意义的恒等函数’而翻译为‘简单的复制功能’，从机理上说并不是copying（尽管作者大量使用copying task这种术语），更多是学习映射（恒等函数），直译过来感觉让人理解起来不是太精确。 后面‘有用的信息’的表达个人感觉也不够‘达意’，我想真正的表述应该类似”we can discover interesting structure about the data （见UFLDL Autoencoders and Sparsity 一节）应该能点出数据中的某种隐藏的结构。', 'origin_text': '些特性包括稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性。  !!!即使模型容量大到足够学习一个简单的复制功能，非线性且过完备的正则自动编码器仍然能学到一些与数据分布相关的有用信息!!!  。除了这里所描述的方法（正则化自动编码器最自然的解释），几乎', 'time': '2017-01-12T10:47'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '一个由函数h=f(x)表示的编码器', 'origin_text': 'hh\\Vh，可以产生编码表示输入。该网络可以看作由两部分组成：  !!!一个编码器函数h=f(x)!!!  h=f(x) \\Vh = f(\\Vx)和一个生成重构的解码器r=', 'time': '2017-01-13T12:37'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '如果一个自动编码器只是设定g(f(x))=x为目标而进行简单地学习，那么这个自动编码器就没什么特别的用处。', 'origin_text': ')r=g(h)\\Vr=g(\\Vh)。\\fig?展示了这种架构。  !!!如果一个自动编码器学会简单地设置g(f(x))=xg(f(x))=xg(f(\\Vx)) =\\Vx，那么这个自动编码器就不会特别有用。!!!  相反，自动编码器应该被设计成不能学会完美地复制。这通常需要强', 'time': '2017-01-12T11:20'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '与训练数据相似的输入', 'origin_text': '这通常需要强加一些约束，使自动编码器只能近似地复制，并只能复制  !!!类似训练数据的输入!!!  。这些约束强制模型划定输入数据不同方面的主次顺序，因此它往往能', 'time': '2017-01-12T11:15'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '自动编码器的思想成为神经网络发展史的一部分（？）', 'origin_text': '{decoder}}(\\Vx \\mid \\Vh)。数十年间，  !!!自动编码器的想法一直是神经网络历史景象的一部分~{cite?}!!!  。传统自动编码器被用于降维或特征学习。近年来，自动编码器与隐', 'time': '2017-01-12T11:34'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '生成式', 'origin_text': '习。近年来，自动编码器与隐变量模型理论的联系将自动编码器带到了  !!!生成!!!  建模的前沿，我们将在\\chap?看到更多细节。自动编码器可以被', 'time': '2017-01-12T11:36'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '揭示', 'origin_text': '理论的联系将自动编码器带到了生成建模的前沿，我们将在\\chap?  !!!看到!!!  更多细节。自动编码器可以被看作是前馈网络的一种特殊情况，并且可', 'time': '2017-01-12T11:37'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '一个特例', 'origin_text': '将在\\chap?看到更多细节。自动编码器可以被看作是前馈网络的  !!!一种特殊情况!!!  ，并且可以使用完全相同的技术进行训练，通常使用minibatch', 'time': '2017-01-12T11:38'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '通常情况下即基于误差反向传播算法的minibatch梯度下降法', 'origin_text': '作是前馈网络的一种特殊情况，并且可以使用完全相同的技术进行训练，  !!!通常使用minibatch梯度下降法（基于反向传播计算的梯度）!!!  。不像一般的前馈网络，自动编码器也可以使用再循环训练{cite', 'time': '2017-01-12T11:41'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '比较网络对于原始输入的激活值和重构结果的激活值', 'origin_text': '网络，自动编码器也可以使用再循环训练{cite?}，这是一种基于  !!!比较原始输入和重构输入激活!!!  的学习算法。相比反向传播算法，再循环算法从生物学上看似更有道理', 'time': '2017-01-12T11:49'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '对输入进行复制', 'origin_text': '但我们通常不关心解码器的输出。相反，我们希望通过训练自动编码器  !!!对输入进行复制的任务!!!  使hh\\Vh获得有用的特性。从自动编码器获得有用特征的一种方', 'time': '2017-01-12T11:56'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '对于g(f(x))和x的差异予以惩罚（？）', 'origin_text': '\\Vx)),\\end{align}其中LLL是一个损失函数，  !!!衡量g(f(x))g(f(x))g(f(\\Vx))与xx\\Vx的不相似性!!!  ，如均方误差。当解码器是线性的且LLL是均方误差，欠完备的自', 'time': '2017-01-12T11:59'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '自动编码器被训练复制输入的同时会学习到训练数据的主元子空间', 'origin_text': '完备的自动编码器会学习出与PCA相同生成子空间。在这种情况下，  !!!自动编码器学到了训练数据的主元子空间（执行复制任务的副效用）!!!  。因此拥有非线性编码函数fff和非线性解码器函数ggg的自动', 'time': '2017-01-12T12:08'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '过大', 'origin_text': '出更强大的PCA非线性推广。不幸的是，如果编码器和解码器被赋予  !!!太大!!!  的容量，自动编码器会执行复制任务而捕捉不到任何有关数据分布的有用', 'time': '2017-01-12T12:10'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '实际情况', 'origin_text': '以学习将这些整数索引映射回特定训练样本的值。这种特定情形不会在  !!!实践!!!  中发生，但它清楚地说明，如果自动编码器的容量太大，那训练来执行复', 'time': '2017-01-12T12:19'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '我们可以设想有这样一个自动编码器，它只有一维编码，但是它具有一个强大到可以利用每个编码i去表示每一个训练样本x(i)的非线性编码器，以及可以将这些整数索引映射回特定的训练样本数据的解码器（？）', 'origin_text': '执行复制任务而捕捉不到任何有关数据分布的有用信息。从理论上说，  !!!我们可以想象只有一维编码的自动编码器，但具有一个非常强大的非线性编码器，能够将每个训练数据x(i)x(i)\\Vx^{(i)}表示为编码\xa0i\xa0i~i。解码器可以学习将这些整数索引映射回特定训练样本的值。!!!  这种特定情形不会在实践中发生，但它清楚地说明，如果自动编码器的容', 'time': '2017-01-12T12:30'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'These models naturally learn high-capacity, overcomplete encodings of the input\nand do not require regularization for these encodings to be useful.Their encodings\nare naturally useful because the models were trained to approximately maximize\nthe probability of the training data rather than to copy the input to the output.\n\n调整后：\n\n“这些变种（或衍生）自编码器能够学习出高容量且过完备的模型，进而发现输入数据中有用的结构信息，并且也无需对数据做正则化。\n相对于只学习简单恒等函数的自编码器而言，变种（衍生自编码器）产生的编码数据更有效，因为得到的模型是在训练数据集上学习的数据概率分布。”\n\n理由：\n\n    这段原文指代感觉比较不清晰，\n    1：我觉得  capacity、overcomplete 是用来修饰模型的，在前文也是这样使用的，在这句突然用来修饰输入数据的编码 不是太合理。\n    2：the models were trained to approximately maximize the probability of the training data 说说对这句的理解，感觉隐藏了很多信息。\n      译文中“这些模型被训练为近似训练数据的最大概率......” 这句我的理解是 为求解问题构造的目标损失函数是一种概率函数，以最大化概率值为目标。\n      而不是产生编码数据的方式是按照近似最大概率的原则产生，因为一旦学习结束，分布就已然得到（通过下文提到的概率分布代入求解），可以直接产生新编码样本点了。因为产生z_mean和z_log_sigma（见相关背景资料）这两个值都是统计量纲，不会有最大化概率这种动作。\n\n      相关背景：\n\n       VAE是个生成模型，三步：\n        首先，建立编码网络，将输入映射为隐分布的参数\n        然后从这些参数确定的分布中采样，这个样本相当于之前的隐层值\n        最后，将采样得到的点映射回去重构原输入。\n      由此可知变分编码器的工作原理：\n        首先，编码器网络将输入样本x转换为隐空间的两个参数，记作z_mean和z_log_sigma。然后，我们随机从隐藏的正态分布中采样得到数据点z，\n        这个隐藏分布我们假设就是产生输入数据的那个分布。z = z_mean + exp(z_log_sigma)*epsilon，epsilon是一个服从正态分布的张量。最后，\n        使用解码器网络将隐空间映射到显空间，即将z转换回原来的输入数据空间。\n        即：变分编码器不再学习一个恒等的函数，而是学习数据概率分布的一组参数。通过在这个概率分布中采样。\n        隐空间参数由两个损失函数来训练，一个是重构损失函数，该函数要求解码出来的样本与输入的样本相似（与之前的自编码器相同），第二项损失函数\n        是学习到的隐分布与先验分布的KL距离，作为一个正则。实际上把后面这项损失函数去掉也可以，它对学习符合要求的隐空间和防止过拟合有帮助。\n\n拓扑结构图：\n\n![](http://www.dengfanxin.cn/wp-content/uploads/2016/11/222.png)', 'origin_text': '，如变分自动编码器（\\sec?）和生成随机网络（\\sec?）。  !!!这些模型能自然地学习大容量、对输入过完备的有用编码，而不需要正则化。这些编码显然是有用的，因为这些模型被训练为近似训练数据的最大概率而不是将输入复制到输出。!!!  稀疏自动编码器稀疏自动编码器简单地在训练时结合编码层', 'time': '2017-01-13T11:07'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'we can think of the entire sparse autoencoder framework as approximating maximum likelihood training of a generative model that has latent variables. \n\n这句话是否可以帮助理解\n\nTheir encodings are naturally useful because the models were trained to approximately maximize the probability of the training data rather than to copy the input to the output.呢？\n近似最大化概率（似然）是指的对隐变量训练，这个解析比较明确了。', 'origin_text': '验。我们仍可以认为这些正则项隐式地表达了对函数的偏好。  !!!我们可以认为整个稀疏自动编码器框架是对带有隐变量的生成模型的近似最大似然训练!!!  ，而不将稀疏惩罚视为复制任务的正则化。假如我们有一个带有可见变', 'time': '2017-01-16T03:52'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '不太通顺\n\nhis means\nthat an autoencoder with a single hidden layer is able to represent the identity\nfunction along the domain of the data arbitrarily well.\n\n这意味着具有单隐层的自编码器在数据域内能逼近任意恒等函数。', 'origin_text': '度近似任意函数（在很大范围里），这是非平凡深度的一个主要优点。  !!!这意味着单层隐藏层的自动编码器在数据范围能表示任意接近数据的恒等函数。!!!  但是，从输入到编码的映射是浅层的。这意味这我们不能任意添加约', 'time': '2017-01-16T10:23'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'non-trivial depth 我想直译较为生硬，不是太好理解\n此句调整为 “这是有一定深度的网络其最主要的优点”', 'origin_text': '元足够多的前馈神经网络能以任意精度近似任意函数（在很大范围里），  !!!这是非平凡深度的一个主要优点!!!  。这意味着单层隐藏层的自动编码器在数据范围能表示任意接近数据的', 'time': '2017-01-17T08:49'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '缺少一个右括号', 'origin_text': '能学到有用信息的自动编码器。传统的自动编码器最小化以下目标  !!!L(x,g(f(x)),!!!  L(x,g(f(x)),\\begin{align}L(\\Vx,', 'time': '2017-01-17T01:57'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '缺少一个右括号', 'origin_text': 'irc  f学成一个恒等函数。相反，去噪自动编码器最小化   !!!L(x,g(f(x~)),!!!  L(x,g(f(x~)),\\begin{align}L(\\Vx', 'time': '2017-01-17T01:57'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '第一项损失函数缺少一个右括号', 'origin_text': '的策略是使用一个类似稀疏自动编码器中的惩罚项ΩΩ\\Omega，  !!!L(x,g(f(x))+Ω(h,x),L(x,g(f(x))+Ω(h,x),!!!  \\begin{align}L(\\Vx, g(f(\\Vx)) +', 'time': '2017-01-17T01:58'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '历史展望', 'origin_text': '成模型，并从该分布中进行采样。这将在\\sec?中讨论。  !!!历史观点!!!  采用MLP去噪的想法可以追溯到{Lecun-these87}', 'time': '2017-01-17T07:57'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': ' a deep autoencoderwith gradually smaller hidden layers\n一个隐含层逐渐减小的深度自动编码器\n(仍觉得逐渐减小翻译不太好，可否增加一些词语改为“一个隐含层中神经元个数逐渐减少的深度自动编码器”)', 'origin_text': 'ence2006}训练了一个堆叠RBM，然后利用它们的权重初始化  !!!一个深度自动编码器并逐渐变小隐藏层!!!  ，在30个单元的瓶颈处达到极值。生成的编码比30维的PCA产生', 'time': '2017-01-17T10:40'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': ' culminating in a bottleneck of 30 units\n终结于30个单元的瓶颈（原文中感觉没有写到“极值”）', 'origin_text': 'M，然后利用它们的权重初始化一个深度自动编码器并逐渐变小隐藏层，  !!!在30个单元的瓶颈处达到极值!!!  。生成的编码比30维的PCA产生更少的重构误差，所学到的表示更', 'time': '2017-01-17T10:45'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'Many forms of dimensionality reduction place semantically related examples near each other\n许多降维的形式会将语义上相关的样本彼此置于相近的位置。', 'origin_text': '和{Torralba+Fergus+Weiss-2008}观察，  !!!降维的许多形式是跟彼此邻近的样本语义相关的!!!  。映射到低维空间能帮助泛化提示了这个想法。从降维中比普通任', 'time': '2017-01-17T10:53'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'The hints provided by the mapping to the lower-dimensional space aid generalization.映射到低维空间所提供的线索有助于泛化。', 'origin_text': 's-2008}观察，降维的许多形式是跟彼此邻近的样本语义相关的。  !!!映射到低维空间能帮助泛化提示了这个想法!!!  。从降维中比普通任务受益更多的是信息检索，即在数据库中查询类', 'time': '2017-01-17T10:57'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'compute the learned features已经习得的特征（原译处可能会产生歧义）', 'origin_text': '码器 [Math Processing Error]ff用于计算  !!!学习好的特征!!!  。相比通过梯度下降推断[Math Processing Err', 'time': '2017-01-18T11:10'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '不应该将自编码器设计成输入到输出完全相等', 'origin_text': '(\\Vx)) =\\Vx，那么这个自编码器就不会特别有用。相反，  !!!自编码器应该被设计成不能学会完美地复制!!!  。这通常需要强加一些约束，使自编码器只能近似地复制，并只能复制', 'time': '2017-01-19T07:22'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '加一些约束，让它只能近似的复制', 'origin_text': '不会特别有用。相反，自编码器应该被设计成不能学会完美地复制。  !!!这通常需要强加一些约束，使自编码器只能近似地复制!!!  ，并只能复制类似训练数据的输入。这些约束强制模型划定输入数据不', 'time': '2017-01-19T07:23'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '限制', 'origin_text': '自编码器只能近似地复制，并只能复制类似训练数据的输入。这些约束  !!!强制!!!  模型划定输入数据不同方面的主次顺序，因此它往往能学习到数据的有用', 'time': '2017-01-19T07:26'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '将输入数据按照不同方面划定主次顺序', 'origin_text': '只能近似地复制，并只能复制类似训练数据的输入。这些约束强制模型  !!!划定输入数据不同方面的主次顺序!!!  ，因此它往往能学习到数据的有用特性。现代自编码器将编码器和解', 'time': '2017-01-19T07:27'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '概念推而广之', 'origin_text': '它往往能学习到数据的有用特性。现代自编码器将编码器和解码器的  !!!思想推广!!!  ，将其中的确定函数推广为随机映射pencoder(h∣x)pen', 'time': '2017-01-19T07:29'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '联结', 'origin_text': '编码器被用于降维或特征学习。近年来，自编码器与隐变量模型理论的  !!!联系!!!  将自编码器带到了生成建模的前沿，我们将在\\chap?看到更多细节', 'time': '2017-01-20T02:19'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '其', 'origin_text': '被用于降维或特征学习。近年来，自编码器与隐变量模型理论的联系将  !!!自编码器!!!  带到了生成建模的前沿，我们将在\\chap?看到更多细节。自编码', 'time': '2017-01-20T02:18'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '不同于', 'origin_text': '常使用minibatch梯度下降法（基于反向传播计算的梯度）。  !!!不像!!!  一般的前馈网络，自编码器也可以使用再循环训练{cite?}，这是', 'time': '2017-01-19T07:32'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '再循环算法更具生物学意义，不过较少引入到机器学习领域。', 'origin_text': '种基于比较原始输入和重构输入激活的学习算法。相比反向传播算法，  !!!再循环算法从生物学上看似更有道理，但很少用于机器学习。!!!  \\begin{figure}[!htb]\\ifOpenSou', 'time': '2017-01-19T07:34'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '类似的', 'origin_text': '码器是线性的且LLL是均方误差，欠完备的自编码器会学习出与PCA  !!!相同!!!  生成子空间。在这种情况下，自编码器学到了训练数据的主元子空间（', 'time': '2017-01-19T07:39'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '解码', 'origin_text': '执行复制任务的副效用）。因此拥有非线性编码函数fff和非线性  !!!解码器!!!  函数ggg的自编码器能够学习出更强大的PCA非线性推广。不幸的', 'time': '2017-01-19T07:40'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '我们已经知道', 'origin_text': '码维数小于输入维数的欠完备自编码器可以学习数据分布最显著的特征。  !!!我们已经看到!!!  ，如果这类自编码器被赋予过大的容量，它就不能学到任何有用的信息。', 'time': '2017-01-19T07:42'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '赋予的容量过大', 'origin_text': '可以学习数据分布最显著的特征。我们已经看到，如果这类自编码器被  !!!赋予过大的容量!!!  ，它就不能学到任何有用的信息。如果隐藏层编码的维数允许与输入', 'time': '2017-01-19T07:43'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '超完备', 'origin_text': '如果隐藏层编码的维数允许与输入相等，或隐藏层编码维数大于输入的  !!!过完备!!!  情况下，会发生类似的问题。在这些情况下，即使是线性编码器和线性', 'time': '2017-01-19T07:44'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '这样的能力', 'origin_text': '解码器容量，就可以成功训练任意架构的自编码器。正则自编码器提供  !!!这样做的可能!!!  。正则自编码器使用的损失函数可以鼓励模型学习其它特性（除了将输', 'time': '2017-01-19T07:50'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '不必', 'origin_text': '用的损失函数可以鼓励模型学习其它特性（除了将输入复制到输出），而  !!!不用!!!  限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量。', 'time': '2017-01-19T07:51'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '一般用来', 'origin_text': '=f(x)h=f(x)\\Vh = f(\\Vx)。稀疏自编码器  !!!通常用于!!!  学习特征，以便用于其他任务如分类。稀疏正则化的自编码器必须反映', 'time': '2017-01-19T07:56'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '以便用于像分类这样的任务', 'origin_text': 'x)\\Vh = f(\\Vx)。稀疏自编码器通常用于学习特征，  !!!以便用于其他任务如分类!!!  。稀疏正则化的自编码器必须反映训练数据集的独特统计特征，而不是', 'time': '2017-01-20T10:01'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '两句合并\n“不像其它正则项如权重衰减——没有直观的贝叶斯解释。”', 'origin_text': '能地根据这些稀疏特征执行一些监督学习任务（根据监督学习的目标）。  !!!不像其它正则项如权重衰减，这个正则化没有直观的贝叶斯解释。!!!  如\\sec?描述，权重衰减和其他正则惩罚可以被解释为一个MAP', 'time': '2017-01-19T08:03'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '它', 'origin_text': '这个正则化没有直观的贝叶斯解释。如\\sec?描述，权重衰减和其  !!!他!!!  正则惩罚可以被解释为一个MAP近似贝叶斯推断，正则化的惩罚对应于', 'time': '2017-01-19T08:03'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '它虽不是一个先验', 'origin_text': '是因为正则项取决于数据，因此根据定义上（从文字的正式意义）来说，  !!!它不是一个先验!!!  。我们仍可以认为这些正则项隐式地表达了对函数的偏好。我', 'time': '2017-01-19T08:06'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '趋向', 'origin_text': '}的方式。该想法是使用整流线性单元产生编码层。基于将表示真正  !!!推向!!!  零（如绝对值惩罚）的先验，可以间接控制表示中零的平均数量。去', 'time': '2017-01-19T08:11'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '我们也可以通过改变重构误差项来得到一个能学到有用信息的自编码器', 'origin_text': '零的平均数量。去噪自编码器除了向代价函数增加一个惩罚项，  !!!我们也可以改变重构误差项得到一个能学到有用信息的自编码器!!!  。传统的自编码器最小化以下目标L(x,g(f(x))),L', 'time': '2017-01-19T08:12'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '差异', 'origin_text': '衡量g(f(x))g(f(x))g(f(\\Vx))与xx\\Vx的  !!!不相似性!!!  ，如它们不相似度的L2L2L^2范数。如果模型被赋予足够的容量', 'time': '2017-01-19T08:13'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '差异', 'origin_text': ')g(f(x))g(f(\\Vx))与xx\\Vx的不相似性，如它们  !!!不相似!!!  度的L2L2L^2范数。如果模型被赋予足够的容量，LLL仅仅鼓', 'time': '2017-01-19T08:13'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '过大', 'origin_text': '的不相似性，如它们不相似度的L2L2L^2范数。如果模型被赋予  !!!足够!!!  的容量，LLL仅仅鼓励g∘fg∘fg \\circ  f学成一个恒', 'time': '2017-01-19T08:14'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '使得', 'origin_text': '似度的L2L2L^2范数。如果模型被赋予足够的容量，LLL仅仅  !!!鼓励!!!  g∘fg∘fg \\circ  f学成一个恒等函数。相反，去噪', 'time': '2017-01-19T08:16'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '各自', 'origin_text': '同样适用于自编码器，因为它也属于前馈网络。此外，编码器和解码器  !!!自身!!!  都是一个前馈网络，因此这两个部分也能各自从深度中获得好处。通', 'time': '2017-01-19T08:24'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '深度结构', 'origin_text': '外，编码器和解码器自身都是一个前馈网络，因此这两个部分也能各自从  !!!深度!!!  中获得好处。通用近似定理, %万能逼近定理保证至少有一层隐含', 'time': '2017-01-19T08:25'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '深度自编码器至少要包含一层额外的隐藏层，在隐藏单元足够多的情况下能以任意精度近似任何从输入到编码的映射。', 'origin_text': '射是浅层的。这意味这我们不能任意添加约束，比如约束编码稀疏。  !!!编码器至少包含一层额外隐藏层的深度自编码器能够在给定足够多隐藏单元的情况，以任意精度近似任何从输入到编码的映射。!!!  深度可以指数地减少表示某些函数的计算成本。深度也能指数', 'time': '2017-01-19T08:52'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '如果x是实数，那么我们通常使用线性输出单元为高斯分布的均值设置相关参数', 'origin_text': 'text{decoder}}的形式而定。就传统的前馈网络来说，  !!!我们通常使用线性输出单元参数化高斯分布的均值（如果xx\\Vx是实的）!!!  。在这种情况下，负对数似然对应均方误差准则。类似地，二值xx', 'time': '2017-01-28T15:40'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '如果x满足Bernoulli分布，那么使用Sigmoid输出单元为相关参数赋值（意译了一下）', 'origin_text': '是实的）。在这种情况下，负对数似然对应均方误差准则。类似地，  !!!二值xx\\Vx对应参数由sigmoid单元确定的Bernoulli分布!!!  ，离散的xx\\Vx对应softmax分布等等。为了便于计算概率', 'time': '2017-01-28T14:38'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '，以此类推', 'origin_text': '的Bernoulli分布，离散的xx\\Vx对应softmax分布  !!!等等!!!  。为了便于计算概率分布，我们通常认为输出变量与给定hh\\Vh是', 'time': '2017-01-28T14:40'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'Typically, the output variables are treated as being conditionally independent given h\n通常认为在给定h情况下的输出变量是条件独立的', 'origin_text': '的xx\\Vx对应softmax分布等等。为了便于计算概率分布，  !!!我们通常认为输出变量与给定hh\\Vh是条件独立的!!!  ，但一些技术（如混合密度输出）可以解决输出相关的建模。为', 'time': '2017-01-28T14:45'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'but some techniques such as mixture density outputs allow tractable modeling of outputs with correlations.但是一些模型，例如混合密度输出，可以解决输出变量之间具有相关性的问题。（意译，仍感觉不太准确）', 'origin_text': '计算概率分布，我们通常认为输出变量与给定hh\\Vh是条件独立的，  !!!但一些技术（如混合密度输出）可以解决输出相关的建模!!!  。为了更彻底地区别之前看到的前馈网络，我们也可以将\\te', 'time': '2017-01-28T14:50'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '与我们之前了解到的前馈网络相区别', 'origin_text': '术（如混合密度输出）可以解决输出相关的建模。为了更彻底地  !!!区别之前看到的前馈网络!!!  ，我们也可以将\\textbf{编码函数}(encoding fu', 'time': '2017-01-28T14:53'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'In general, the encoder and decoder distributions are not necessarily conditional distributions compatible with a unique joint distribution pmodel(x, h)\n通常情况下，编码器和解码器的分布没必要是与唯一一个联合分布相兼容的条件分布。', 'origin_text': '(\\Vx\\mid\\Vh).\\end{align}一般情况下，  !!!编码器和解码器的分布没有必要与一个唯一的联合分布pmodel(x,h)pmodel(x,h)p_{\\text{model}}(\\Vx, \\Vh)的条件分布相容!!!  。{Alain-et-al-arxiv2015}指出将编码器和', 'time': '2017-01-28T15:05'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '在保证足够的容量和样本的情况下，将编码器和解码器作为去噪自编码器训练，能使它们渐近地相容（调整了一下语序）', 'origin_text': '件分布相容。{Alain-et-al-arxiv2015}指出  !!!将编码器和解码器作为去噪自编码器训练，能使它们渐近地相容（有足够的容量和样本）!!!  。去噪自编码器去噪自编码器是一类接受损坏数据作为输入，并', 'time': '2017-01-28T15:07'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'cushi', 'origin_text': '配{cite?}是最大似然的代替。它提供了概率分布的一致估计，  !!!鼓励!!!  模型在各个数据点xx\\Vx上获得与数据分布相同的得分。在这种情', 'time': '2017-01-28T15:57'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'Denoising training of a speciﬁc kind of autoencoder (sigmoidal hidden units,linear reconstruction units) using Gaussian noise and mean squared error as the reconstruction cost以高斯噪声和均方误差作为重构误差，对一种包含sigmoid隐含层，线性重构单元的特定自动编码器的去噪训练过程', 'origin_text': '法因子，即平均均方根重构误差。}\\end{figure}  !!!去噪地训练一类采用高斯噪声和均方误差作为重构误差的特定去噪自编码器（sigmoid隐藏单元， 线性重构单元）!!!  ，与训练一类特定的被称为RBM的无向概率模型是等价的{cite?', 'time': '2017-01-28T16:03'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '决定了在保持x处于流形上的同时，如何微弱地变动x', 'origin_text': '的局部方向的ddd维基向量给出。如\\fig?所示，这些局部方向  !!!说明了我们能如何微小地改变xx\\Vx而一直处于流形上!!!  。\\begin{figure}[!htb]\\ifOpenSo', 'time': '2017-01-29T02:21'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '这一点很关键', 'origin_text': '通过解码器近似地从$\\Vh$中恢复。$\\Vx$是从训练数据挑出的  !!!事实是关键的!!!  ，因为这意味着在自编码器不需要成功重构不属于数据生成分布下的输入', 'time': '2017-01-29T02:25'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '技巧（原文中一些techniques翻译成“技巧”，“手法”更通顺些）', 'origin_text': '自编码器容量的架构约束，也可以是加入到重构代价的一个正则项。这些  !!!技术!!!  一般倾向那些对输入较不敏感的解。\\end{enumerate}\\', 'time': '2017-01-29T02:30'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '驱使', 'origin_text': '同样忽略输入也是没用的。相反，两种推动力结合是有用的，因为它们  !!!迫使!!!  隐藏的表示能捕获有关数据分布结构的信息。重要的原则是，自编码器', 'time': '2017-01-29T02:34'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': ' by making the reconstruction function insensitive to perturbations of the input around the data points, we cause the autoencoder to recover the manifold structure我们可以通过构建对于数据点周围的输入扰动不敏感重构函数，使得自动编码器恢复流形结构', 'origin_text': '并且对流形正交方向的变化不敏感。\\fig?中一维的例子说明，  !!!为了使重构函数对数据点周围的扰动输入不敏感，我们可以让自编码器恢复流形的结构!!!  。\\begin{figure}[!htb]\\ifOpenSo', 'time': '2017-01-29T10:40'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': 'To understand why autoencoders are useful for manifold learning, it is instructive to compare them to other approaches为了理解自动编码器可用于流行学习的原因，可以将自动编码器和其他方法进行对比', 'origin_text': '形，重构函数必须具有大的导数。\\end{figure}  !!!对比其他方法是有用且受启发的，可以了解自编码器为什么对流形学习是有用的!!!  。学习表征流形最常见的是流形上（或附近）数据点的表示。对于特', 'time': '2017-01-29T10:46'}
{'user': 'acct:Seaball0022@hypothes.is', 'text': '曲折（弯曲偏向于动词，此处需要名词）', 'origin_text': '于流形学习的根本困难：如果流形不是很光滑（它们有许多波峰、波谷和  !!!弯曲!!!  ），为覆盖其中的每一个变化，我们可能需要非常多的训练样本，导致没', 'time': '2017-01-29T11:40'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '指数级的降低', 'origin_text': '元的情况，以任意精度近似任何从输入到编码的映射。深度可以  !!!指数地减少!!!  表示某些函数的计算成本。深度也能指数地减少学习一些函数所需的训', 'time': '2017-02-06T04:01'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '深度特性也能帮助在训练一些函数时使所需要的数据量呈指数级降低。', 'origin_text': '编码的映射。深度可以指数地减少表示某些函数的计算成本。  !!!深度也能指数地减少学习一些函数所需的训练数据量!!!  。读者可以参考\\sec?巩固深度在前馈网络中的优势。实验中', 'time': '2017-02-06T04:07'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '本质上', 'origin_text': '我们也经常会遇到浅层自编码器。随机编码器和解码器自编码器  !!!仅仅!!!  是一个前馈网络，可以使用与传统前馈网络相同的损失函数和输出单元。', 'time': '2017-02-07T02:26'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '隐变量', 'origin_text': 'Vx\\mid \\Vh)$）。}\\end{figure}任何  !!!潜变量!!!  模型pmodel(h,x)pmodel(h,x)p_{\\text', 'time': '2017-02-07T02:30'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '去噪得分匹配算法', 'origin_text': 'xt{model}}(\\Vx; \\Vtheta)。当RBM使用  !!!去噪得分匹配!!!  ~{cite?}训练时，它的学习算法与训练对应的去噪自编码器是等', 'time': '2017-02-09T05:38'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '在RBM上应用得分匹配后', 'origin_text': '更详细地讨论去噪得分匹配。自编码器和RBM还存在其他联系。  !!!得分匹配应用于RBM后!!!  ，其代价函数将等价于重构误差结合类似CAE惩罚的正则项 {cit', 'time': '2017-02-09T05:40'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '无意义', 'origin_text': '机是允许使用容量非常大的编码器，同时防止在编码器和解码器学习一个  !!!毫无用处!!!  的恒等函数 。在引入现代DAE之前，{Inayoshi-an', 'time': '2017-02-09T05:57'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '探索了一些同样的方法和目标', 'origin_text': 'DAE之前，{Inayoshi-and-Kurita-2005}  !!!探讨了与一些相同的方法和相同的目标!!!  。他们在监督目标的情况下最小化重构误差 ，并在监督MLP的隐藏', 'time': '2017-02-09T06:18'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '他们的方法除了最小化重构误差这一监督目标项之外，还在隐藏层加入了噪音，目地是通过引入重构误差和注入噪音来改善泛化能力', 'origin_text': '-Kurita-2005}探讨了与一些相同的方法和相同的目标。  !!!他们在监督目标的情况下最小化重构误差 ，并在监督MLP的隐藏层注入噪声，通过引入重构误差和注入噪声提升泛化能力!!!  。然而，他们的方法基于线性编码器，因此无法学习到现代DAE能学', 'time': '2017-02-09T06:59'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'Like many other machine learning algorithms,autoencoders exploit the idea\nthat data concentrates around a low-dimensional manifold or a small set of such\nmanifolds,\n如同其它的机器学习算法,自编码器也探索了集中在低维流形上的数据或者此类流形的小数据集上的想法', 'origin_text': '学习的强大函数族。使用自编码器学习流形如\\sec?描述，  !!!自编码器跟其他很多机器学习算法一样，也应用了将数据集中在一个低维流形或者一小组这样的流形的思想。!!!  其中一些机器学习算法仅能学习到在流形上表现良好但给定不在流形上', 'time': '2017-02-09T07:28'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '相比普通任务，信息检索从降维技术中获得的受益更多', 'origin_text': '的样本语义相关的。映射到低维空间能帮助泛化提示了这个想法。  !!!从降维中比普通任务受益更多的是信息检索!!!  ，即在数据库中查询类似条目的任务。此任务从降维获得类似其他任务', 'time': '2017-02-09T09:06'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '驱动力', 'origin_text': '降维是表示学习和深度学习的第一批应用之一。它是研究自编码器早期  !!!动机!!!  之一。例如， {Hinton-Science2006}训练了一', 'time': '2017-02-09T09:22'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'a stack of RBMs\n\n“栈式” RBM', 'origin_text': '一。例如， {Hinton-Science2006}训练了一个  !!!堆叠!!!  RBM，然后利用它们的权重初始化一个深度自编码器并逐渐变小隐藏层', 'time': '2017-02-09T09:23'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '此任务不仅和其他任务一样从降维技术中获取简单受益，更进一步地在某些低维空间中带来高效搜索性能。', 'origin_text': '普通任务受益更多的是信息检索，即在数据库中查询类似条目的任务。  !!!此任务从降维获得类似其他任务的一般益处，同时在某些种低维空间中的搜索变得极为高效!!!  。特别的，如果我们训练降维算法生成一个低维且\\emph{二值}', 'time': '2017-02-09T09:29'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '多个', 'origin_text': 'moid函数的幅度，直到饱和。学习哈希函数的思想已在其他  !!!数个!!!  方向进一步探讨，包括改变损失训练表示的想法，其中所需优化的损失与', 'time': '2017-02-09T09:42'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '相似', 'origin_text': '的数据库条目作为查询结果进行信息检索。我们也可以非常高效地搜索  !!!稍有不同!!!  条目，只需反转查询编码的各个位。这种通过降维和二值化的信息检索', 'time': '2017-02-09T09:46'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'DAE的机制是允许学习容量很高的编码器', 'origin_text': '与稀疏自编码器、稀疏编码、收缩自编码器等正则化的自编码器类似，  !!! DAE的动机是允许使用容量非常大的编码器!!!  ，同时防止在编码器和解码器学习一个毫无用处的恒等函数 。在引', 'time': '2017-02-09T09:50'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '生成的', 'origin_text': '概率分布。更一般的，我们可能希望使用自编码器作为生成模型，并从  !!!该!!!  分布中进行采样。这将在\\sec?中讨论。历史观点采', 'time': '2017-02-09T09:51'}

=============================   Replies   =============================

{'user': 'acct:swordyork@hypothes.is', 'text': '多谢，这里我完全理解反了。', 'time': '2017-03-01T12:08'}
{'user': 'acct:swordyork@hypothes.is', 'text': '这里exploit应该是利用的意思。', 'time': '2017-03-01T12:02'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': 'OK 了解', 'time': '2017-02-10T07:18'}
{'user': 'acct:swordyork@hypothes.is', 'text': '这3处我得再仔细考虑一下，有点难讲清楚。', 'time': '2017-02-07T14:28'}
{'user': 'acct:swordyork@hypothes.is', 'text': '【其它】通“其他”（用于事物）。我们统一用其他。', 'time': '2017-02-07T14:12'}
{'user': 'acct:swordyork@hypothes.is', 'text': '原文使用encoder function，此处我把前面的编码函数改成编码器函数了。', 'time': '2017-02-07T13:53'}
{'user': 'acct:swordyork@hypothes.is', 'text': '此处是相同（完全一样），是一个东西。', 'time': '2017-02-07T13:46'}
{'user': 'acct:swordyork@hypothes.is', 'text': '你好！跟老师同学商量之后，latent一致翻译为潜（潜在的），hidden翻译成隐（隐藏的隐含的）。', 'time': '2017-02-07T10:58'}
{'user': 'acct:swordyork@hypothes.is', 'text': '确实全漏了，已经补全。多谢！', 'time': '2017-01-17T05:14'}
{'user': 'acct:swordyork@hypothes.is', 'text': '恩，可以帮助理解，将自动编码器视为生成模型，最大化似然就可以理解了。就像你之前提的，VAE学习数据分布，所以不会过拟合。我之后罗嗦一点讲清楚好了。', 'time': '2017-01-16T07:51'}
{'user': 'acct:huangpingchun@hypothes.is', 'text': '最大概率这个确实我不知道怎么体现进去，感觉没搞明白作者为啥做了这个表达在这个地方，本来我最初的想法是“因为模型是在训练数据集上通过求解出损失函数的一组参数（最大化理论概率分布），从而近似得到生成数据的概率分布”。看公式中只有最小化相对熵中需要最大化理论概率分布，而且也不是最大化概率。这块还得好好理解理解。', 'time': '2017-01-14T04:13'}
{'user': 'acct:swordyork@hypothes.is', 'text': '理由1很充分，我可能会改成\n> 这些变种（或衍生）自编码器能够学习出高容量且过完备的模型，进而发现输入数据中有用的结构信息，并且也无需对模型进行正则化。\n\n第二句话还是感觉不太恰当，\n> Their encodings are naturally useful because the models were trained to approximately maximize the probability of the training data rather than to copy the input to the output.\n\n译文肯定有问题，但改之后还是没有说明跟原文一样的原因（近似最大化概率），这里我得在好好理解理解。多谢！', 'time': '2017-01-13T14:47'}
{'user': 'acct:swordyork@hypothes.is', 'text': '这里我把“a trivial identity function”意译为“简单的复制功能”，反倒直译是“平凡的恒等函数”。不过 “即使容量足够大的模型去学习一个无意义的恒等函数”这个翻译确实更合理。\n因为数据分布是要从数据中学习出来的，我觉得翻成\n“非线性和过完备的正则化自编码器仍然能从数据中学习到数据分布的一些有趣结构” 更好，多谢！', 'time': '2017-01-12T13:20'}
{'user': 'acct:swordyork@hypothes.is', 'text': 'get!', 'time': '2017-01-12T13:12'}
